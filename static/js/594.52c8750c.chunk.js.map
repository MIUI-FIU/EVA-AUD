{"version":3,"file":"static/js/594.52c8750c.chunk.js","mappings":"sIAyMA,QAvMA,MACIA,WAAAA,CAAYC,EAAkBC,EAAQC,GAClCC,KAAKH,iBAAmBA,EACxBG,KAAKF,OAASA,EACdE,KAAKD,OAASA,EACdC,KAAKC,MAAQ,GACbD,KAAKE,YAAa,EAClBF,KAAKG,gBAAiB,EAEtBH,KAAKI,aAAeC,EAAAA,aAAaC,iBAAiBN,KAAKF,OAAQE,KAAKD,QAGpEC,KAAKO,iBACT,CAEAC,2BAAAA,GACI,OAAO,IAAIC,SAAQ,CAACC,EAASC,KACzB,MAAMC,EAAcC,EAAAA,YAAYC,6BAC1BC,EAAa,IAAIC,EAAAA,iBAAiBhB,KAAKI,aAAcQ,GAE3D,IAAIK,EAAiB,GACjBC,EAAe,KAEnB,IAAIC,EAA4B,KAGhCD,EAAeE,YAAW,KACtBC,QAAQC,IAAI,kEACZP,EAAWQ,gCAA+B,KACtCb,EAAQ,CACJc,KAAMP,EAAeQ,OACrBC,UAAWP,GACb,GACJ,GAXmB,KAezBJ,EAAWY,YAAc,CAACC,EAAGC,KACzBR,QAAQC,IAAI,+BACZD,QAAQC,IAAI,mBAADQ,OAAoBD,EAAEE,OAAOP,OAGxCQ,aAAad,GACbA,EAAeE,YAAW,KACtBC,QAAQC,IAAI,oDACZP,EAAWQ,gCAA+B,KACtCb,EAAQ,CACJc,KAAMP,EAAeQ,OACrBC,UAAWP,GACb,GACJ,GA5Be,IA6BD,EAIxBJ,EAAWkB,WAAa,CAACL,EAAGC,KACxBR,QAAQC,IAAI,8BACRO,EAAEE,OAAOG,SAAWC,EAAAA,aAAaC,kBACjCf,QAAQC,IAAI,iBAADQ,OAAkBD,EAAEE,OAAOP,OACtCP,GAAkBY,EAAEE,OAAOP,KAAO,IAClCL,EAA4BkB,KAAKC,OAC1BT,EAAEE,OAAOG,SAAWC,EAAAA,aAAaI,SACxClB,QAAQC,IAAI,kBAChB,EAIJP,EAAWyB,SAAW,CAACZ,EAAGC,KACtBR,QAAQoB,MAAM,4BACdpB,QAAQoB,MAAM,yBAADX,OAA0BD,EAAEK,SACzCnB,EAAWQ,gCAA+B,KACtCZ,EAAO,IAAI+B,MAAM,yBAADZ,OAA0BD,EAAEK,SAAU,GACxD,EAINnB,EAAW4B,eAAiB,CAACf,EAAGC,KAC5BR,QAAQC,IAAI,kCACZP,EAAWQ,gCAA+B,KACtCb,EAAQ,CACJc,KAAMP,EAAeQ,OACrBC,UAAWP,GACb,GACJ,EAGNE,QAAQC,IAAI,sCACZP,EAAW6B,iCACP,IAAMvB,QAAQC,IAAI,sCACjBuB,IACGxB,QAAQoB,MAAM,8BAA+BI,GAC7ClC,EAAOkC,EAAI,GAElB,GAET,CAEAtC,eAAAA,GACIP,KAAKI,aAAa0C,yBAA2B,oBAE7C,MAAMC,EAAS,IAAIC,EAAAA,wBACbpC,EAAcC,EAAAA,YAAYoC,kBAAkBF,GAElD/C,KAAKkD,YAAc,IAAIC,EAAAA,kBAAkBnD,KAAKI,aAAcQ,GAE5DZ,KAAKkD,YAAYE,eAAiB,CAACxB,EAAGC,KAClC7B,KAAKqD,0BAA0BxB,EAAEyB,SAAUzB,EAAE0B,YAAY,CAEjE,CAEAC,WAAAA,CAAYhC,GACRxB,KAAKC,MAAMwD,KAAKjC,GACXxB,KAAKE,YACNF,KAAK0D,cAEb,CAEAA,YAAAA,GACI,GAA0B,IAAtB1D,KAAKC,MAAM0D,OAEX,YADA3D,KAAKE,YAAa,GAItBF,KAAKE,YAAa,EAClB,MAAMsB,EAAOxB,KAAKC,MAAM2D,QACxB5D,KAAK6D,iBAAiBrC,GAAMsC,MAAK,IAAM9D,KAAK0D,gBAChD,CAEAG,gBAAAA,CAAiBrC,GACb,OAAO,IAAIf,SAAQ,CAACC,EAASC,KACzBX,KAAKkD,YAAYa,eAAevC,GAC5BO,IACIV,QAAQC,IAAI,+BACZZ,EAAQqB,EAAO,IAEnBU,IACIpB,QAAQC,IAAI,6CACZX,EAAO8B,EAAM,GACf,GAEd,CAEAY,yBAAAA,CAA0BC,EAAUC,GAEhCnC,YAAW,KACPpB,KAAKgE,uBAAuBV,EAAS,GAFZC,EAAc,IAI/C,CAEAS,sBAAAA,CAAuBV,GACnB,MAAMW,EAAUjE,KAAKH,iBAAiBoE,QAEjCjE,KAAKG,gBASNH,KAAKH,iBAAiBqE,mBACtBD,EAAQE,oBATS,IAAbb,EACAW,EAAQE,iBAAiB,IAEzBb,GAAY,EACZW,EAAQG,gBAAgBd,EAAU,GAAI,IAQ9CW,EAAQI,cACZ,CAEAC,UAAAA,GAEItE,KAAKkD,YAAYqB,QAGjBvE,KAAKC,MAAQ,GACbD,KAAKE,YAAa,EAGlBF,KAAKO,kBAELc,QAAQC,IAAI,4BAChB,CAEAkD,eAAAA,CAAgBhD,GAEZxB,KAAKkD,YAAYqB,QAGjBvE,KAAKC,MAAQ,GACbD,KAAKE,YAAa,EAClBF,KAAKO,kBAEDiB,GACAxB,KAAKwD,YAAYhC,GAGrBH,QAAQC,IAAI,gCAChB,E,gDC/IF,QAvDF,MACI1B,WAAAA,CAAYE,GACV,IAAKA,GAA4B,kBAAXA,EACpB,MAAM,IAAI4C,MAAM,4CAElB1C,KAAKF,OAASA,EACdE,KAAKyE,OAAS,4CAChB,CAQA,iBAAMC,CAAYlD,GAChB,MAAMmD,EAAU,CACdC,MAAO,gBACPC,SAAU,CACR,CAAEC,KAAM,SAAUC,QAJWC,UAAArB,OAAA,QAAAsB,IAAAD,UAAA,GAAAA,UAAA,GAAG,oCAKhC,CAAEF,KAAM,OAAQC,QAASvD,IAE3B0D,WAAY,IACZC,YAAa,IAGf,IAAK,IAADC,EAAAC,EAAAC,EAAAC,EACF,MAAMC,QAAiBC,MAAMzF,KAAKyE,OAAQ,CACxCiB,OAAQ,OACRC,QAAS,CACP,cAAgB,UAAD7D,OAAY9B,KAAKF,QAChC,eAAgB,oBAElB8F,KAAMC,KAAKC,UAAUnB,KAGvB,IAAKa,EAASO,GACZ,MAAM,IAAIrD,MAAM,mCAADZ,OAAoC0D,EAASQ,SAG9D,MACMC,EAA0B,QAAfb,SADEI,EAASU,QACHC,eAAO,IAAAf,GAAK,QAALC,EAAZD,EAAe,UAAE,IAAAC,GAAS,QAATC,EAAjBD,EAAmBe,eAAO,IAAAd,GAAS,QAATC,EAA1BD,EAA4BP,eAAO,IAAAQ,OAAvB,EAAZA,EAAqC9D,OAEzD,IAAKwE,EACH,MAAM,IAAIvD,MAAM,+CAGlB,OAAOuD,CACT,CAAE,MAAOxD,GAEP,MADApB,QAAQoB,MAAM,kCAAmCA,GAC3CA,CACR,CACF,E,gGC7CJ,IACI4D,EAGAC,EAEAC,EAKAC,EAXAC,GAAuB,EAEvBC,EAAiB,GAMjBC,GAAoB,EACpBC,GAAkB,EAItB,SAASC,EAAQC,GAEbA,EAAQC,SAAQC,IAAoD,IAAnD,GAAEC,EAAE,UAAEC,EAAS,SAAEC,EAAQ,YAAEC,EAAc,IAAIJ,EAC1DnH,iBAAiBwH,eAAeJ,EAAgB,GAAZC,EAAgBC,EAAU,EAAGC,EAAY,GAErF,CAEA,SAASE,IACL,IAAIR,EAAU,CACV,CACE,GAAM,IACN,UAAa,GACb,SAAY,IACZ,YAAe,IAEjB,CACE,GAAM,KACN,UAAa,GACb,SAAY,IACZ,YAAe,KAIrBD,EAAgBC,EACpB,CAeA,SAASS,IACL1H,iBAAiBqE,kBACrB,CAGA,SAASsD,EAAMC,GACX,OAAO,IAAIhH,SAAQC,GAAWU,WAAWV,EAAS+G,IACtD,CAEAC,eAAeC,EAAWnG,GACtB+E,EAAchG,kBACd,IAAIqH,QAA0BrB,EAAc1C,iBAAiBrC,GAE7DH,QAAQC,IAAI,4BAA6BsG,GACzCvG,QAAQC,IAAI,gCAAiCE,GAC7CH,QAAQC,IAAI,mCAAoCsG,EAAkBC,eAElExG,QAAQC,IAAI,iBAEZ,UACUkG,EAAMI,EAAkBC,cAAgB,KAC9CxG,QAAQC,IAAI,eAChB,CAAE,MAAOmB,GACiB,kBAAlBA,EAAM2D,QACN/E,QAAQC,IAAI,qBAEZD,QAAQoB,MAAM,sBAAuBA,EAE7C,CACJ,CA4BAiF,eAAeI,EAAMjI,EAAkBkI,GA8BnC,GA7BA1G,QAAQC,IAAI,kBAEZD,QAAQC,IAAI,qBAAsByG,EAASC,gBAC3C3G,QAAQC,IAAI,oBAAqByG,EAASE,eAE1C5G,QAAQC,IAAI,iCAAkCyG,EAASC,gBACvD3G,QAAQC,IAAI,gCAAiCyG,EAASE,eAEtD5G,QAAQC,IAAI,sBAAuByG,EAASC,eAAeE,SAC3D7G,QAAQC,IAAI,qBAAsByG,EAASE,cAAcC,SAEzD1B,EAAkB,EAIdD,EADAwB,EAASE,eAAkD,iBAA1BF,EAASE,cAC1B,IAAIE,EAAAA,EAActI,EAAkBkI,EAASE,cAAe,UAG5D,IAAIE,EAAAA,EAActI,EAAkBkI,EAASE,cAAcC,QAAS,UAGxFZ,UAEMK,EAAW,4FAAD7F,OAA6FiG,EAASK,YAAW,uCAEjIb,IAEAlG,QAAQC,IAAI,mBAAoBsF,GAE5B,4BAA6ByB,OAAQ,CACrChH,QAAQC,IAAI,gCACZqF,GAAoB,EAEpB,IAAI2B,EAAe,CAAC,EAGpB,IAAI1B,EAAgB,CAChBH,GAAuB,EACvBG,GAAkB,EAGlB,MAAM2B,EAAoBF,OAAOE,mBAAqBF,OAAOG,wBAC7DnC,EAAc,IAAIkC,EAIdjC,EADAyB,EAASC,gBAAoD,iBAA3BD,EAASC,eAC9B,IAAIS,EAAAA,EAAoBV,EAASC,gBAGjC,IAAIS,EAAAA,EAAoBV,EAASC,eAAeE,SAGjE7B,EAAYqC,YAAa,EACzBrC,EAAYsC,KAAO,QACnBtC,EAAYuC,gBAAiB,EAC7BvC,EAAYwC,gBAAkB,EAE9BxC,EAAYyC,SAAWpB,eAAeqB,GAClC,IAAIC,EAAa,GACbC,GAAU,EAEVC,EAAsB,CAAC,EAE3B,IAAK,IAAIC,EAAIJ,EAAMK,YAAaD,EAAIJ,EAAMM,QAAQ1F,OAAQwF,IACtDH,GAAcD,EAAMM,QAAQF,GAAG,GAAGH,WAC9BD,EAAMM,QAAQF,GAAGF,UACjBA,GAAU,GAOlB,GAHA5H,QAAQC,IAAI,qBAAsBqF,GAG9BsC,GAAWD,IAAetC,IAAmBC,EAAmB,CAChED,EAAiBsC,EACjBrC,GAAoB,EAGpB,IAAI2C,EAAoBN,EAAWO,cAAc9H,OAWjD,GATAJ,QAAQC,IAAI,qBAAsBgI,GAS9BA,EAAkB,CAElBjI,QAAQC,IAAI,cAAeyG,EAASC,gBACpC3G,QAAQC,IAAI,aAAcyG,EAASE,eAEnCiB,EAAiC,YAAII,EAErC,IAAIE,EAAgB,GAiBpB,GAhBAA,QA5HxB9B,eAA6BsB,EAAY1C,GACrC,IAAIL,EAAc,GAElB,IACIA,QAAoBK,EAAW5B,YAAYsE,GAI3C3H,QAAQC,IAAI,gBAAiB2E,EACjC,CACA,MAAOpE,GACHR,QAAQoB,MAAM,iCAAkCZ,EACpD,CACA,IAMI,OALGoE,UACO0B,EAAW1B,GACjBO,GAAmB,GAGhBP,CACX,CACA,MAAOpE,GACHR,QAAQoB,MAAM,iCAAkCZ,EACpD,CACJ,CAoG8C4H,CAAcH,EAAmBhD,GAEvD4C,EAAoC,eAAIM,EAIxCnI,QAAQC,IAAI,qBACZD,QAAQC,IAAI,mBAAoBkF,GAChCnF,QAAQC,IAAI,wBAAyByG,EAASK,aAE9CE,EAAa9B,EAAgB,GAAK0C,EAElCvC,GAAoB,EAEpBtF,QAAQC,IAAI,qBAAsBqF,GAE9BH,EAAkBuB,EAASK,YAAY,CACvCzB,GAAoB,QACdgB,EAAW,+BAAD7F,OAAgCiG,EAASK,YAAW,sBAC9DT,EAAW,gGACXA,EAAW,sBAEjBL,IApLxBT,EATc,CACV,CACI,GAAM,MACN,UAAa,GACb,SAAY,IACZ,YAAe,YA0LOW,EAAM,KACZD,IAEA,IAAImC,EAAc7D,KAAKC,UAAUwC,EAAc,KAAM,GAErDjH,QAAQC,IAAI,yBAADQ,OAA0B4H,IAErC9C,GAAkB,EAClBP,EAAYsD,MAChB,CACJ,CACJ,CACJ,EACAtD,EAAYuD,MAAQ,WACXhD,IACDvF,QAAQC,IAAI,oCACRmF,IACAJ,EAAYyB,QACZzG,QAAQC,IAAI,kCAGxB,CACJ,CACJ,MAEID,QAAQoB,MAAM,oDAGU,GAAxBgE,IACAJ,EAAYyB,QACZzG,QAAQC,IAAI,+BAEpB,CAEA,SAASqI,IACLtI,QAAQC,IAAI,iBAEZsF,GAAkB,EAClBvF,QAAQC,IAAI,mBAAoBsF,GAO5BH,IACAJ,EAAYsD,OACZtI,QAAQC,IAAI,gCAGhBD,QAAQC,IAAI,oBAChB,C","sources":["VISOS/action/verbalizers/SpeechManager.js","VISOS/cognition/TextToGptReconciler.js","modules/evaLLM.js"],"sourcesContent":["import { AudioConfig, PropertyId, ResultReason, SpeakerAudioDestination, SpeechConfig, SpeechRecognizer, SpeechSynthesizer } from \"microsoft-cognitiveservices-speech-sdk\";\n\nclass SpeechManager {\n    constructor(animationManager, apiKey, region) {\n        this.animationManager = animationManager;\n        this.apiKey = apiKey,\n        this.region = region,\n        this.queue = [];\n        this.isSpeaking = false;\n        this.audioInterrupt = false;\n\n        this.speechConfig = SpeechConfig.fromSubscription(this.apiKey, this.region)\n\n        // Initialize synthesizer\n        this.initSynthesizer();\n    }\n\n    recognizeSpeechUntilSilence() {\n        return new Promise((resolve, reject) => {\n            const audioConfig = AudioConfig.fromDefaultMicrophoneInput();\n            const recognizer = new SpeechRecognizer(this.speechConfig, audioConfig);\n    \n            let recognizedText = '';\n            let silenceTimer = null;\n            const silenceThreshold = 3000; // Silence threshold in milliseconds (3 seconds)\n            let finalRecognitionTimestamp = null; // To store the timestamp of the final recognition event\n            \n            // Start the silence timer immediately\n            silenceTimer = setTimeout(() => {\n                console.log('Silence detected (before recognition), stopping recognition...');\n                recognizer.stopContinuousRecognitionAsync(() => {\n                    resolve({\n                        text: recognizedText.trim(), \n                        timestamp: finalRecognitionTimestamp\n                    }); // Resolve with any recognized text (may be empty if no recognition)\n                });\n            }, silenceThreshold);\n    \n            // Handle partial results\n            recognizer.recognizing = (s, e) => {\n                console.log('Recognizing event triggered');\n                console.log(`Partial result: ${e.result.text}`);\n                \n                // If there's speech, reset the silence timer\n                clearTimeout(silenceTimer);\n                silenceTimer = setTimeout(() => {\n                    console.log('Silence detected during recognition, stopping...');\n                    recognizer.stopContinuousRecognitionAsync(() => {\n                        resolve({\n                            text: recognizedText.trim(),\n                            timestamp: finalRecognitionTimestamp\n                        }); // Resolve with the recognized text and timestamp\n                    });\n                }, silenceThreshold);\n            };\n    \n            // Handle final recognition results\n            recognizer.recognized = (s, e) => {\n                console.log('Recognized event triggered');\n                if (e.result.reason === ResultReason.RecognizedSpeech) {\n                    console.log(`Final result: ${e.result.text}`);\n                    recognizedText += e.result.text + ' ';\n                    finalRecognitionTimestamp = Date.now(); // Capture the timestamp of the final result\n                } else if (e.result.reason === ResultReason.NoMatch) {\n                    console.log('No match found.');\n                }\n            };\n    \n            // Handle cancellation events (e.g., when the recognition is interrupted)\n            recognizer.canceled = (s, e) => {\n                console.error('Canceled event triggered');\n                console.error(`Recognition canceled: ${e.reason}`);\n                recognizer.stopContinuousRecognitionAsync(() => {\n                    reject(new Error(`Recognition canceled: ${e.reason}`));\n                });\n            };\n    \n            // Handle session stopped event (e.g., when recognition ends)\n            recognizer.sessionStopped = (s, e) => {\n                console.log('Session stopped due to silence');\n                recognizer.stopContinuousRecognitionAsync(() => {\n                    resolve({\n                        text: recognizedText.trim(), \n                        timestamp: finalRecognitionTimestamp\n                    }); // Return the accumulated text and final timestamp\n                });\n            };\n    \n            console.log('Starting continuous recognition...');\n            recognizer.startContinuousRecognitionAsync(\n                () => console.log('Recognition started successfully'),\n                (err) => {\n                    console.error('Error starting recognition:', err);\n                    reject(err);\n                }\n            );\n        });\n    }\n    \n    initSynthesizer() {\n        this.speechConfig.speechSynthesisVoiceName = \"en-US-JennyNeural\";\n\n        const player = new SpeakerAudioDestination();;\n        const audioConfig = AudioConfig.fromSpeakerOutput(player);\n\n        this.synthesizer = new SpeechSynthesizer(this.speechConfig, audioConfig);\n\n        this.synthesizer.visemeReceived = (s, e) => {\n            this.scheduleVisemeApplication(e.visemeId, e.audioOffset);\n        };\n    }\n\n    enqueueText(text) {\n        this.queue.push(text);\n        if (!this.isSpeaking) {\n            this.processQueue();\n        }\n    }\n\n    processQueue() {\n        if (this.queue.length === 0) {\n            this.isSpeaking = false;\n            return;\n        }\n\n        this.isSpeaking = true;\n        const text = this.queue.shift();\n        this.synthesizeSpeech(text).then(() => this.processQueue());\n    }\n\n    synthesizeSpeech(text) {\n        return new Promise((resolve, reject) => {\n            this.synthesizer.speakTextAsync(text,\n                result => {\n                    console.log(\"Speech synthesis completed.\");\n                    resolve(result);\n                },\n                error => {\n                    console.log(\"Error during speech synthesis, restarting\");\n                    reject(error);\n                });\n        });\n    }\n\n    scheduleVisemeApplication(visemeId, audioOffset) {\n        const offsetInMilliseconds = audioOffset / 10000;\n        setTimeout(() => {\n            this.applyVisemeToCharacter(visemeId);\n        }, offsetInMilliseconds);\n    }\n\n    applyVisemeToCharacter(visemeId) {\n        const facsLib = this.animationManager.facsLib;\n        // console.log('audioInterrupt:', this.audioInterrupt)\n        if (!this.audioInterrupt){\n            if (visemeId === 0) {\n                facsLib.setNeutralViseme(0.0);\n            } else {\n                visemeId -= 1;\n                facsLib.setTargetViseme(visemeId, 70, 0);\n            }\n        }\n        else{\n            this.animationManager.setFaceToNeutral()\n            facsLib.setNeutralViseme()\n        }\n\n        facsLib.updateEngine();\n    }\n\n    stopSpeech() {\n        // Dispose of the current synthesizer to stop speech\n        this.synthesizer.close();\n\n        // Clear the queue and reset the speaking state\n        this.queue = [];\n        this.isSpeaking = false;\n\n        // Reinitialize the synthesizer for future use\n        this.initSynthesizer();\n\n        console.log(\"Speech synthesis stopped.\");\n    }\n\n    interruptSpeech(text) {\n        // Dispose of the current synthesizer to stop speech\n        this.synthesizer.close();\n\n        // Clear the queue, reset the speaking state, and reinitialize the synthesizer\n        this.queue = [];\n        this.isSpeaking = false;\n        this.initSynthesizer();\n\n        if (text) {\n            this.enqueueText(text); // Enqueue and play the new text\n        }\n\n        console.log(\"Speech synthesis interrupted.\");\n    }\n}\n\nexport default SpeechManager;\n","class TextToGptReconciler {\n    constructor(apiKey) {\n      if (!apiKey || typeof apiKey !== 'string') {\n        throw new Error('A valid OpenAI API key must be provided.');\n      }\n      this.apiKey = apiKey;\n      this.apiUrl = 'https://api.openai.com/v1/chat/completions';\n    }\n  \n    /**\n     * Processes the provided text by sending it to the OpenAI API.\n     * @param {string} text - The input text to process.\n     * @param {string} instruction - Instruction or system prompt for the AI.\n     * @returns {Promise<string>} - The GPT response.\n     */\n    async processText(text, instruction = 'Answer in a professional manner:') {\n      const payload = {\n        model: 'gpt-3.5-turbo',  // You can switch this model based on your requirement\n        messages: [\n          { role: 'system', content: instruction },\n          { role: 'user', content: text }\n        ],\n        max_tokens: 150,  // Adjust as necessary\n        temperature: 0.7\n      };\n  \n      try {\n        const response = await fetch(this.apiUrl, {\n          method: 'POST',\n          headers: {\n            'Authorization': `Bearer ${this.apiKey}`,\n            'Content-Type': 'application/json'\n          },\n          body: JSON.stringify(payload)\n        });\n  \n        if (!response.ok) {\n          throw new Error(`API request failed with status: ${response.status}`);\n        }\n  \n        const data = await response.json();\n        const gptResponse = data.choices?.[0]?.message?.content?.trim();\n  \n        if (!gptResponse) {\n          throw new Error('Failed to get a valid response from the API');\n        }\n  \n        return gptResponse;\n      } catch (error) {\n        console.error('Error processing text with GPT:', error);\n        throw error;\n      }\n    }\n  }\n  \n  export default TextToGptReconciler;","// This module does STT -> QA (LLM) -> TTS.\nimport SpeechManager from \"../VISOS/action/verbalizers/SpeechManager\";\nimport TextToGptReconciler from \"../VISOS/cognition/TextToGptReconciler\";\n\n// This doesn't work requires CLI access which is not available on github pages.\n// import ollama from 'ollama'\n\nlet recognitionAvailable = false;\nlet recognition;\nlet lastTranscript = '';\n\nlet gptManager;\n\nlet speechManager;\n\nlet processingRequest = false;\nlet recognitionStop = false;\n\nlet numTransactions;\n\nfunction setFace(au_data){\n    // Extrapolated from AnimationManager.js\n    au_data.forEach(({ id, intensity, duration, explanation = \"\" }) => {\n        animationManager.scheduleChange(id, intensity * 90, duration, 0, explanation);\n    });\n}\n\nfunction slightSmileFace(){\n    let au_data = [\n        {\n          \"id\": \"6\",\n          \"intensity\": 0.4,\n          \"duration\": 750,\n          \"explanation\": \"\"\n        },\n        {\n          \"id\": \"12\",\n          \"intensity\": 0.5,\n          \"duration\": 750,\n          \"explanation\": \"\"\n        }\n      ]\n\n    setFace(au_data=au_data)\n}\n\nfunction wink(){\n    let au_data = [\n        {\n            \"id\": \"46L\",\n            \"intensity\": 0.9,\n            \"duration\": 300,\n            \"explanation\": \"\"\n        }\n    ]\n\n    setFace(au_data=au_data)\n}\n\nfunction neutralFace(){\n    animationManager.setFaceToNeutral()\n}\n\n\nfunction delay(ms) {\n    return new Promise(resolve => setTimeout(resolve, ms));\n}\n\nasync function agentSpeak(text) {\n    speechManager.initSynthesizer();\n    let synthesize_result = await speechManager.synthesizeSpeech(text);\n\n    console.log('synthesize_result object:', synthesize_result);\n    console.log('synthesize result transcript:', text)\n    console.log('synthesize result audioDuration:', synthesize_result.audioDuration);\n\n    console.log('before delay!');\n\n    try {\n        await delay(synthesize_result.audioDuration / 10000);\n        console.log('after delay!');\n    } catch (error) {\n        if (error.message === 'Delay aborted') {\n            console.log('Delay was aborted');\n        } else {\n            console.error('Error during delay:', error);\n        }\n    }\n}\n\nasync function processSpeech(transcript, gptManager){\n    let gptResponse = ''\n\n    try{\n        gptResponse = await gptManager.processText(transcript)\n        \n        // gptResponse = transcript\n\n        console.log('gptResponse: ', gptResponse)\n    }\n    catch (e){\n        console.error('Error processing GPT response:', e)\n    }\n    try{\n        if(gptResponse){\n            await agentSpeak(gptResponse)\n            numTransactions += 1\n        }\n\n        return gptResponse\n    }\n    catch (e) {\n        console.error('Error processing TTS response:', e)\n    }\n}\n\nasync function start(animationManager, settings) {\n    console.log('Calling start!')\n\n    console.log('openai key object:', settings.openai_api_key)\n    console.log('azure key object:', settings.azure_api_key)\n\n    console.log('openai key object type:', typeof settings.openai_api_key)\n    console.log('azure key object type:', typeof settings.azure_api_key)\n\n    console.log('openai key default:', settings.openai_api_key.default)\n    console.log('azure key default:', settings.azure_api_key.default)\n\n    numTransactions = 1\n\n    // Initialize SpeechManager object\n    if (settings.azure_api_key && typeof settings.azure_api_key == 'string'){\n        speechManager = new SpeechManager(animationManager, settings.azure_api_key, 'eastus')\n    }\n    else{\n        speechManager = new SpeechManager(animationManager, settings.azure_api_key.default, 'eastus')\n    }\n\n    slightSmileFace()\n\n    await agentSpeak(`Hello, my name is eva. I am powered by ChatGPT. I am programmed to provide a response to ${settings.num_prompts} questions. Feel free to ask away!`)\n\n    neutralFace()\n\n    console.log('recognitionStop:', recognitionStop)\n\n    if ('webkitSpeechRecognition' in window) {\n        console.log('SpeechRecognition available!')\n        processingRequest = false\n\n        let transactions = {}\n\n\n        if(!recognitionStop){\n            recognitionAvailable = true\n            recognitionStop = false;\n        \n            // Initialize SpeechRecognition object\n            const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;\n            recognition = new SpeechRecognition();\n\n            // Initialize TextToGptReconciler object\n            if (settings.openai_api_key && typeof settings.openai_api_key == 'string'){\n                gptManager = new TextToGptReconciler(settings.openai_api_key)\n            }\n            else{\n                gptManager = new TextToGptReconciler(settings.openai_api_key.default)\n            }\n\n            recognition.continuous = true;\n            recognition.lang = 'en-US'\n            recognition.interimResults = true\n            recognition.maxAlternatives = 1\n        \n            recognition.onresult = async function(event) {\n                let transcript = '';\n                let isFinal = false;\n\n                let current_transaction = {}\n        \n                for (let i = event.resultIndex; i < event.results.length; i++) {\n                    transcript += event.results[i][0].transcript;\n                    if (event.results[i].isFinal) {\n                        isFinal = true;\n                    }\n                }\n\n                console.log('processingRequest:', processingRequest)\n        \n                // Process only final results to avoid duplicates\n                if (isFinal && transcript !== lastTranscript && !processingRequest) {\n                    lastTranscript = transcript;\n                    processingRequest = true;\n\n                    // Process transcript\n                    let processTranscript = transcript.toLowerCase().trim();\n\n                    console.log('Recognized Speech:', processTranscript)\n\n                    // const response = await ollama.chat({\n                    //     model: 'phi3',\n                    //     messages: [{role: 'user', content: transcript}]\n                    // })\n\n                    // console.log(\"llama response:\", response)\n\n                    if (processTranscript){\n\n                        console.log('openai key:', settings.openai_api_key)\n                        console.log('azure key:', settings.azure_api_key)\n\n                        current_transaction[\"user_prompt\"] = processTranscript\n\n                        let agentResponse = ''\n                        agentResponse = await processSpeech(processTranscript, gptManager)\n\n                        current_transaction['agent_response'] = agentResponse\n\n                        // console.log('returned speechResponse:', speechResponse)\n\n                        console.log('finished request!')\n                        console.log('numTransactions:', numTransactions)\n                        console.log('settings.num_prompts:', settings.num_prompts)\n\n                        transactions[numTransactions-1] = current_transaction\n\n                        processingRequest = false\n\n                        console.log('processingRequest:', processingRequest)\n                        \n                        if (numTransactions > settings.num_prompts){\n                            processingRequest = true;\n                            await agentSpeak(`It appears I've answered my ${settings.num_prompts} questions.`)\n                            await agentSpeak(\"If you'd like to review this conversation, a transcript is available in the inspector.\")\n                            await agentSpeak('Talk to you later!')\n\n                            slightSmileFace()\n                            wink()\n                            await delay(1000)\n                            neutralFace()\n\n                            let json_string = JSON.stringify(transactions, null, 2)\n\n                            console.log(`Transaction History:\\n${json_string}`)\n\n                            recognitionStop = true\n                            recognition.stop()\n                        }\n                    }\n                }\n            };\n            recognition.onend = function() {\n                if (!recognitionStop){\n                    console.log('Speech recognition restarting...');\n                    if (recognitionAvailable) {\n                        recognition.start();  // Restart recognition\n                        console.log('Speech recognition restarted!');\n                    }\n                }\n            };\n        }\n    }\n    else {\n        console.error('SpeechRecognition not available in this browser!')\n    }\n\n    if (recognitionAvailable == true) {\n        recognition.start()\n        console.log(\"speech recognition started!\")\n    }\n}\n\nfunction stop() {\n    console.log('Calling stop!')\n    \n    recognitionStop = true\n    console.log('recognitionStop:', recognitionStop)\n\n    // Interrupt Speech (TBD)\n    // speechManager.player.pause()\n    // speechManager.audioInterrupt = true\n    // setWaitCondition(speechManager.audioInterrupt)\n    \n    if (recognitionAvailable){\n        recognition.stop()    \n        console.log(\"speech recognition stopped!\")\n    }\n\n    console.log('Reached end stop!')\n}\n\nexport { start, stop }"],"names":["constructor","animationManager","apiKey","region","this","queue","isSpeaking","audioInterrupt","speechConfig","SpeechConfig","fromSubscription","initSynthesizer","recognizeSpeechUntilSilence","Promise","resolve","reject","audioConfig","AudioConfig","fromDefaultMicrophoneInput","recognizer","SpeechRecognizer","recognizedText","silenceTimer","finalRecognitionTimestamp","setTimeout","console","log","stopContinuousRecognitionAsync","text","trim","timestamp","recognizing","s","e","concat","result","clearTimeout","recognized","reason","ResultReason","RecognizedSpeech","Date","now","NoMatch","canceled","error","Error","sessionStopped","startContinuousRecognitionAsync","err","speechSynthesisVoiceName","player","SpeakerAudioDestination","fromSpeakerOutput","synthesizer","SpeechSynthesizer","visemeReceived","scheduleVisemeApplication","visemeId","audioOffset","enqueueText","push","processQueue","length","shift","synthesizeSpeech","then","speakTextAsync","applyVisemeToCharacter","facsLib","setFaceToNeutral","setNeutralViseme","setTargetViseme","updateEngine","stopSpeech","close","interruptSpeech","apiUrl","processText","payload","model","messages","role","content","arguments","undefined","max_tokens","temperature","_data$choices","_data$choices$","_data$choices$$messag","_data$choices$$messag2","response","fetch","method","headers","body","JSON","stringify","ok","status","gptResponse","json","choices","message","recognition","gptManager","speechManager","numTransactions","recognitionAvailable","lastTranscript","processingRequest","recognitionStop","setFace","au_data","forEach","_ref","id","intensity","duration","explanation","scheduleChange","slightSmileFace","neutralFace","delay","ms","async","agentSpeak","synthesize_result","audioDuration","start","settings","openai_api_key","azure_api_key","default","SpeechManager","num_prompts","window","transactions","SpeechRecognition","webkitSpeechRecognition","TextToGptReconciler","continuous","lang","interimResults","maxAlternatives","onresult","event","transcript","isFinal","current_transaction","i","resultIndex","results","processTranscript","toLowerCase","agentResponse","processSpeech","json_string","stop","onend"],"sourceRoot":""}