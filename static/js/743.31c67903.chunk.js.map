{"version":3,"file":"static/js/743.31c67903.chunk.js","mappings":"kIAyMA,QAvMA,MACIA,WAAAA,CAAYC,EAAkBC,EAAQC,GAClCC,KAAKH,iBAAmBA,EACxBG,KAAKF,OAASA,EACdE,KAAKD,OAASA,EACdC,KAAKC,MAAQ,GACbD,KAAKE,YAAa,EAClBF,KAAKG,gBAAiB,EAEtBH,KAAKI,aAAeC,EAAAA,aAAaC,iBAAiBN,KAAKF,OAAQE,KAAKD,QAGpEC,KAAKO,iBACT,CAEAC,2BAAAA,GACI,OAAO,IAAIC,SAAQ,CAACC,EAASC,KACzB,MAAMC,EAAcC,EAAAA,YAAYC,6BAC1BC,EAAa,IAAIC,EAAAA,iBAAiBhB,KAAKI,aAAcQ,GAE3D,IAAIK,EAAiB,GACjBC,EAAe,KAEnB,IAAIC,EAA4B,KAGhCD,EAAeE,YAAW,KACtBC,QAAQC,IAAI,kEACZP,EAAWQ,gCAA+B,KACtCb,EAAQ,CACJc,KAAMP,EAAeQ,OACrBC,UAAWP,GACb,GACJ,GAXmB,KAezBJ,EAAWY,YAAc,CAACC,EAAGC,KACzBR,QAAQC,IAAI,+BACZD,QAAQC,IAAI,mBAADQ,OAAoBD,EAAEE,OAAOP,OAGxCQ,aAAad,GACbA,EAAeE,YAAW,KACtBC,QAAQC,IAAI,oDACZP,EAAWQ,gCAA+B,KACtCb,EAAQ,CACJc,KAAMP,EAAeQ,OACrBC,UAAWP,GACb,GACJ,GA5Be,IA6BD,EAIxBJ,EAAWkB,WAAa,CAACL,EAAGC,KACxBR,QAAQC,IAAI,8BACRO,EAAEE,OAAOG,SAAWC,EAAAA,aAAaC,kBACjCf,QAAQC,IAAI,iBAADQ,OAAkBD,EAAEE,OAAOP,OACtCP,GAAkBY,EAAEE,OAAOP,KAAO,IAClCL,EAA4BkB,KAAKC,OAC1BT,EAAEE,OAAOG,SAAWC,EAAAA,aAAaI,SACxClB,QAAQC,IAAI,kBAChB,EAIJP,EAAWyB,SAAW,CAACZ,EAAGC,KACtBR,QAAQoB,MAAM,4BACdpB,QAAQoB,MAAM,yBAADX,OAA0BD,EAAEK,SACzCnB,EAAWQ,gCAA+B,KACtCZ,EAAO,IAAI+B,MAAM,yBAADZ,OAA0BD,EAAEK,SAAU,GACxD,EAINnB,EAAW4B,eAAiB,CAACf,EAAGC,KAC5BR,QAAQC,IAAI,kCACZP,EAAWQ,gCAA+B,KACtCb,EAAQ,CACJc,KAAMP,EAAeQ,OACrBC,UAAWP,GACb,GACJ,EAGNE,QAAQC,IAAI,sCACZP,EAAW6B,iCACP,IAAMvB,QAAQC,IAAI,sCACjBuB,IACGxB,QAAQoB,MAAM,8BAA+BI,GAC7ClC,EAAOkC,EAAI,GAElB,GAET,CAEAtC,eAAAA,GACIP,KAAKI,aAAa0C,yBAA2B,oBAE7C,MAAMC,EAAS,IAAIC,EAAAA,wBACbpC,EAAcC,EAAAA,YAAYoC,kBAAkBF,GAElD/C,KAAKkD,YAAc,IAAIC,EAAAA,kBAAkBnD,KAAKI,aAAcQ,GAE5DZ,KAAKkD,YAAYE,eAAiB,CAACxB,EAAGC,KAClC7B,KAAKqD,0BAA0BxB,EAAEyB,SAAUzB,EAAE0B,YAAY,CAEjE,CAEAC,WAAAA,CAAYhC,GACRxB,KAAKC,MAAMwD,KAAKjC,GACXxB,KAAKE,YACNF,KAAK0D,cAEb,CAEAA,YAAAA,GACI,GAA0B,IAAtB1D,KAAKC,MAAM0D,OAEX,YADA3D,KAAKE,YAAa,GAItBF,KAAKE,YAAa,EAClB,MAAMsB,EAAOxB,KAAKC,MAAM2D,QACxB5D,KAAK6D,iBAAiBrC,GAAMsC,MAAK,IAAM9D,KAAK0D,gBAChD,CAEAG,gBAAAA,CAAiBrC,GACb,OAAO,IAAIf,SAAQ,CAACC,EAASC,KACzBX,KAAKkD,YAAYa,eAAevC,GAC5BO,IACIV,QAAQC,IAAI,+BACZZ,EAAQqB,EAAO,IAEnBU,IACIpB,QAAQC,IAAI,6CACZX,EAAO8B,EAAM,GACf,GAEd,CAEAY,yBAAAA,CAA0BC,EAAUC,GAEhCnC,YAAW,KACPpB,KAAKgE,uBAAuBV,EAAS,GAFZC,EAAc,IAI/C,CAEAS,sBAAAA,CAAuBV,GACnB,MAAMW,EAAUjE,KAAKH,iBAAiBoE,QAEjCjE,KAAKG,gBASNH,KAAKH,iBAAiBqE,mBACtBD,EAAQE,oBATS,IAAbb,EACAW,EAAQE,iBAAiB,IAEzBb,GAAY,EACZW,EAAQG,gBAAgBd,EAAU,GAAI,IAQ9CW,EAAQI,cACZ,CAEAC,UAAAA,GAEItE,KAAKkD,YAAYqB,QAGjBvE,KAAKC,MAAQ,GACbD,KAAKE,YAAa,EAGlBF,KAAKO,kBAELc,QAAQC,IAAI,4BAChB,CAEAkD,eAAAA,CAAgBhD,GAEZxB,KAAKkD,YAAYqB,QAGjBvE,KAAKC,MAAQ,GACbD,KAAKE,YAAa,EAClBF,KAAKO,kBAEDiB,GACAxB,KAAKwD,YAAYhC,GAGrBH,QAAQC,IAAI,gCAChB,E,qFCnMJ,MAiIA,EAjIsBmD,IAA0C,IAAzC,kBAAEC,EAAiB,aAAEC,GAAcF,EACxD,MAAMG,GAAWC,EAAAA,EAAAA,UACXC,GAAYD,EAAAA,EAAAA,UACZE,GAAcF,EAAAA,EAAAA,UACdG,GAAYH,EAAAA,EAAAA,WAGlBI,EAAAA,EAAAA,YAAU,KACWC,WACjB,IACE,MAAMC,EAAYC,wBACZC,EAAAA,GAAaC,iBAAiBC,YAAYJ,SAC1CE,EAAAA,GAAaG,kBAAkBD,YAAYJ,SAC3CE,EAAAA,GAAaI,mBAAmBF,YAAYJ,SAC5CE,EAAAA,GAAaK,kBAAkBH,YAAYJ,GAEjDQ,GACF,CAAE,MAAOlD,GACPpB,QAAQoB,MAAM,wBAAyBA,EACzC,GAGFmD,EAAY,GACX,IAGH,MAAMD,EAAaA,KACjBE,UAAUC,aACPC,aAAa,CAAEC,MAAO,CAAC,IACvBlC,MAAMmC,IACLjB,EAAUkB,QAAUD,EACpBrB,EAASsB,QAAQC,UAAYF,CAAM,IAEpCG,OAAOvD,GAAQxB,QAAQoB,MAAM,0BAA2BI,IAAK,EAI5DwD,EAAYA,KACZrB,EAAUkB,SACZlB,EAAUkB,QAAQI,YAAYC,SAASC,GAAUA,EAAMC,QACzD,EAIIC,EAAaxB,UACjB,IAAKN,EAASsB,SAAWtB,EAASsB,QAAQS,QAAU/B,EAASsB,QAAQU,MACnE,OAGF,MAAMC,QAAmBxB,EAAAA,EAAuBT,EAASsB,QAAS,IAAIb,EAAAA,IACnEyB,oBACAC,sBAEGC,EAASlC,EAAUoB,QACzB,IAAKc,EAEH,YADA3F,QAAQoB,MAAM,2BAIhB,MAAMwE,EAAc,CAAEC,MAAOtC,EAASsB,QAAQiB,WAAYC,OAAQxC,EAASsB,QAAQmB,aACnFhC,EAAAA,GAAwB2B,EAAQC,GAChC,MAAMK,EAAoBjC,EAAAA,GAAsBwB,EAAYI,GAQ5D,GANAD,EAAOO,WAAW,MAAMC,UAAU,EAAG,EAAGR,EAAOE,MAAOF,EAAOI,QAC7D/B,EAAAA,GAAAA,eAA4B2B,EAAQM,GACpCjC,EAAAA,GAAAA,kBAA+B2B,EAAQM,GACvCjC,EAAAA,GAAAA,oBAAiC2B,EAAQM,GAGrCT,EAAWlD,OAAS,EAAG,CACzB,MAAM8D,EAAcZ,EAAW,GAAGY,YAS5BC,EALgBC,OAAOC,QAAQH,GAAaI,KAAIC,IAAA,IAAEC,EAASC,GAAWF,EAAA,MAAK,CAACC,EAASC,EAAY3F,KAAKC,MAAM,IAKxE2F,MAAK,CAACC,EAAGC,IAAMA,EAAE,GAAKD,EAAE,KAa9DxD,GACFA,EAAkBgD,EAEtB,GA0BF,OAtBAzC,EAAAA,EAAAA,YAAU,KACR,MAAMmD,EAAaA,KACZrD,EAAYmB,UACfnB,EAAYmB,QAAUmC,YAAY3B,EAAY,KAChD,EAGI4B,EAAe1D,EAASsB,QAQ9B,OAPY,OAAZoC,QAAY,IAAZA,GAAAA,EAAcC,iBAAiB,OAAQH,GAGnCzD,IACFA,EAAauB,QAAUG,GAGlB,KACLmC,cAAczD,EAAYmB,SAC1BnB,EAAYmB,QAAU,KACV,OAAZoC,QAAY,IAAZA,GAAAA,EAAcG,oBAAoB,OAAQL,EAAW,CACtD,GACA,KAGDM,EAAAA,EAAAA,MAAA,OAAKC,MAAO,CAAEC,SAAU,YAAaC,SAAA,EACnCC,EAAAA,EAAAA,KAAA,SAAOC,IAAKnE,EAAUoE,UAAQ,EAACC,OAAK,EAAC/B,MAAM,MAAME,OAAO,MAAMuB,MAAO,CAAEC,SAAU,eACjFE,EAAAA,EAAAA,KAAA,UAAQC,IAAKjE,EAAW6D,MAAO,CAAEC,SAAU,WAAYM,KAAM,EAAGC,IAAK,OACjE,C,oKC9GV,IAAIC,EAAmB,KAGvB,MAAMC,EAA0B,CAAC,YAAa,YAAa,UAAW,MAAO,QAAS,QAAS,WAG/F,IAEIC,EACAC,EAKAC,EAGAC,EAXAC,EAAO,KACP/E,EAAe,KAIfgF,GAAwB,EACxBC,EAAuB,GAM3B,MAAMC,EAAa,IAGnB,IAAIC,EAAiB,UACjBC,EAAyB,EACzBC,EAAwB,EACxBC,EAAuB,EAG3B,MAAMC,EAAsB,CAAC,aAAc,SAAU,OAAQ,UAAW,SAGxE,IACIC,EAOAC,EAMAC,EAGAC,EAjBAC,GAAuB,EAKvBC,GAAkB,EAUlBC,GAAuB,EAIvBC,EAAgB,eAIpB,MAAMC,EAAyBC,IAG3BrB,EAAkBqB,EAAqB,GACvCtB,EAAkBC,EAAgB,GAEL,GAAzBI,GACAC,EAAqBnG,KAAK8F,EAG9B,EA6CJ,SAASsB,EAAMC,GACX,OAAO,IAAIrK,SAAQC,GAAWU,WAAWV,EAASoK,IACtD,CAEA5F,eAAe6F,EAAWvJ,EAAMwJ,GAA0C,IAAnCC,EAAwBC,UAAAvH,OAAA,QAAAwH,IAAAD,UAAA,IAAAA,UAAA,GAC3Dd,EAAc7J,kBACd,IAAI6K,QAA0BhB,EAAcvG,iBAAiBrC,GAE7D,MAAM6J,EAAYD,EAAkBE,cAAgB,IAEpDZ,EAAgBlJ,EAChB+J,KAEIN,GACAD,EAAM,CACFQ,MAAO,gBACPC,YAAajK,EACbkK,OAAQ,OACRC,SAAU,EACVC,YAAY,IAGpBvK,QAAQC,IAAI,4BAA6B8J,GACzC/J,QAAQC,IAAI,gCAAiCE,GAC7CH,QAAQC,IAAI,mCAAoC8J,EAAkBE,eAElEjK,QAAQC,IAAI,iBAEZ,UACUuJ,EAAMQ,GACZhK,QAAQC,IAAI,eAChB,CAAE,MAAOmB,GACiB,kBAAlBA,EAAMoJ,QACNxK,QAAQC,IAAI,qBAEZD,QAAQoB,MAAM,sBAAuBA,EAE7C,CACJ,CAiIA,SAASqJ,IACL,MAAMC,EAA2B,CAC7B,KAAQ,sBACR,UAAa,GACb,aAAgB,2IAyChB,UAAa,CACT,oDACA,yFACA,6DACA,8GACA,gHACA,oIACA,4FACA,gIACA,sEACA,8HAEJ,oBAAuB,CACnB,CAAC,QAAS,eAAgB,4BAA6B,4BAA6B,gCACpF,CAAC,OAAQ,aAAc,gBAAiB,cAAe,gBAAiB,eACxE,CAAC,QAAS,oBAAqB,UAAW,SAAU,QAAS,gBAC7D,CAAC,QAAS,oBAAqB,UAAW,SAAU,QAAS,gBAC7D,CAAC,QAAS,oBAAqB,UAAW,SAAU,QAAS,gBAC7D,CAAC,QAAS,oBAAqB,UAAW,SAAU,QAAS,gBAC7D,CAAC,QAAS,oBAAqB,UAAW,SAAU,QAAS,gBAC7D,CAAC,QAAS,oBAAqB,UAAW,SAAU,QAAS,gBAC7D,CAAC,KAAM,gCAAiC,6BACxC,CAAC,KAAM,gCAAiC,8BAE5C,0BAA6B,CACzB,CAAC,QAAS,eAAgB,oBAAqB,sBAAuB,qBAAsB,mBAAoB,qBAAsB,gCACtI,CAAC,OAAQ,MAAO,MAAO,QAAS,OAAQ,OAAQ,MAAO,QAAS,QAAS,OAAQ,iBACjF,CAAC,QAAS,oBAAqB,UAAW,SAAU,QAAS,gBAC7D,CAAC,QAAS,oBAAqB,UAAW,SAAU,QAAS,gBAC7D,CAAC,QAAS,oBAAqB,UAAW,SAAU,QAAS,gBAC7D,CAAC,QAAS,oBAAqB,UAAW,SAAU,QAAS,gBAC7D,CAAC,QAAS,oBAAqB,UAAW,SAAU,QAAS,gBAC7D,CAAC,QAAS,oBAAqB,UAAW,SAAU,QAAS,gBAC7D,CAAC,KAAM,gCAAiC,6BACxC,CAAC,KAAM,gCAAiC,8BAE5C,mBAAsB,CAClB,CAAC,EAAG,EAAG,EAAG,EAAG,EAAG,EAAG,EAAG,GACtB,CAAC,EAAG,EAAG,EAAG,EAAG,EAAG,EAAG,EAAG,EAAG,EAAG,EAAG,GAC/B,CAAC,EAAG,EAAG,EAAG,EAAG,EAAG,GAChB,CAAC,EAAG,EAAG,EAAG,EAAG,EAAG,GAChB,CAAC,EAAG,EAAG,EAAG,EAAG,EAAG,GAChB,CAAC,EAAG,EAAG,EAAG,EAAG,EAAG,GAChB,CAAC,EAAG,EAAG,EAAG,EAAG,EAAG,GAChB,CAAC,EAAG,EAAG,EAAG,EAAG,EAAG,GAChB,CAAC,EAAG,EAAG,GACP,CAAC,EAAG,EAAG,KA+HTC,EAA6B,CAC/B,eAAkB,yBAClB,aAAgB,qEAChB,eAAkB,CAACD,IAGjBE,EAA2B,CAC7B,eAAkB,yBAClB,aAAgB,qEAGhB,eAAkB,CAvIuB,CACzC,KAAQ,sBACR,UAAa,GACb,aAAgB,2IAEhB,UAAa,CACT,oDACA,yFACA,6DACA,8GACA,gHACA,oIACA,4FACA,gIACA,sEACA,8HAEJ,iBAAoB,CAChB,oDACA,yFACA,6DACA,8GACA,gHACA,oIACA,4FACA,gIACA,sEACA,8HAEJ,oBAAuB,CACnB,CAAC,QAAS,eAAgB,4BAA6B,4BAA6B,gCACpF,CAAC,OAAQ,aAAc,gBAAiB,cAAe,gBAAiB,eACxE,CAAC,QAAS,oBAAqB,UAAW,SAAU,QAAS,gBAC7D,CAAC,QAAS,oBAAqB,UAAW,SAAU,QAAS,gBAC7D,CAAC,QAAS,oBAAqB,UAAW,SAAU,QAAS,gBAC7D,CAAC,QAAS,oBAAqB,UAAW,SAAU,QAAS,gBAC7D,CAAC,QAAS,oBAAqB,UAAW,SAAU,QAAS,gBAC7D,CAAC,QAAS,oBAAqB,UAAW,SAAU,QAAS,gBAC7D,CAAC,KAAM,gCAAiC,6BACxC,CAAC,KAAM,gCAAiC,8BAE5C,0BAA6B,CACzB,CAAC,QAAS,eAAgB,oBAAqB,sBAAuB,qBAAsB,mBAAoB,qBAAsB,gCACtI,CAAC,OAAQ,MAAO,MAAO,QAAS,OAAQ,OAAQ,MAAO,QAAS,QAAS,OAAQ,iBACjF,CAAC,QAAS,oBAAqB,UAAW,SAAU,QAAS,gBAC7D,CAAC,QAAS,oBAAqB,UAAW,SAAU,QAAS,gBAC7D,CAAC,QAAS,oBAAqB,UAAW,SAAU,QAAS,gBAC7D,CAAC,QAAS,oBAAqB,UAAW,SAAU,QAAS,gBAC7D,CAAC,QAAS,oBAAqB,UAAW,SAAU,QAAS,gBAC7D,CAAC,QAAS,oBAAqB,UAAW,SAAU,QAAS,gBAC7D,CAAC,KAAM,gCAAiC,6BACxC,CAAC,KAAM,gCAAiC,8BAE5C,mBAAsB,CAClB,CAAC,EAAG,EAAG,EAAG,EAAG,EAAG,EAAG,EAAG,GACtB,CAAC,EAAG,EAAG,EAAG,EAAG,EAAG,EAAG,EAAG,EAAG,EAAG,EAAG,GAC/B,CAAC,EAAG,EAAG,EAAG,EAAG,EAAG,GAChB,CAAC,EAAG,EAAG,EAAG,EAAG,EAAG,GAChB,CAAC,EAAG,EAAG,EAAG,EAAG,EAAG,GAChB,CAAC,EAAG,EAAG,EAAG,EAAG,EAAG,GAChB,CAAC,EAAG,EAAG,EAAG,EAAG,EAAG,GAChB,CAAC,EAAG,EAAG,EAAG,EAAG,EAAG,GAChB,CAAC,EAAG,EAAG,GACP,CAAC,EAAG,EAAG,OAiFf5K,QAAQC,IAAI,0BAA2BgJ,GAGnCD,EAD0B,GAA1BC,EACoB2B,EAGAD,EAOxB3K,QAAQC,IAAI,qBAAsB+I,EACtC,CAwIAnF,eAAegH,EAAyBlB,GAA0C,IAAnCC,EAAwBC,UAAAvH,OAAA,QAAAwH,IAAAD,UAAA,IAAAA,UAAA,GACnE,IACQD,GACAD,EAAM,CACFQ,MAAO,gBACPC,YAAa,uBACbC,OAAQ,UACRC,SAAU,EACVC,YAAY,IAGpB,MAAM7J,QAAeqI,EAAc5J,8BAgBnC,OAbAkK,EAAgB,GAChBa,KAGIN,GACAD,EAAM,CACFQ,MAAO,gBACPC,YAAa,uBACbC,OAAQ,QACRC,SAAU,EACVC,YAAY,IAGb7J,CACX,CACA,MAAOF,GAaH,MAZAR,QAAQC,IAAI,mCAAoCO,GAE5CoJ,GACAD,EAAM,CACFQ,MAAO,QACPC,YAAa,6BACbC,OAAQ,QACRC,SAAU,EACVC,YAAY,IAId/J,CACV,CACJ,CAEA,SAASsK,EAAcC,GACnB,IAAIC,EAAiBD,EAQrB,OALAC,EAAiBA,EAAeC,QAAQ,aAAc,IAAIA,QAAQ,OAAQ,KAG1ED,EAAiBA,EAAeE,cAEzBF,CACX,CAEAnH,eAAesH,EAAUC,GAA6C,IAAjCC,EAAsBxB,UAAAvH,OAAA,QAAAwH,IAAAD,UAAA,IAAAA,UAAA,GAUnDyB,SATqBC,EAAAA,EAAOC,KAAK,CACjCC,MAAOhD,EACPiD,SAAU,CAAC,CACPC,KAAM,OACNC,QAASR,OAImBZ,QACYoB,QAKhD,OAH8B,GAA1BP,IACAC,EAA2BR,EAAcQ,IAEtCA,CACX,CAsBAzH,eAAegI,EAA2BC,EAAeC,GAGrD,IAAIX,EAAU,iDAAA3K,OAAoDqL,EAAa,gEAAArL,OAA+DsL,EAAQ,iMAEtJ/L,QAAQC,IAAI,yBAADQ,OAA0B2K,IAErC,IAAIE,QAAiCH,EAAUC,GAAY,GAQ3D,OANAE,EAA2BA,EAAyBJ,cAAc9K,OAElEJ,QAAQC,IAAI,2BAADQ,OAA4B6K,IAEZA,EAAyBU,SAAS,MAGjE,CA4BAnI,eAAeoI,EAAuBH,EAAeC,GAGjD,aA7BJlI,eAA8CiI,EAAeC,GACzD,IAAIG,EAA0B,GAE9B,IAAK,IAAIC,EAAI,EAAGA,EAAIzD,EAAwByD,IACxCD,EAAwB9J,WAAWyJ,EAA2BC,EAAeC,IAGjF,IAAIK,EAA2B,EAC/B,IAAK,IAAIC,EAAI,EAAGA,EAAIH,EAAwB5J,OAAQ+J,IAG3B,GAFDH,EAAwBG,KAGxCD,GAA4B,GAIpC,IAEIE,GAAuB,EAK3B,OAJIF,EAHYG,SAASC,KAAKC,MAAMP,EAAwB5J,OAAS,MAIjEgK,GAAuB,GAGpBA,CACX,CAKiBI,CAA+BZ,EAAeC,EAC/D,CAEA,SAASY,EAAeC,GACpB,IAAIC,EAAe,GAEnB7M,QAAQC,IAAI,sBAADQ,cAA+BmM,IAE1C,IAAK,IAAIT,EAAI,EAAGA,EAAIS,EAAatK,OAAQ6J,IAAK,CAC1C,IAAIW,EAAUF,EAAaT,GAEvBA,GAAMS,EAAatK,OAAS,EAC5BuK,GAAY,MAAApM,OAAUqM,EAAO,KAG7BD,GAAY,GAAApM,OAAOqM,EAAO,KAElC,CAMA,OAAOD,CACX,CAEA,SAASE,EAAyBH,GAC9B,IAAIC,EAAe,GAMnB,OAJAD,EAAa1H,SAAQ4H,IACjBD,GAAY,MAAApM,OAAUqM,EAAS,IAG5BD,CACX,CAEA,SAASG,EAAmBC,GACxB,IAAIC,EAAe,CAAC,EAChBC,EAAW,EACXC,EAAkB,KAGtB,IAAK,IAAIC,KAAOJ,EACD,MAAPI,IAEJH,EAAaG,IAAQH,EAAaG,IAAQ,GAAK,EAG3CH,EAAaG,GAAOF,IACpBA,EAAWD,EAAaG,GACxBD,EAAkBC,IAI1B,OAAOD,CACX,CAsBAvJ,eAAeyJ,EAAqBxB,EAAeyB,GAC/C,IAAIC,EAA8BT,EAAyBQ,GAE3DC,EAA8BA,EAA4BtC,cAAc9K,OAExE,IAAIgL,EAAU,8CAAA3K,OAAiDqL,EAAa,0CAAArL,OAAyC+M,EAA2B,+EAEhJxN,QAAQC,IAAI,qBAADQ,OAAsB2K,IAEjC,IASIE,SATqBC,EAAAA,EAAOC,KAAK,CACjCC,MAAOhD,EACPiD,SAAU,CAAC,CACPC,KAAM,OACNC,QAASR,OAImBZ,QACYoB,QAEhDN,EAA2BA,EAAyBJ,cAAc9K,OAElEJ,QAAQC,IAAI,uBAADQ,OAAwB6K,IAEnC,IAAImC,EAAc,KAElB,IAAK,IAAItB,EAAI,EAAGA,EAAIoB,EAAqBjL,OAAQ6J,IAAK,CAClD,IAAIW,EAAUS,EAAqBpB,GAAGjB,cAAc9K,OACpD,GAAIkL,IAA6BwB,EAAS,CACtC9M,QAAQC,IAAI,gBAEZD,QAAQC,IAAI,4BAA6BqL,GACzCtL,QAAQC,IAAI,WAAY6M,GAExBW,EAActB,EACd,KACJ,CACK,GAAIb,EAAyBU,SAASc,GAAU,CACjD9M,QAAQC,IAAI,oBAEZD,QAAQC,IAAI,4BAA6BqL,GACzCtL,QAAQC,IAAI,WAAY6M,GAExBW,EAActB,EACd,KACJ,CACJ,CAIA,OAFAnM,QAAQC,IAAI,eAAgBwN,GAErBA,CACX,CAEA5J,eAAe6J,EAAyB5B,EAAeyB,GACnD,IAAII,EAAmB,GACvB,IAAK,IAAIxB,EAAI,EAAGA,EAAIxD,EAAuBwD,IACvCwB,EAAiBvL,WAAWkL,EAAqBxB,EAAeyB,IAMpE,OAHAvN,QAAQC,IAAI,oBAAqB0N,GACfX,EAAmBW,EAGzC,CAEA9J,eAAe+J,EAAiB9B,EAAeyB,GAC3C,IAAIE,EAAc,KAEdI,EAAc,EAClB,KAAsB,MAAfJ,GAAuBI,EAAc,GAGxCJ,QAAoBC,EAAyB5B,EAAeyB,GAE5DM,GAAe,EAGnB,OAAOJ,CACX,CAEA5J,eAAeiK,EAAoChC,EAAeiC,EAAkBC,GAChF,IAAI5C,EAAU,gDAAA3K,OAAmDqL,EAAa,UAAArL,OAASsN,EAAgB,mIAAAtN,OAAkIuN,EAAc,wEAEvPhO,QAAQC,IAAI,qCAAsCmL,GAElD,IAAIE,QAAiCH,EAAUC,GAQ/C,OANAE,EAA2BA,EAAyBJ,cAAc9K,OAElEJ,QAAQC,IAAI,0BAADQ,OAA2B6K,IAEFA,CAGxC,CAEAzH,eAAeoK,EAAmBnC,EAAeoC,GAC7C,IAAI9C,EAAU,uCAAA3K,OAA0CqL,EAAa,mCAAArL,OAAkCyN,EAAiB,+LAExHlO,QAAQC,IAAI,uBAADQ,OAAwB2K,IAEnC,IAAIE,QAAiCH,EAAUC,GAAY,GAQ3D,OANAE,EAA2BA,EAAyBJ,cAAc9K,OAElEJ,QAAQC,IAAI,wBAADQ,OAAyB6K,IAERA,EAAyBU,SAAS,MAGlE,CAEAnI,eAAesK,EAAyBrC,EAAeiC,EAAkBK,GACrE,IAAIhD,EAAU,uCAAA3K,OAA0CqL,EAAa,UAAArL,OAASsN,EAAgB,uGAAAtN,OAAsG2N,EAAe,MAEnNpO,QAAQC,IAAI,wBAADQ,OAAyB2K,IAEpC,IAAIE,QAAiCH,EAAUC,GAQ/C,OANAE,EAA2BA,EAAyBJ,cAAc9K,OAElEJ,QAAQC,IAAI,0BAADQ,OAA2B6K,IAEdA,CAG5B,CAEAzH,eAAewK,EAA8BlO,GACzC,IAAImO,EAA4BvB,EAAyB/E,GAEzDsG,EAA4BA,EAA0BpD,cAAc9K,OAEpE,IAAIgL,EAAU,0CAAA3K,OAA8CN,EAAI,yCAAAM,OAAyC6N,EAAyB,MAElItO,QAAQC,IAAI,uBAADQ,OAAwB2K,IAEnC,IAAIE,QAAiCH,EAAUC,GAAY,GAE3DE,EAA2BA,EAAyBJ,cAAc9K,OAElEJ,QAAQC,IAAI,yBAADQ,OAA0B6K,IAErC,IAAImC,EAAc,KAElB,IAAK,IAAItB,EAAI,EAAGA,EAAInE,EAAwB1F,OAAQ6J,IAAK,CACrD,IAAIW,EAAU9E,EAAwBmE,GAAGjB,cAAc9K,OAIvD,GAAIkL,EAAyBU,SAASc,GAAU,CAC5C9M,QAAQC,IAAI,UAEZD,QAAQC,IAAI,4BAA6BqL,GACzCtL,QAAQC,IAAI,WAAY6M,GAExBW,EAActB,EACd,KACJ,CACJ,CAIA,OAFAnM,QAAQC,IAAI,eAAgBwN,GAErBA,CACX,CAEA5J,eAAe0K,GAAkCpO,GAC7C,IAAIwN,EAAmB,GACvB,IAAK,IAAIxB,EAAI,EAAGA,EAAIvD,EAAsBuD,IACtCwB,EAAiBvL,WAAWiM,EAA8BlO,IAM9D,OAHAH,QAAQC,IAAI,oBAAqB0N,GACfX,EAAmBW,EAGzC,CAEA9J,eAAe2K,GAA0BrO,GACrC,IAAIsN,EAAc,KAEdI,EAAc,EAClB,KAAsB,MAAfJ,GAAuBI,EAAc,GAGxCJ,QAAoBc,GAAkCpO,GAEtD0N,GAAe,EAGnB,OAAOJ,CACX,CAiBA,SAASgB,KAXT,IAAiCpO,EAY7B0H,EAAmB,CACf,GAhBG2G,OAAOC,aAiBV,UAdyBtO,EAcWW,KAAKC,MAbtC,IAAID,KAAKX,GAAWuO,eAAe,QAAS,CAC/CC,KAAM,UACNC,MAAO,UACPC,IAAK,UACLC,KAAM,UACNC,OAAQ,UACRC,OAAQ,aAQR,UAAa,CAAC,EACd,WAAc,CACV,aAAgB,GAChB,eAAkB,KAI1BlP,QAAQC,IAAI,yBAA0BkP,KAAKC,UAAUrH,EAAkB,KAAM,GACjF,CAkuBA,SAASsH,GAA4BC,GACjC,IAAIC,EAAiB,KACjBC,EAAuBC,IAG3B,IAAK,IAAItD,EAAI,EAAGA,EAAI5D,EAAqBjG,OAAQ6J,IAAK,CAClD,MAAMuD,EAAwBnH,EAAqB4D,GAC7CwD,EAAoBD,EAAsB,GAG1CE,EAAgBpD,KAAKqD,IAAIF,EAAoBL,GAG/CM,EAAgBJ,IAChBA,EAAuBI,EACvBL,EAAiBG,EAEzB,CAEA,OAAOH,CACX,CAoYA1L,eAAeiM,GAAenG,GAC1B3J,QAAQC,IAAI,uBACZD,QAAQC,IAAI,0BAA2BgJ,GAEvCG,GAAuB,QAKjBI,EAAM,KAEkB,GAA1BP,GACAjJ,QAAQC,IAAI,yDAEZ0J,EAAM,CACFQ,MAAO,iBACPC,YAAa,wDACbC,OAAQ,UACRC,SAAU,EACVC,YAAY,IA3oCxB1G,eAAsD8F,GAA0C,IAAnCC,EAAwBC,UAAAvH,OAAA,QAAAwH,IAAAD,UAAA,IAAAA,UAAA,GACjFY,IACAgE,KACAsB,KACA,IAAIC,EAAiBhH,EAAkC,qBAEjDU,EAAWsG,EAAgBrG,GAAO,SAElCD,EAAWV,EAAgC,aAAGW,GAAO,GAE3D5B,EAA6B,WAAgB,aAAC,GAAAtH,OAAMuP,EAAc,KAAAvP,OAAIuI,EAAgC,cAEtG,IAAIiH,EAAiBjH,EAAkC,eAEnDkH,EAA6B,CAAC,EAElC,IAAK,IAAI/D,EAAI,EAAGA,EAAI8D,EAAe3N,SAC3B8G,EADmC+C,IAAK,CAG5C,IAAIgE,EAAwBF,EAAe9D,GAEvCiE,EAAqBD,EAA4B,KACrDnQ,QAAQC,IAAI,yBAA0BmQ,GAEtCF,EAAiC,KAAIE,EACrCF,EAAyC,aAAI,CAAC,EAE9C,IAAIG,EAA0BF,EAAiC,UAE3DG,EAAeH,EAAoC,mBAEjDzG,EAAW4G,EAAc3G,GAAO,GAEtC,IAAI4G,EAAYJ,EAAiC,UAC7CK,EAAmBL,EAAwC,iBAC3D5C,EAAuB4C,EAA2C,oBAClEM,EAA6BN,EAAiD,0BAC9EO,EAAqBP,EAA0C,mBAEnEnQ,QAAQC,IAAI,aAAcsQ,GAC1BvQ,QAAQC,IAAI,sBAAuByQ,GAEnC,IAAIC,EAAsB,EAE1BT,EAAsD,0BAAIS,EAG1D,IAAK,IAAItE,EAAI,EAAGA,EAAIkE,EAAUjO,SACtB8G,EAD8BiD,IAAK,CAGvC,IAAIuE,EAAyB,EAEzB7C,EAAmBwC,EAAUlE,GAC7BwE,EAA0BL,EAAiBnE,GAC3CyE,EAA+BvD,EAAqBlB,GACpD0E,EAAqCN,EAA2BpE,GAChE2E,EAA6BN,EAAmBrE,GAEpD6D,EAAyC,aAAE,IAADzP,OAAK4L,IAAO,CAAC,EAEvD6D,EAAyC,aAAE,IAADzP,OAAK4L,IAAqB,eAAI0B,EAExEmC,EAAyC,aAAE,IAADzP,OAAK4L,IAAsB,gBAAIwE,EAEzEX,EAAyC,aAAE,IAADzP,OAAK4L,IAAoB,cAAI,CAAC,EAExE6D,EAAyC,aAAE,IAADzP,OAAK4L,IAAsB,gBAAI,CAAC,EAE1E6D,EAAyC,aAAE,IAADzP,OAAK4L,IAAgB,UAAI,CAAC,EAEpE,IAAI4E,EAA4BlD,EAE5BmD,EAA2BvE,EAAemE,GAA8B5F,cAAc9K,OAuB1F,GArBA2N,EAAgB,GAAAtN,OAAMwQ,EAAyB,KAAAxQ,OAAIyQ,SAK7CxH,EAAWuH,EAA2BtH,GAAO,SAE7CD,EAAWwH,EAA0BvH,GAAO,GAElDuG,EAAyC,aAAE,IAADzP,OAAK4L,IAAgB,UAAE,GAAD5L,OAAImQ,EAAsB,YAAa7C,EACvGmC,EAAyC,aAAE,IAADzP,OAAK4L,IAAgB,UAAE,GAAD5L,OAAImQ,EAAsB,WAAY,GAEtGV,EAAyC,aAAE,IAADzP,OAAK4L,IAAoB,cAAE,GAAD5L,OAAImQ,EAAsB,YAAa,GAC3GV,EAAyC,aAAE,IAADzP,OAAK4L,IAAoB,cAAE,GAAD5L,OAAImQ,EAAsB,WAAY,GAE1GV,EAAyC,aAAE,IAADzP,OAAK4L,IAAsB,gBAAE,GAAD5L,OAAImQ,EAAsB,YAAa,GAC7GV,EAAyC,aAAE,IAADzP,OAAK4L,IAAsB,gBAAE,GAAD5L,OAAImQ,EAAsB,WAAY,GAE5GV,EAAyC,aAAE,IAADzP,OAAK4L,IAA2B,qBAAIyE,EAC9EZ,EAAyC,aAAE,IAADzP,OAAK4L,IAAiC,2BAAI0E,EAEhF3H,EAAsB,MAE1B,IACIkD,GAAuB,EACvB6E,GAA8B,EAE9BC,EAAgC,KAChCC,EAAe,EACfC,EAAmB,EAEvB,OAAQhF,GAA0D,MAAjC8E,KACzBhI,GADiE,CAGrE,IAAI0C,EAAgB,GAEpB,MAAQA,IACA1C,GADe,CAGnBd,GAAwB,EAGxBwD,QAAsBjB,EAAyBlB,GAAO,GAEtD,IAAI4H,EAA0BzF,EAAczL,UAmB5C,GAlBAL,QAAQC,IAAI,2BAA4BsR,GAExCjJ,GAAwB,EACxBtI,QAAQC,IAAI,iCAAkCsI,GAE1CA,EAAqBjG,OAAS,IAC9B6F,EAAkCkH,GAA4BkC,GAC9DvR,QAAQC,IAAI,mCAAoCkI,IAGpDI,EAAuB,GAEvBvI,QAAQC,IAAI,gCAAiCsI,GAE7CuD,EAAgBA,EAAc3L,KAE9BH,QAAQC,IAAI,iBAAkB6L,GAE1B1C,EAAsB,MAE1B,IAAK0C,EAAe,CAKhB,GAJA9L,QAAQC,IAAI,+BAEZD,QAAQC,IAAI,oBAAqBoR,GAE7BA,GAAgBC,EAWhB,OAVAD,EAAe,QAQT3H,EAAW,6UAA8UC,GAAO,QACtWvE,KAGC,CACDiM,GAAgB,EAEhB,IAAIG,EAAa,kFACX9H,EAAW8H,EAAY7H,GAAO,SAE9BD,EAAWuH,EAA2BtH,GAAO,SAE7CD,EAAWwH,EAA0BvH,GAAO,EACtD,CACJ,CAEA,GAAIP,EAAsB,KAC9B,CAEA,IAAK0C,EACD,MAoBJ,GAjBAzC,EAAgB,sCAChBa,WAIMV,EAAM,KAERI,GACAD,EAAM,CACFQ,MAAO,gBACPC,YAAa,sCACbC,OAAQ,UACRC,SAAU,EACVC,YAAY,IAIhBnB,EAAsB,MAE1B,IAAIqI,EAAwB3G,EAAcgB,GAmC1C,GAlCA9L,QAAQC,IAAI,yBAA0BwR,GAEtCvB,EAAyC,aAAE,IAADzP,OAAK4L,IAAoB,cAAE,GAAD5L,OAAImQ,EAAsB,YAAa,GAC3GV,EAAyC,aAAE,IAADzP,OAAK4L,IAAoB,cAAE,GAAD5L,OAAImQ,EAAsB,WAAY,GAE1GV,EAAyC,aAAE,IAADzP,OAAK4L,IAAsB,gBAAE,GAAD5L,OAAImQ,EAAsB,YAAa,GAC7GV,EAAyC,aAAE,IAADzP,OAAK4L,IAAsB,gBAAE,GAAD5L,OAAImQ,EAAsB,WAAY,GAE5GV,EAAyC,aAAE,IAADzP,OAAK4L,IAAgB,UAAE,GAAD5L,OAAImQ,EAAsB,YAAa7C,EACvGmC,EAAyC,aAAE,IAADzP,OAAK4L,IAAgB,UAAE,GAAD5L,OAAImQ,EAAsB,WAAYa,EAyBlGrI,EAAsB,MAM1B,GAJAkD,QAA6BL,EAAuBwF,EAAuB1D,GAE3E/N,QAAQC,IAAI,wBAAyBqM,GAEjClD,EAAsB,MA0B1B,GAJA+H,QAAoClF,EAAuBwF,EAAuBZ,GAElF7Q,QAAQC,IAAI,+BAAgCkR,GAExC/H,EAAsB,MAE1B,IAAIsI,EAAgCrF,EAAI,GAAK,EAE7C,GAAIC,GAAwB6E,EAA6B,CACrDC,QAAsCxD,EAAiB6D,EAAuBV,GAC9E/Q,QAAQC,IAAI,iCAAkCmR,GAE9C,IAAIpD,EAAiB+C,EAAmCK,GAExD,GAAIhI,EAAsB,MAE1B,IAAIuI,QAAkDnD,GAA0BT,GAC5E6D,QAAiDpD,GAA0BiD,GAE3EI,EAA6B7J,EAAwB2J,GACrDG,EAA4B9J,EAAwB4J,GAiBxD,GAfA5R,QAAQC,IAAI,8BAADQ,OAA+B4L,EAAC,KAAKwF,GAChD7R,QAAQC,IAAI,8BAADQ,OAA+B4L,EAAC,KAAKyF,GAEhD5B,EAAyC,aAAE,IAADzP,OAAK4L,IAAoB,cAAE,GAAD5L,OAAImQ,EAAsB,YAAaiB,EAC3G3B,EAAyC,aAAE,IAADzP,OAAK4L,IAAoB,cAAE,GAAD5L,OAAImQ,EAAsB,WAAYkB,EAE1G5B,EAAyC,aAAE,IAADzP,OAAK4L,IAAgB,UAAE,GAAD5L,OAAImQ,EAAsB,YAAa7C,EACvGmC,EAAyC,aAAE,IAADzP,OAAK4L,IAAgB,UAAE,GAAD5L,OAAImQ,EAAsB,WAAYa,EAElGtJ,IACA+H,EAAyC,aAAE,IAADzP,OAAK4L,IAAsB,gBAAE,GAAD5L,OAAImQ,EAAsB,WAAYzI,EAAgC,IAGhJyI,GAA0B,EAEtBxH,EAAsB,MAE1B,GAAqC,MAAjCgI,EAAuC,CACvC,GAAIhI,EAAsB,MAE1B,GAAIsI,EAA8B,CAE9B,IAAIK,QAAsCjE,EAAoC2D,EAAuB1D,EAAkBC,SAEjHtE,EAAWqI,EAA+BpI,GAAO,GAEvD,IAAIqI,EAAwB,GACxBC,GAAwB,EAE5B,GAAI7I,EAAsB,MAE1B,MAAQ4I,IACA5I,IAEJ4I,QAA8BnH,EAAyBlB,GAAO,GAE1DC,GACAD,EAAM,CACFQ,MAAO,gBACPC,YAAa,sCACbC,OAAQ,UACRC,SAAU,EACVC,YAAY,IAIpByH,EAAwBlH,EAAckH,EAAsB7R,OAExDiJ,IAjBuB,CAmB3B,IAAIuI,QAAkDnD,GAA0BT,GAC5E6D,QAAiDpD,GAA0BiD,GAE3EI,EAA6B7J,EAAwB2J,GACrDG,EAA4B9J,EAAwB4J,GAaxD,GAXA5R,QAAQC,IAAI,8BAADQ,OAA+B4L,EAAC,KAAKwF,GAChD7R,QAAQC,IAAI,8BAADQ,OAA+B4L,EAAC,KAAKyF,GAEhD5B,EAAyC,aAAE,IAADzP,OAAK4L,IAAoB,cAAE,GAAD5L,OAAImQ,EAAsB,YAAaiB,EAC3G3B,EAAyC,aAAE,IAADzP,OAAK4L,IAAoB,cAAE,GAAD5L,OAAImQ,EAAsB,WAAYkB,EAE1G5B,EAAyC,aAAE,IAADzP,OAAK4L,IAAgB,UAAE,GAAD5L,OAAImQ,EAAsB,YAAamB,EACvG7B,EAAyC,aAAE,IAADzP,OAAK4L,IAAgB,UAAE,GAAD5L,OAAImQ,EAAsB,WAAYoB,EAEtGpB,GAA0B,EAEtBxH,EAAsB,MAE1B,IAAK4I,EAAuB,CAKxB,GAJAhS,QAAQC,IAAI,uCAEZD,QAAQC,IAAI,oBAAqBoR,GAE7BA,GAAgBC,EAWhB,OAVAD,EAAe,QAQT3H,EAAW,6UAA8UC,GAAO,QACtWvE,KAGC,CACDiM,GAAgB,EAEhB,IAAIG,EAAa,kFACX9H,EAAW8H,EAAY7H,GAAO,SAE9BD,EAAWuH,EAA2BtH,GAAO,SAE7CD,EAAWwH,EAA0BvH,GAAO,EACtD,CACJ,CACJ,CAEA,IAAKqI,EAAuB,MAE5BhS,QAAQC,IAAI,yBAA0B+R,GAEtCC,QAA8BhE,EAAmB+D,EAAuBD,GAExE/R,QAAQC,IAAI,yBAA0BgS,GAEtC,IAAIC,EAAS1F,KAAKC,MAAMD,KAAK2F,SAAWtJ,EAAoBvG,QACxD8P,EAAyBvJ,EAAoBqJ,GAEjD,IAAKD,EAAuB,OAElBvI,EAAW,GAADjJ,OAAI2R,EAAsB,iCAAiCzI,GAAO,SAC5ED,EAAWqE,EAAkBpE,GAAO,GAE1CuG,EAAyC,aAAE,IAADzP,OAAK4L,IAAgB,UAAE,GAAD5L,OAAImQ,EAAsB,YAAamB,EAGvGzF,GAAuB,EACvB8E,EAAgC,KAEhC,QACJ,CAIIiB,WAEM7I,EAAM,IAEpB,CACJ,CAyBA,GAAIJ,EAAsB,MAE1B,GAAqC,MAAjCgI,EAAuC,CACvC,IAAIkB,EAAiBtB,EAA2BI,GAEhDpR,QAAQC,IAAI,kBAAmBqS,GAE/BpC,EAAyC,aAAE,IAADzP,OAAK4L,IAAkB,YAAI+E,EACrElB,EAAyC,aAAE,IAADzP,OAAK4L,IAAqB,eAAI2B,EACxEkC,EAAyC,aAAE,IAADzP,OAAK4L,IAAqB,eAAIiG,OAIbxI,GAAvD/B,EAA6B,WAAkB,eAAEoE,GACjDpE,EAA6B,WAAkB,eAAE3F,KAAK8N,GAEtDnI,EAA6B,WAAkB,eAAEoE,GAAK+D,EAE1DS,GAAuB2B,EACvBpC,EAAsD,0BAAIS,EAE1D3Q,QAAQC,IAAI,qBAADQ,OAAsB0O,KAAKC,UAAUrH,EAAkB,KAAM,IAC5E,CAGJ,CAIA,GAFA6I,GAA0B,EAEW,MAAjCQ,EAAuC,CACvCpR,QAAQC,IAAI,kCACZD,QAAQC,IAAI,2BAEZ,IAAIsS,QAA0BpE,EAAyBsD,EAAuB1D,EAAkB8C,GAE5F2B,GAAsB,EAE1B,GAAIpJ,EAAsB,MAE1B,OAAQkD,GAA0D,MAAjC8E,KACzBoB,SAEM9I,EAAW6I,EAAmB5I,GAAO,GAC3C6I,GAAsB,EAEtBzE,EAAmBwE,UAIb7I,EAAWmH,EAAyBlH,GAAO,GAEjDoE,EAAmB8C,IAGnBzH,IAfiE,CAqBrE,IAJA8G,EAAyC,aAAE,IAADzP,OAAK4L,IAAgB,UAAE,GAAD5L,OAAImQ,EAAsB,YAAa7C,EAEvGjC,EAAgB,IAERA,IACA1C,GADe,CAGnBd,GAAwB,EAIxBwD,QAAsBjB,EAAyBlB,GAAO,GAEtD,IAAI4H,EAA0BzF,EAAczL,UAmB5C,GAlBAL,QAAQC,IAAI,2BAA4BsR,GAExCjJ,GAAwB,EACxBtI,QAAQC,IAAI,iCAAkCsI,GAE1CA,EAAqBjG,OAAS,IAC9B6F,EAAkCkH,GAA4BkC,GAC9DvR,QAAQC,IAAI,mCAAoCkI,IAGpDI,EAAuB,GAEvBvI,QAAQC,IAAI,gCAAiCsI,GAE7CuD,EAAgBA,EAAc3L,KAE9BH,QAAQC,IAAI,iBAAkB6L,GAE1B1C,EAAsB,MAE1B,IAAK0C,EAAe,CAKhB,GAJA9L,QAAQC,IAAI,+BAEZD,QAAQC,IAAI,oBAAqBoR,GAE7BA,GAAgBC,EAWhB,OAVAD,EAAe,QAQT3H,EAAW,6UAA8UC,GAAO,QACtWvE,KAGC,CACDiM,GAAgB,EAEhB,IAAIG,EAAa,kFACX9H,EAAW8H,EAAY7H,GAAO,SAE9BD,EAAWuH,EAA2BtH,GAAO,SAE7CD,EAAWwH,EAA0BvH,GAAO,EACtD,CACJ,CAEA,GAAIP,EAAsB,KAC9B,CAEA,IAAK0C,EACD,MAiBJ,SAZMtC,EAAM,KAERI,GACAD,EAAM,CACFQ,MAAO,gBACPC,YAAa,sCACbC,OAAQ,UACRC,SAAU,EACVC,YAAY,IAIhBnB,EAAsB,MAuB1B,GArBAqI,EAAwB3G,EAAcgB,GACtC9L,QAAQC,IAAI,yBAA0BwR,GAEtCvB,EAAyC,aAAE,IAADzP,OAAK4L,IAAoB,cAAE,GAAD5L,OAAImQ,EAAsB,YAAa,GAC3GV,EAAyC,aAAE,IAADzP,OAAK4L,IAAoB,cAAE,GAAD5L,OAAImQ,EAAsB,WAAY,GAE1GV,EAAyC,aAAE,IAADzP,OAAK4L,IAAsB,gBAAE,GAAD5L,OAAImQ,EAAsB,YAAa,GAC7GV,EAAyC,aAAE,IAADzP,OAAK4L,IAAsB,gBAAE,GAAD5L,OAAImQ,EAAsB,WAAY,GAE5GV,EAAyC,aAAE,IAADzP,OAAK4L,IAAgB,UAAE,GAAD5L,OAAImQ,EAAsB,WAAYa,EAYlGrI,EAAsB,MAU1B,GARAkD,QAA6BL,EAAuBwF,EAAuB1D,GAE3E/N,QAAQC,IAAI,wBAAyBqM,GAErC6E,QAAoClF,EAAuBwF,EAAuBZ,GAElF7Q,QAAQC,IAAI,+BAAgCkR,GAExC/H,EAAsB,MAE1B,GAAIkD,GAAwB6E,EAA6B,CACrD,IAAIQ,QAAkDnD,GAA0BT,GAC5E6D,QAAiDpD,GAA0BiD,GAE3EI,EAA6B7J,EAAwB2J,GACrDG,EAA4B9J,EAAwB4J,GAoBxD,GAlBA5R,QAAQC,IAAI,8BAADQ,OAA+B4L,EAAC,KAAKwF,GAChD7R,QAAQC,IAAI,8BAADQ,OAA+B4L,EAAC,KAAKyF,GAEhD5B,EAAyC,aAAE,IAADzP,OAAK4L,IAAoB,cAAE,GAAD5L,OAAImQ,EAAsB,YAAaiB,EAC3G3B,EAAyC,aAAE,IAADzP,OAAK4L,IAAoB,cAAE,GAAD5L,OAAImQ,EAAsB,WAAYkB,EAEtG3J,IACA+H,EAAyC,aAAE,IAADzP,OAAK4L,IAAsB,gBAAE,GAAD5L,OAAImQ,EAAsB,WAAYzI,EAAgC,IAIhJiJ,QAAsCxD,EAAiB6D,EAAuBX,GAK9E9Q,QAAQC,IAAI,+BAAgCmR,GAExChI,EAAsB,MAE1B,GAAqC,MAAjCgI,EAAuC,CACvC,IAAIkB,EAAiBtB,EAA2BI,GAEhDpR,QAAQC,IAAI,kBAAmBqS,GAE/BpC,EAAyC,aAAE,IAADzP,OAAK4L,IAAkB,YAAI+E,EACrElB,EAAyC,aAAE,IAADzP,OAAK4L,IAAqB,eAAIyE,EAA6BM,GACrGlB,EAAyC,aAAE,IAADzP,OAAK4L,IAAqB,eAAIiG,OAIbxI,GAAvD/B,EAA6B,WAAkB,eAAEoE,GACjDpE,EAA6B,WAAkB,eAAE3F,KAAK8N,GAEtDnI,EAA6B,WAAkB,eAAEoE,GAAK+D,EAE1DS,GAAuB2B,EACvBpC,EAAsD,0BAAIS,EAE1D3Q,QAAQC,IAAI,qBAADQ,OAAsB0O,KAAKC,UAAUrH,EAAkB,KAAM,IAC5E,CACJ,CACA,GAAqC,MAAjCqJ,EAAuC,CAMvC,GALApR,QAAQC,IAAI,6BACZD,QAAQC,IAAI,wBAAyBqM,GAErCtM,QAAQC,IAAI,oBAAqBoR,GAE7BA,GAAgBC,EAWhB,OAVAD,EAAe,QAQT3H,EAAW,6UAA8UC,GAAO,QACtWvE,KAGC,CACDiM,GAAgB,EAEhB,IAAIG,EAAa,yFAEX9H,EAAW8H,EAAY7H,GAAO,SAE9BD,EAAWuH,EAA2BtH,GAAO,SAE7CD,EAAWwH,EAA0BvH,GAAO,EACtD,CACJ,CACJ,CACA,KACJ,CACJ,CAEJ,CAEA,IAAKP,UAEKM,EAAW,yDAADjJ,OAA0D2P,EAAkB,iCAAiCzG,GAAO,SAE9HD,EAAW,kBAADjJ,OAAmBkQ,EAAmB,YAAAlQ,OAAW4P,EAAuB,KAAK1G,GAAO,GAE1E,uBAAtByG,GAGA,OAFApQ,QAAQC,IAAI,2CACZD,QAAQC,IAAI,uBAAwB0Q,IAC5B,GACJ,KAAKA,GAAuB,GAAKA,GAAuB,QAE9CjH,EAAW,wFAAyFC,GAAO,GACjH,MACJ,KAAKgH,GAAuB,GAAKA,GAAuB,SAE9CjH,EAAW,oFAAqFC,GAAO,GAC7G,MACJ,KAAKgH,GAAuB,IAAMA,GAAuB,SAE/CjH,EAAW,mIAAoIC,GAAO,GAC5J,MACJ,KAAKgH,GAAuB,SAElBjH,EAAW,yHAA0HC,GAAO,GAKtK,OAEMD,EAAW,iCAAkCC,GAAO,GAC1DN,EAAgB,GAChBa,IACJ,CAgbQuI,CAAuC9I,GAAO,KAG9C3J,QAAQC,IAAI,wCAEZ0J,EAAM,CACFQ,MAAO,iBACPC,YAAa,uCACbC,OAAQ,UACRC,SAAU,EACVC,YAAY,IAlaxB1G,eAAuC8F,GAA0C,IAAnCC,EAAwBC,UAAAvH,OAAA,QAAAwH,IAAAD,UAAA,IAAAA,UAAA,GAClEY,IACAgE,KACAsB,KAEA,IAAIC,EAAiBhH,EAAkC,qBAEjDU,EAAWV,EAAkC,eAAGW,GAAO,SAIvDD,EAAWV,EAAgC,aAAGW,GAAO,GAE3D5B,EAA6B,WAAgB,aAAC,GAAAtH,OAAMuP,EAAc,KAAAvP,OAAIuI,EAAgC,cAEtG,IAAIiH,EAAiBjH,EAAkC,eAEnDkH,EAA6B,CAAC,EAElC,IAAK,IAAI/D,EAAI,EAAGA,EAAI8D,EAAe3N,SAC3B8G,EADmC+C,IAAK,CAG5C,IAAIgE,EAAwBF,EAAe9D,GAEvCiE,EAAqBD,EAA4B,KACrDnQ,QAAQC,IAAI,yBAA0BmQ,GAEtCF,EAAiC,KAAIE,EACrCF,EAAyC,aAAI,CAAC,EAE9C,IAAIG,EAA0BF,EAAiC,UAE3DG,EAAeH,EAAoC,mBAEjDzG,EAAW4G,EAAc3G,GAAO,GAEtC,IAAI4G,EAAYJ,EAAiC,UAC7C5C,EAAuB4C,EAA2C,oBAClEM,EAA6BN,EAAiD,0BAC9EO,EAAqBP,EAA0C,mBAEnEnQ,QAAQC,IAAI,aAAcsQ,GAC1BvQ,QAAQC,IAAI,uBAAwBsN,GACpCvN,QAAQC,IAAI,8BAA+BwQ,GAC3CzQ,QAAQC,IAAI,sBAAuByQ,GAEnC,IAAIC,EAAsB,EAE1BT,EAAsD,0BAAIS,EAE1D,IAAK,IAAItE,EAAI,EAAGA,EAAIkE,EAAUjO,SACtB8G,EAD8BiD,IAAK,CAGvC,IAAIuE,EAAyB,EAEzB7C,EAAmBwC,EAAUlE,GAC7ByE,EAA+BvD,EAAqBlB,GACpD0E,EAAqCN,EAA2BpE,GAChE2E,EAA6BN,EAAmBrE,GAEpD6D,EAAyC,aAAE,IAADzP,OAAK4L,IAAO,CAAC,EAEvD6D,EAAyC,aAAE,IAADzP,OAAK4L,IAAe,SAAI0B,EAElEmC,EAAyC,aAAE,IAADzP,OAAK4L,IAAoB,cAAI,CAAC,EAExE6D,EAAyC,aAAE,IAADzP,OAAK4L,IAAsB,gBAAI,CAAC,EAE1E6D,EAAyC,aAAE,IAADzP,OAAK4L,IAAgB,UAAI,CAAC,EAEpE,IAAI4E,EAA4BlD,EAE5BmD,EAA2BvE,EAAemE,GAA8B5F,cAAc9K,OAE1F2N,EAAgB,GAAAtN,OAAMwQ,EAAyB,KAAAxQ,OAAIyQ,SAK7CxH,EAAWuH,EAA2BtH,GAAO,SAE7CD,EAAWwH,EAA0BvH,GAAO,GAElDuG,EAAyC,aAAE,IAADzP,OAAK4L,IAAgB,UAAE,GAAD5L,OAAImQ,EAAsB,YAAa7C,EACvGmC,EAAyC,aAAE,IAADzP,OAAK4L,IAAgB,UAAE,GAAD5L,OAAImQ,EAAsB,WAAY,GAEtGV,EAAyC,aAAE,IAADzP,OAAK4L,IAAoB,cAAE,GAAD5L,OAAImQ,EAAsB,YAAa,GAC3GV,EAAyC,aAAE,IAADzP,OAAK4L,IAAoB,cAAE,GAAD5L,OAAImQ,EAAsB,WAAY,GAE1GV,EAAyC,aAAE,IAADzP,OAAK4L,IAAsB,gBAAE,GAAD5L,OAAImQ,EAAsB,YAAa,GAC7GV,EAAyC,aAAE,IAADzP,OAAK4L,IAAsB,gBAAE,GAAD5L,OAAImQ,EAAsB,WAAY,GAE5GV,EAAyC,aAAE,IAADzP,OAAK4L,IAA2B,qBAAIyE,EAC9EZ,EAAyC,aAAE,IAADzP,OAAK4L,IAAiC,2BAAI0E,EAEpF,IAAIzE,GAAuB,EACvB8E,EAAgC,KAChCC,EAAe,EACfC,EAAmB,EAEvB,OAAQhF,GAA0D,MAAjC8E,KACzBhI,GADiE,CAGrE,IAAI0C,EAAgB,GAEpB,MAAQA,IACA1C,GADe,CAGnBd,GAAwB,EAIxBwD,QAAsBjB,EAAyBlB,GAAO,GAEtD,IAAI4H,EAA0BzF,EAAczL,UAmB5C,GAlBAL,QAAQC,IAAI,2BAA4BsR,GAExCjJ,GAAwB,EACxBtI,QAAQC,IAAI,iCAAkCsI,GAE1CA,EAAqBjG,OAAS,IAC9B6F,EAAkCkH,GAA4BkC,GAC9DvR,QAAQC,IAAI,mCAAoCkI,IAGpDI,EAAuB,GAEvBvI,QAAQC,IAAI,gCAAiCsI,GAE7CuD,EAAgBA,EAAc3L,KAE9BH,QAAQC,IAAI,iBAAkB6L,GAE1B1C,EAAsB,MAE1B,IAAK0C,EAAe,CAIhB,GAHA9L,QAAQC,IAAI,+BACZD,QAAQC,IAAI,iBAAkB6L,GAE1BuF,GAAgBC,EAAkB,CAClCD,EAAe,EAQf,IAAIG,EAAa,6UAIjB,aAHM9H,EAAW8H,EAAY7H,GAAO,QAEpCvE,IAEJ,CACK,CACDiM,GAAgB,EAGhB,IAAIG,EAAa,kFACX9H,EAAW8H,EAAY7H,GAAO,SAE9BD,EAAWuH,EAA2BtH,GAAO,SAE7CD,EAAWwH,EAA0BvH,GAAO,EACtD,CACJ,CAEA,GAAIP,EAAsB,KAC9B,CAEA,IAAK0C,EACD,MAoBJ,SAfMtC,EAAM,KAEZH,EAAgB,sCAChBa,KAEIN,GACAD,EAAM,CACFQ,MAAO,gBACPC,YAAa,sCACbC,OAAQ,UACRC,SAAU,IACVC,YAAY,IAIhBnB,EAAsB,MAE1B,IAAIqI,EAAwB3G,EAAcgB,GAgB1C,GAfA9L,QAAQC,IAAI,yBAA0BwR,GAEtCvB,EAAyC,aAAE,IAADzP,OAAK4L,IAAoB,cAAE,GAAD5L,OAAImQ,EAAsB,YAAa,GAC3GV,EAAyC,aAAE,IAADzP,OAAK4L,IAAoB,cAAE,GAAD5L,OAAImQ,EAAsB,WAAY,GAE1GV,EAAyC,aAAE,IAADzP,OAAK4L,IAAsB,gBAAE,GAAD5L,OAAImQ,EAAsB,YAAa,GAC7GV,EAAyC,aAAE,IAADzP,OAAK4L,IAAsB,gBAAE,GAAD5L,OAAImQ,EAAsB,WAAY,GAE5GV,EAAyC,aAAE,IAADzP,OAAK4L,IAAgB,UAAE,GAAD5L,OAAImQ,EAAsB,YAAa7C,EACvGmC,EAAyC,aAAE,IAADzP,OAAK4L,IAAgB,UAAE,GAAD5L,OAAImQ,EAAsB,WAAYa,EAEtGnF,QAA6BL,EAAuBwF,EAAuB1D,GAE3E/N,QAAQC,IAAI,wBAAyBqM,GAEjClD,EAAsB,MAE1B,IAAIsJ,EAA2BrG,EAAI,GAAK,EAExC,GAAIC,EAAsB,CAEtBtM,QAAQC,IAAI,qCAAsC8Q,GAElDK,QAAsCxD,EAAiB6D,EAAuBV,GAC9E/Q,QAAQC,IAAI,iCAAkCmR,GAE9C,IAAIpD,EAAiB+C,EAAmCK,GAExD,GAAIhI,EAAsB,MAE1B,IAAIuI,QAAkDnD,GAA0BT,GAC5E6D,QAAiDpD,GAA0BiD,GAE3EI,EAA6B7J,EAAwB2J,GACrDG,EAA4B9J,EAAwB4J,GAcxD,GAZA5R,QAAQC,IAAI,8BAADQ,OAA+B4L,EAAC,KAAKwF,GAChD7R,QAAQC,IAAI,8BAADQ,OAA+B4L,EAAC,KAAKyF,GAEhD5B,EAAyC,aAAE,IAADzP,OAAK4L,IAAoB,cAAE,GAAD5L,OAAImQ,EAAsB,YAAaiB,EAC3G3B,EAAyC,aAAE,IAADzP,OAAK4L,IAAoB,cAAE,GAAD5L,OAAImQ,EAAsB,WAAYkB,EAEtG3J,IACA+H,EAAyC,aAAE,IAADzP,OAAK4L,IAAsB,gBAAE,GAAD5L,OAAImQ,EAAsB,WAAYzI,EAAgC,IAGhJyI,GAA0B,EAEtBxH,EAAsB,MAE1B,GAAqC,MAAjCgI,EAAuC,CACvC,GAAIhI,EAAsB,MAI1B,IAAI8I,EAAS1F,KAAKC,MAAMD,KAAK2F,SAAWtJ,EAAoBvG,QACxD8P,EAAyBvJ,EAAoBqJ,GAE7CQ,SAEMhJ,EAAW,GAADjJ,OAAI2R,EAAsB,yCAAyCzI,GAAO,GAG9F,IAAI2I,EAAiBtB,EAA2BI,GAEhDpR,QAAQC,IAAI,kBAAmBqS,GAE/BpC,EAAyC,aAAE,IAADzP,OAAK4L,IAAkB,YAAI+E,EACrElB,EAAyC,aAAE,IAADzP,OAAK4L,IAAqB,eAAI2B,EACxEkC,EAAyC,aAAE,IAADzP,OAAK4L,IAAqB,eAAIiG,OAIbxI,GAAvD/B,EAA6B,WAAkB,eAAEoE,GACjDpE,EAA6B,WAAkB,eAAE3F,KAAK8N,GAEtDnI,EAA6B,WAAkB,eAAEoE,GAAK+D,EAE1DS,GAAuB2B,EACvBpC,EAAsD,0BAAIS,EAE1D3Q,QAAQC,IAAI,qBAADQ,OAAsB0O,KAAKC,UAAUrH,EAAkB,KAAM,IAC5E,KACK,CAID,GAHA/H,QAAQC,IAAI,+CACZD,QAAQC,IAAI,iCAAkCmR,GAE1CC,GAAgBC,EAAkB,CAClCD,EAAe,EAQf,IAAIG,EAAa,6UAKjB,aAHM9H,EAAW8H,EAAY7H,GAAO,QAEpCvE,IAEJ,CACK,CACDiM,GAAgB,EAEhB,IAAIG,EAAa,yFAEX9H,EAAW8H,EAAY7H,GAAO,SAE9BD,EAAWuH,EAA2BtH,GAAO,SAE7CD,EAAWwH,EAA0BvH,GAAO,EACtD,CACJ,CACJ,KACK,CAID,GAHA3J,QAAQC,IAAI,6BACZD,QAAQC,IAAI,wBAAyBqM,GAEjC+E,GAAgBC,EAAkB,CAClCD,EAAe,EAQf,IAAIG,EAAa,6UAKjB,aAHM9H,EAAW8H,EAAY7H,GAAO,QAEpCvE,IAEJ,CACK,CACDiM,GAAgB,EAEhB,IAAIG,EAAa,yFAEX9H,EAAW8H,EAAY7H,GAAO,SAE9BD,EAAWuH,EAA2BtH,GAAO,SAE7CD,EAAWwH,EAA0BvH,GAAO,EACtD,CACJ,CAEA,GAAIP,EAAsB,KAC9B,CACJ,CAEA,IAAKA,UAEKM,EAAW,yDAADjJ,OAA0D2P,EAAkB,iCAAiCzG,GAAO,SAE9HD,EAAW,kBAADjJ,OAAmBkQ,EAAmB,YAAAlQ,OAAW4P,EAAuB,KAAK1G,GAAO,GAE1E,uBAAtByG,GAGA,OAFApQ,QAAQC,IAAI,2CACZD,QAAQC,IAAI,uBAAwB0Q,IAC5B,GACJ,KAAKA,GAAuB,GAAKA,GAAuB,QAE9CjH,EAAW,wFAAyFC,GAAO,GACjH,MACJ,KAAKgH,GAAuB,GAAKA,GAAuB,SAC9CjH,EAAW,oFAAqFC,GAAO,GAC7G,MACJ,KAAKgH,GAAuB,IAAMA,GAAuB,SAE/CjH,EAAW,mIAAoIC,GAAO,GAC5J,MACJ,KAAKgH,GAAuB,SAElBjH,EAAW,yHAA0HC,GAAO,GAKtK,CACKP,SAEKM,EAAW,iDAAkDC,GAAO,SAGxED,EAAW,iCAAkCC,GAAO,GAC1DN,EAAgB,GAChBa,IACJ,CAqCQyI,CAAwBhJ,GAAO,GAEvC,CA+CA,SAASiJ,KACL,IAAIC,EAAU,CACV,CACI,GAAM,KACN,UAAa,mBACb,SAAY,IACZ,YAAe,IAEnB,CACI,GAAM,KACN,UAAa,mBACb,SAAY,IACZ,YAAe,IAEnB,CACI,GAAM,KACN,UAAa,mBACb,SAAY,IACZ,YAAe,MAzB3B,SAAiBA,GAEbA,EAAQ3N,SAAQ9B,IAAoD,IAAnD,GAAE0P,EAAE,UAAEC,EAAS,SAAEzI,EAAQ,YAAE0I,EAAc,IAAI5P,EAC1D5E,iBAAiByU,eAAeH,EAAgB,GAAZC,EAAgBzI,EAAU,EAAG0I,EAAY,GAErF,CAwBIE,CAAkBL,EACtB,CAEA,SAASM,KAA8C,IAAlCC,EAAQvJ,UAAAvH,OAAA,QAAAwH,IAAAD,UAAA,GAAAA,UAAA,GAAG,GAAIwJ,EAAUxJ,UAAAvH,OAAA,QAAAwH,IAAAD,UAAA,GAAAA,UAAA,GAAG,GAG7C7J,QAAQC,IAAI,YAAamT,GAGzBE,EAAAA,EAAgBpO,SAASqO,IACrBvT,QAAQC,IAAI,iBAAkBsT,EAAGT,IAC7BM,EAASpH,SAASuH,EAAGT,IACrB9S,QAAQC,IAAI,kBAAmBsT,EAAGT,IAItCtU,iBAAiBgV,cAAcD,EAAGT,GAAI,EAAG,EAAG,GAAIO,EAAY,GAAG,IAGnEI,EAAAA,GAAYvO,SAASwO,IACjBlV,iBAAiBmV,qBAAqBD,EAAOZ,GAAI,EAAG,IAAI,IAG5DF,IACJ,CAEA/O,eAAewO,KACXc,GAAY,CAAC,KAAM,KAAM,KAAM,aAExB3J,EAAM,KAEbhL,iBAAiBgV,cAAc,KAAM,GAAI,EAAG,GAAI,EAAG,UAE5ChK,EAAM,KAEb2J,GAAY,CAAC,KAAM,KAAM,KAAM,MAAO,EAC1C,CAiBAtP,eAAe+P,GAAMpV,EAAkBqV,EAAUC,GAW7C,GAVA9T,QAAQ+T,QAER/T,QAAQC,IAAI,kBAEZkT,GAAY,CAAC,KAAM,KAAM,KAAM,OAE3B/K,GACAjB,cAAciB,IAGb0L,IAAiBA,EAAajP,QAE/B,YADA7E,QAAQoB,MAAM,+BAIbiH,IACDA,EAAO2L,EAAAA,WAA0BF,EAAajP,UAGlDsE,GAAkB,EAElBnJ,QAAQC,IAAI,YAAa4T,GAEzB5K,EAAyB4K,EAASI,yBAzHtC,SAAmBzV,EAAkBqV,GAEjC7T,QAAQC,IAAI,oBAAqB4T,EAASK,eAC1ClU,QAAQC,IAAI,gCAAiC4T,EAASK,eACtDlU,QAAQC,IAAI,qBAAsB4T,EAASK,cAAcC,SAIrDpL,EADA8K,EAASK,eAAkD,iBAA1BL,EAASK,cAC1B,IAAIE,EAAAA,EAAc5V,EAAkBqV,EAASK,cAAe,UAG5D,IAAIE,EAAAA,EAAc5V,EAAkBqV,EAASK,cAAcC,QAAS,SAE5F,CAiHIE,CAAU7V,EAAkBqV,GAE5B,MAAMS,EAAcA,KAChB,MAAM3K,GAAQ4K,EAAAA,EAAAA,MAp0EtB,SAAmB/V,EAAkBqV,EAAUxL,GAE3C/E,EAAekR,EAAAA,YAEfnM,EAAKoM,QACDhN,EAAAA,EAAAA,KAACiN,EAAAA,EAAa,CACVlW,iBAAkBA,EAClBqV,SAAUA,EACVxQ,kBAAmBiG,EACnBhG,aAAcA,IAG1B,CA0zEQqR,CAAUnW,EAAkBqV,EAAUxL,GAjD9C,SAAuB7J,GAEf4J,GACAjB,cAAciB,GAIlBA,EAAgBpB,aAAY,KAExBxI,EAAiByU,eAAe,KAAM,IAAK,IAAK,GAEhDlT,YAAW,IAAMvB,EAAiByU,eAAe,KAAM,EAAG,IAAK,IAAI,IAAI,GACxEzK,EAAa,IACpB,CAqCQoM,CAAcpW,GACdsR,GAAenG,EAAM,EAGzBtB,EAAKoM,QAAOhN,EAAAA,EAAAA,KAAC6M,EAAW,IAC5B,CAEA,SAASlP,GAAK5G,EAAkBqV,EAAUC,GACtC9T,QAAQC,IAAI,iBAjzERqD,GAAgBA,EAAauB,SAC7BvB,EAAauB,UAGbwD,IACAA,EAAKwM,UACLxM,EAAO,MA+yEXgB,EAAgB,GAChBa,KAEI9B,GACAjB,cAAciB,GAGdc,IACAC,GAAkB,EAClBL,EAAY1D,OACZpF,QAAQC,IAAI,gCAGhBmJ,GAAuB,CAC3B,CAKA,SAAS2G,KACL,IAAK+E,SAASC,eAAe,aAAc,CACvC,MAAMC,EAAMF,SAASG,cAAc,OACnCD,EAAIlC,GAAK,YACTkC,EAAI1N,MAAMC,SAAW,QACrByN,EAAI1N,MAAM4N,OAAS,OACnBF,EAAI1N,MAAMO,KAAO,MACjBmN,EAAI1N,MAAM6N,UAAY,mBACtBH,EAAI1N,MAAMzB,MAAQ,MAClBmP,EAAI1N,MAAM8N,gBAAkB,OAC5BJ,EAAI1N,MAAM+N,MAAQ,OAClBL,EAAI1N,MAAMgO,QAAU,MACpBN,EAAI1N,MAAMiO,UAAY,SACtBP,EAAI1N,MAAMkO,OAAS,OACnBR,EAAI1N,MAAMmO,SAAW,OACrBT,EAAI1N,MAAMoO,aAAe,MACzBZ,SAASa,KAAKC,YAAYZ,EAC9B,CACJ,CAEA,SAAS9K,KACL,MAAM8K,EAAMF,SAASC,eAAe,aAChCC,IAC6B,KAAzB3L,EAAcjJ,QACd4U,EAAIa,UAAY,GAChBb,EAAI1N,MAAMwO,QAAU,QAEpBd,EAAIa,UAAYxM,EAChB2L,EAAI1N,MAAMwO,QAAU,KAGhC,C","sources":["VISOS/action/verbalizers/SpeechManager.js","components/FaceDetection.jsx","modules/miLLM.js"],"sourcesContent":["import { AudioConfig, PropertyId, ResultReason, SpeakerAudioDestination, SpeechConfig, SpeechRecognizer, SpeechSynthesizer } from \"microsoft-cognitiveservices-speech-sdk\";\n\nclass SpeechManager {\n    constructor(animationManager, apiKey, region) {\n        this.animationManager = animationManager;\n        this.apiKey = apiKey,\n        this.region = region,\n        this.queue = [];\n        this.isSpeaking = false;\n        this.audioInterrupt = false;\n\n        this.speechConfig = SpeechConfig.fromSubscription(this.apiKey, this.region)\n\n        // Initialize synthesizer\n        this.initSynthesizer();\n    }\n\n    recognizeSpeechUntilSilence() {\n        return new Promise((resolve, reject) => {\n            const audioConfig = AudioConfig.fromDefaultMicrophoneInput();\n            const recognizer = new SpeechRecognizer(this.speechConfig, audioConfig);\n    \n            let recognizedText = '';\n            let silenceTimer = null;\n            const silenceThreshold = 3000; // Silence threshold in milliseconds (3 seconds)\n            let finalRecognitionTimestamp = null; // To store the timestamp of the final recognition event\n            \n            // Start the silence timer immediately\n            silenceTimer = setTimeout(() => {\n                console.log('Silence detected (before recognition), stopping recognition...');\n                recognizer.stopContinuousRecognitionAsync(() => {\n                    resolve({\n                        text: recognizedText.trim(), \n                        timestamp: finalRecognitionTimestamp\n                    }); // Resolve with any recognized text (may be empty if no recognition)\n                });\n            }, silenceThreshold);\n    \n            // Handle partial results\n            recognizer.recognizing = (s, e) => {\n                console.log('Recognizing event triggered');\n                console.log(`Partial result: ${e.result.text}`);\n                \n                // If there's speech, reset the silence timer\n                clearTimeout(silenceTimer);\n                silenceTimer = setTimeout(() => {\n                    console.log('Silence detected during recognition, stopping...');\n                    recognizer.stopContinuousRecognitionAsync(() => {\n                        resolve({\n                            text: recognizedText.trim(),\n                            timestamp: finalRecognitionTimestamp\n                        }); // Resolve with the recognized text and timestamp\n                    });\n                }, silenceThreshold);\n            };\n    \n            // Handle final recognition results\n            recognizer.recognized = (s, e) => {\n                console.log('Recognized event triggered');\n                if (e.result.reason === ResultReason.RecognizedSpeech) {\n                    console.log(`Final result: ${e.result.text}`);\n                    recognizedText += e.result.text + ' ';\n                    finalRecognitionTimestamp = Date.now(); // Capture the timestamp of the final result\n                } else if (e.result.reason === ResultReason.NoMatch) {\n                    console.log('No match found.');\n                }\n            };\n    \n            // Handle cancellation events (e.g., when the recognition is interrupted)\n            recognizer.canceled = (s, e) => {\n                console.error('Canceled event triggered');\n                console.error(`Recognition canceled: ${e.reason}`);\n                recognizer.stopContinuousRecognitionAsync(() => {\n                    reject(new Error(`Recognition canceled: ${e.reason}`));\n                });\n            };\n    \n            // Handle session stopped event (e.g., when recognition ends)\n            recognizer.sessionStopped = (s, e) => {\n                console.log('Session stopped due to silence');\n                recognizer.stopContinuousRecognitionAsync(() => {\n                    resolve({\n                        text: recognizedText.trim(), \n                        timestamp: finalRecognitionTimestamp\n                    }); // Return the accumulated text and final timestamp\n                });\n            };\n    \n            console.log('Starting continuous recognition...');\n            recognizer.startContinuousRecognitionAsync(\n                () => console.log('Recognition started successfully'),\n                (err) => {\n                    console.error('Error starting recognition:', err);\n                    reject(err);\n                }\n            );\n        });\n    }\n    \n    initSynthesizer() {\n        this.speechConfig.speechSynthesisVoiceName = \"en-US-JennyNeural\";\n\n        const player = new SpeakerAudioDestination();;\n        const audioConfig = AudioConfig.fromSpeakerOutput(player);\n\n        this.synthesizer = new SpeechSynthesizer(this.speechConfig, audioConfig);\n\n        this.synthesizer.visemeReceived = (s, e) => {\n            this.scheduleVisemeApplication(e.visemeId, e.audioOffset);\n        };\n    }\n\n    enqueueText(text) {\n        this.queue.push(text);\n        if (!this.isSpeaking) {\n            this.processQueue();\n        }\n    }\n\n    processQueue() {\n        if (this.queue.length === 0) {\n            this.isSpeaking = false;\n            return;\n        }\n\n        this.isSpeaking = true;\n        const text = this.queue.shift();\n        this.synthesizeSpeech(text).then(() => this.processQueue());\n    }\n\n    synthesizeSpeech(text) {\n        return new Promise((resolve, reject) => {\n            this.synthesizer.speakTextAsync(text,\n                result => {\n                    console.log(\"Speech synthesis completed.\");\n                    resolve(result);\n                },\n                error => {\n                    console.log(\"Error during speech synthesis, restarting\");\n                    reject(error);\n                });\n        });\n    }\n\n    scheduleVisemeApplication(visemeId, audioOffset) {\n        const offsetInMilliseconds = audioOffset / 10000;\n        setTimeout(() => {\n            this.applyVisemeToCharacter(visemeId);\n        }, offsetInMilliseconds);\n    }\n\n    applyVisemeToCharacter(visemeId) {\n        const facsLib = this.animationManager.facsLib;\n        // console.log('audioInterrupt:', this.audioInterrupt)\n        if (!this.audioInterrupt){\n            if (visemeId === 0) {\n                facsLib.setNeutralViseme(0.0);\n            } else {\n                visemeId -= 1;\n                facsLib.setTargetViseme(visemeId, 70, 0);\n            }\n        }\n        else{\n            this.animationManager.setFaceToNeutral()\n            facsLib.setNeutralViseme()\n        }\n\n        facsLib.updateEngine();\n    }\n\n    stopSpeech() {\n        // Dispose of the current synthesizer to stop speech\n        this.synthesizer.close();\n\n        // Clear the queue and reset the speaking state\n        this.queue = [];\n        this.isSpeaking = false;\n\n        // Reinitialize the synthesizer for future use\n        this.initSynthesizer();\n\n        console.log(\"Speech synthesis stopped.\");\n    }\n\n    interruptSpeech(text) {\n        // Dispose of the current synthesizer to stop speech\n        this.synthesizer.close();\n\n        // Clear the queue, reset the speaking state, and reinitialize the synthesizer\n        this.queue = [];\n        this.isSpeaking = false;\n        this.initSynthesizer();\n\n        if (text) {\n            this.enqueueText(text); // Enqueue and play the new text\n        }\n\n        console.log(\"Speech synthesis interrupted.\");\n    }\n}\n\nexport default SpeechManager;\n","import React, { useRef, useEffect } from 'react';\nimport * as faceapi from 'face-api.js';\n\nconst FaceDetection = ({ onEmotionDetected, stopVideoRef }) => {\n  const videoRef = useRef(); // Reference to the video element\n  const canvasRef = useRef(); // Reference to the canvas element\n  const intervalRef = useRef(); // Reference to store the interval ID\n  const streamRef = useRef(); // Reference to store the video stream\n\n  // Function to load Face API models\n  useEffect(() => {\n    const loadModels = async () => {\n      try {\n        const MODEL_URL = process.env.PUBLIC_URL + '/models';\n        await faceapi.nets.tinyFaceDetector.loadFromUri(MODEL_URL);\n        await faceapi.nets.faceLandmark68Net.loadFromUri(MODEL_URL);\n        await faceapi.nets.faceRecognitionNet.loadFromUri(MODEL_URL);\n        await faceapi.nets.faceExpressionNet.loadFromUri(MODEL_URL);\n\n        startVideo(); // Start webcam after models are loaded\n      } catch (error) {\n        console.error('Error loading models:', error);\n      }\n    };\n\n    loadModels();\n  }, []);\n\n  // Function to start the webcam stream\n  const startVideo = () => {\n    navigator.mediaDevices\n      .getUserMedia({ video: {} })\n      .then((stream) => {\n        streamRef.current = stream; // Store the stream reference\n        videoRef.current.srcObject = stream; // Attach the webcam stream to the video element\n      })\n      .catch((err) => console.error('Error accessing webcam:', err));\n  };\n\n  // Function to stop the webcam stream\n  const stopVideo = () => {\n    if (streamRef.current) {\n      streamRef.current.getTracks().forEach((track) => track.stop()); // Stop all tracks of the stream\n    }\n  };\n\n  // Detect faces and draw bounding boxes\n  const detectFace = async () => {\n    if (!videoRef.current || videoRef.current.paused || videoRef.current.ended) {\n      return; // Stop detection if video is not ready\n    }\n\n    const detections = await faceapi.detectAllFaces(videoRef.current, new faceapi.TinyFaceDetectorOptions())\n      .withFaceLandmarks()\n      .withFaceExpressions();\n\n    const canvas = canvasRef.current;\n    if (!canvas) {\n      console.error('Canvas is not available');\n      return;\n    }\n\n    const displaySize = { width: videoRef.current.videoWidth, height: videoRef.current.videoHeight };\n    faceapi.matchDimensions(canvas, displaySize);\n    const resizedDetections = faceapi.resizeResults(detections, displaySize);\n\n    canvas.getContext('2d').clearRect(0, 0, canvas.width, canvas.height);\n    faceapi.draw.drawDetections(canvas, resizedDetections);\n    faceapi.draw.drawFaceLandmarks(canvas, resizedDetections);\n    faceapi.draw.drawFaceExpressions(canvas, resizedDetections);\n\n    // Log the detected emotions\n    if (detections.length > 0) {\n      const expressions = detections[0].expressions;\n      // console.log('expressions:', expressions);\n    \n      // Create a list of tuples (emotion, confidence, timestamp)\n      const emotionTuples = Object.entries(expressions).map(([emotion, confidence]) => [emotion, confidence, Date.now()]);\n    \n      // console.log('Emotion tuples:', emotionTuples);\n    \n      // Sort the emotion tuples based on confidence in descending order\n      const sortedEmotionTuples = emotionTuples.sort((a, b) => b[1] - a[1]);\n    \n      // Create a list of emotion strings ordered by confidence\n      // const orderedEmotions = sortedEmotionTuples.map(tuple => tuple[0]);\n    \n      // console.log('Ordered emotions based on confidence:', orderedEmotions);\n    \n      // Get the emotion with the highest confidence\n      // const [emotion, confidence] = sortedEmotionTuples[0];\n    \n      // console.log('FaceDetection current emotion:', emotion, 'with confidence:', confidence);\n    \n      // Trigger the callback if provided\n      if (onEmotionDetected) {\n        onEmotionDetected(sortedEmotionTuples); // Pass both emotion and confidence to the callback\n      }\n    }\n  };\n\n  // Detect faces every 100 milliseconds\n  useEffect(() => {\n    const handlePlay = () => {\n      if (!intervalRef.current) {\n        intervalRef.current = setInterval(detectFace, 100);\n      }\n    };\n\n    const videoElement = videoRef.current;\n    videoElement?.addEventListener('play', handlePlay);\n\n    // Pass stopVideo function back to parent through ref\n    if (stopVideoRef) {\n      stopVideoRef.current = stopVideo;\n    }\n\n    return () => {\n      clearInterval(intervalRef.current); // Cleanup interval on unmount\n      intervalRef.current = null;\n      videoElement?.removeEventListener('play', handlePlay);\n    };\n  }, []);\n\n  return (\n    <div style={{ position: 'relative' }}>\n      <video ref={videoRef} autoPlay muted width=\"480\" height=\"270\" style={{ position: 'relative' }} />\n      <canvas ref={canvasRef} style={{ position: 'absolute', left: 0, top: 0 }} />\n    </div>\n  );\n};\n\nexport default FaceDetection;\n","// AU Imports\nimport { ActionUnitsList, VisemesList } from \"../unity/facs/shapeDict\";\n\n// FER Imports\nimport ReactDOMClient from 'react-dom/client';\nimport React, { useRef } from 'react';\nimport FaceDetection from '../components/FaceDetection';\n\n// LLM Imports\nimport ollama from 'ollama'\n\n// TTS Imports\nimport SpeechManager from \"../VISOS/action/verbalizers/SpeechManager\";\n\n// Visualization Imports\nimport { useToast } from '@chakra-ui/react';\n\n// Data Capture vars\nlet conversation_log = null\n\n// ER vars\nconst universal_emotions_list = ['surprised', 'disgusted', 'fearful', 'sad', 'angry', 'happy', 'neutral']\n\n// FER vars\nlet root = null;\nlet stopVideoRef = null;\nlet lastEmotionTime;\nlet topEmotionTuple;\n\nlet captureEmotionTrigger = true;\nlet capturedEmotionsList = [];\n\nlet latestFacialEmotionTupleOnSpeak;\n\n// Blinking Animation vars\nlet blinkInterval\nconst blinkSpeed = 4000\n\n// LLM vars\nlet llm_model_name = 'mistral' // ollama run mistral\nlet llm_iter_relevancy_num = 3\nlet llm_iter_matching_num = 3\nlet llm_iter_emotion_num = 3\n\n// NLP vars\nconst acknowledge_phrases = ['Understood', 'Got it', 'Okay', 'Alright', 'Right']\n\n// STT vars\nlet recognitionAvailable = false;\nlet recognition;\nlet lastTranscript = '';\n\nlet processingRequest = false;\nlet recognitionStop = false;\n\n// TTS vars\nlet speechManager;\n\n// Visualization vars\n// let toast = useToast()\n\n// Intervention vars\nlet intervention_dict;\nlet interventionFullStop = false;\n\nlet activeListeningTrigger;\n//Text Var\nlet bottomBoxText = \"Default text\";\n\n\n// Callback function to handle detected emotions\nconst handleEmotionDetected = (sortedEmotionsTuples) => {\n    // console.log('Detected sortedEmotionsTuples:', sortedEmotionsTuples);\n\n    topEmotionTuple = sortedEmotionsTuples[0]\n    lastEmotionTime = topEmotionTuple[2]\n\n    if (captureEmotionTrigger == true) {\n        capturedEmotionsList.push(topEmotionTuple)\n\n        // console.log('capturedEmotionsList:', capturedEmotionsList)\n    }\n\n    // console.log('Top Emotion Tuple:', topEmotionTuple)\n};\n\nfunction enableFER(animationManager, settings, root) {\n    // Create a ref to pass to FaceDetection for stopping video\n    stopVideoRef = React.createRef();\n\n    root.render(\n        <FaceDetection\n            animationManager={animationManager}\n            settings={settings}\n            onEmotionDetected={handleEmotionDetected}\n            stopVideoRef={stopVideoRef} // Pass the stopVideoRef to FaceDetection\n        />\n    );\n}\n\n// Function to continuously check the latest detected emotion\nfunction monitorFER() {\n    const checkInterval = 1000; // Check every second\n\n    setInterval(() => {\n        if (topEmotionTuple && lastEmotionTime) {\n            const currentTime = Date.now();\n            const timeSinceLastEmotion = currentTime - lastEmotionTime;\n\n            console.log(\"Latest Detected Emotion:\", topEmotionTuple);\n            // console.log(`Time since last detected emotion: ${timeSinceLastEmotion / 1000} seconds`);\n        }\n    }, checkInterval);\n}\n\nfunction stopFER() {\n    if (stopVideoRef && stopVideoRef.current) {\n        stopVideoRef.current(); // Call stopVideo from FaceDetection\n    }\n\n    if (root) {\n        root.unmount();\n        root = null;\n    }\n}\n\nfunction delay(ms) {\n    return new Promise(resolve => setTimeout(resolve, ms));\n}\n\nasync function agentSpeak(text, toast, toastNotificationTrigger = false) {\n    speechManager.initSynthesizer();\n    let synthesize_result = await speechManager.synthesizeSpeech(text);\n\n    const delayTime = synthesize_result.audioDuration / 10000\n\n    bottomBoxText = text;\n    updateBottomBoxText();\n\n    if (toastNotificationTrigger)\n        toast({\n            title: 'eEVA Response',\n            description: text,\n            status: 'info',\n            duration: 0,\n            isClosable: false,\n        });\n\n    console.log('synthesize_result object:', synthesize_result);\n    console.log('synthesize result transcript:', text)\n    console.log('synthesize result audioDuration:', synthesize_result.audioDuration);\n\n    console.log('before delay!');\n\n    try {\n        await delay(delayTime);\n        console.log('after delay!');\n    } catch (error) {\n        if (error.message === 'Delay aborted') {\n            console.log('Delay was aborted');\n        } else {\n            console.error('Error during delay:', error);\n        }\n    }\n}\n\nfunction userDrivenDemo() {\n    if ('webkitSpeechRecognition' in window) {\n        console.log('SpeechRecognition Available!')\n        processingRequest = false;\n\n        let transactions = {}\n\n        console.log('recognitionStop:', recognitionStop)\n\n        if (!recognitionStop) {\n            recognitionAvailable = true;\n            recognitionStop = false\n\n            // Initialize SpeechRecognition object\n            const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;\n            recognition = new SpeechRecognition();\n            console.log('SPEECH RECOGNITION INTIALIZED!')\n\n            // SpeechRecognition options\n            recognition.continuous = true;\n            recognition.lang = 'en-US'\n            recognition.interimResults = true\n            recognition.maxAlternatives = 1\n\n            recognition.onresult = async function (event) {\n                console.log('IN RECOGNITION ONRESULT!')\n                let transcript = '';\n                let isFinal = false;\n\n                let current_transaction = {}\n\n                for (let i = event.resultIndex; i < event.results.length; i++) {\n                    transcript += event.results[i][0].transcript;\n                    if (event.results[i].isFinal) {\n                        isFinal = true;\n                    }\n                }\n\n                console.log('processingRequest:', processingRequest)\n\n                // Process only final results to avoid duplicates\n                if (isFinal && transcript !== lastTranscript && !processingRequest) {\n                    lastTranscript = transcript;\n                    processingRequest = true;\n\n                    // Process transcript\n                    let processTranscript = transcript.toLowerCase().trim();\n\n                    console.log('Recognized Speech:', processTranscript)\n                    console.log(\"Latest Detected Emotion:\", topEmotionTuple);\n\n                    if (processTranscript) {\n                        let user_prompt = ''\n\n                        if (topEmotionTuple) {\n                            let lastEmotionTimeThreshold = 1000\n\n                            let emotionTimePassed = Date.now() - lastEmotionTime\n\n                            // console.log('emotionTimePassed:', emotionTimePassed)\n\n                            let emotionTimeCheck = emotionTimePassed > lastEmotionTimeThreshold\n\n                            // console.log('emotionTimeCheck:', emotionTimeCheck)\n\n                            if (!(emotionTimeCheck))\n                                user_prompt += ('User Emotion: ' + topEmotionTuple[0] + '\\n')\n\n                        }\n\n                        user_prompt += ('User Prompt: ' + processTranscript + '\\n')\n\n                        user_prompt += ('Context: You are a motivational interviewing therapist and are to only answer questions related to this topic. Please provide responses as a sentence to be read by a Text to Speech engine.\\n')\n\n                        user_prompt += ('Response: ')\n\n                        console.log('full user prompt:\\n', user_prompt)\n\n                        let llm_response = await ollama.chat({\n                            model: llm_model_name,\n                            messages: [{\n                                role: 'user',\n                                content: user_prompt\n                            }]\n                        })\n\n                        console.log('llm response:', llm_response)\n\n                        let response_text = llm_response.message\n\n                        console.log('response_text:', response_text)\n\n                        let response_text_role = response_text.role\n\n                        let response_text_content = response_text.content\n\n                        await agentSpeak(response_text_content)\n\n                        let transaction_text = (user_prompt + response_text_content)\n\n                        console.log('transaction_text:\\n' + transaction_text)\n                    }\n\n                    processingRequest = false\n                }\n            };\n            recognition.onend = function () {\n                if (!recognitionStop) {\n                    console.log('Speech recognition restarting...');\n                    if (recognitionAvailable) {\n                        recognition.start();  // Restart recognition\n                        console.log('Speech recognition restarted!');\n                    }\n                }\n            };\n        }\n    }\n    else {\n        console.error('SpeechRecognition not available in this browser!')\n    }\n\n    if (recognitionAvailable) {\n        recognition.start()\n        console.log('SPEECH RECOGNITION STARTED!')\n    }\n}\n\nfunction loadIntervention() {\n    const audit_questionnaire_dict = {\n        \"name\": \"AUDIT questionnaire\",\n        \"max_score\": 40,\n        \"introduction\": \"Let's start the AUDIT questionnaire. This questionnaire is used to determine if this program is worth your time and effort. Let's begin.\",\n\n        //This version is from the original AUDIT questionnaire\n        // \"questions\": [\n        //     \"How often do you have a drink containing alcohol?\",\n        //     \"How many drinks containing alcohol do you have on a typical day when you are drinking?\",\n        //     \"How often do you have four or more drinks on one occasion?\",\n        //     \"How often during the last year have you found that you were not able to stop drinking once you had started?\",\n        //     \"How often during the last year have you failed to do what was normally expected from you because of drinking?\",\n        //     \"How often during the last year have you needed a first drink in the morning to get yourself going after a heavy drinking session?\",\n        //     \"How often during the last year have you had a feeling of guilt or remorse after drinking?\",\n        //     \"How often during the last year have you been unable to remember what happened the night before because you had been drinking?\",\n        //     \"Have you or someone else been injured as a result of your drinking?\",\n        //     \"Has a relative or friend, or a doctor or other health worker been concerned about your drinking or suggested you cut down?\",\n        // ],\n        // \"response_categories\": [\n        //     [\"Never\", \"Monthly\", \"Two to four times a month\", \"Two to three times a week\", \"Four or more times a week\"],\n        //     [\"1 or 2\", \"3 or 4\", \"5 or 6\", \"7 to 9\", \"10 or more\"], // Modified to add the 0 category\n        //     [\"Never\", \"Less than monthly\", \"Monthly\", \"Weekly\", \"Daily\", \"almost daily\"],\n        //     [\"Never\", \"Less than monthly\", \"Monthly\", \"Weekly\", \"Daily\", \"almost daily\"],\n        //     [\"Never\", \"Less than monthly\", \"Monthly\", \"Weekly\", \"Daily\", \"almost daily\"],\n        //     [\"Never\", \"Less than monthly\", \"Monthly\", \"Weekly\", \"Daily\", \"almost daily\"],\n        //     [\"Never\", \"Less than monthly\", \"Monthly\", \"Weekly\", \"Daily\", \"almost daily\"],\n        //     [\"Never\", \"Less than monthly\", \"Monthly\", \"Weekly\", \"Daily\", \"almost daily\"],\n        //     [\"No\", \"Yes, but not in the last year\", \"Yes, during the last year\"],\n        //     [\"No\", \"Yes, but not in the last year\", \"Yes, during the last year\"]\n        // ],\n        // \"scoring_categories\": [\n        //     [0, 1, 2, 3, 4],\n        //     [0, 1, 2, 3, 4],\n        //     [0, 1, 2, 3, 4, 4],\n        //     [0, 1, 2, 3, 4, 4],\n        //     [0, 1, 2, 3, 4, 4],\n        //     [0, 1, 2, 3, 4, 4],\n        //     [0, 1, 2, 3, 4, 4],\n        //     [0, 1, 2, 3, 4, 4],\n        //     [0, 2, 4],\n        //     [0, 2, 4]\n        // ]\n\n        // Slight modifications by Chris to work with LLMs.\n        \"questions\": [\n            \"How often do you have a drink containing alcohol?\",\n            \"How many drinks containing alcohol do you have on a typical day when you are drinking?\",\n            \"How often do you have four or more drinks on one occasion?\",\n            \"How often during the last year have you found that you were not able to stop drinking once you had started?\",\n            \"How often during the last year have you failed to do what was normally expected from you because of drinking?\",\n            \"How often during the last year have you needed a first drink in the morning to get yourself going after a heavy drinking session?\",\n            \"How often during the last year have you had a feeling of guilt or remorse after drinking?\",\n            \"How often during the last year have you been unable to remember what happened the night before because you had been drinking?\",\n            \"Have you or someone else been injured as a result of your drinking?\",\n            \"Has a relative or friend, or a doctor or other health worker been concerned about your drinking or suggested you cut down?\",\n        ],\n        \"response_categories\": [\n            [\"Never\", \"Once a month\", \"Two to four times a month\", \"Two to three times a week\", \"more than three times a week\"], // Modified to \"more than three times a week\".\n            [\"zero\", \"one or two\", \"three or four\", \"five or six\", \"seven to nine\", \"ten or more\"], // Modified to add the 0 category and move to text numbers.\n            [\"Never\", \"Less than monthly\", \"Monthly\", \"Weekly\", \"Daily\", \"almost daily\"],\n            [\"Never\", \"Less than monthly\", \"Monthly\", \"Weekly\", \"Daily\", \"almost daily\"],\n            [\"Never\", \"Less than monthly\", \"Monthly\", \"Weekly\", \"Daily\", \"almost daily\"],\n            [\"Never\", \"Less than monthly\", \"Monthly\", \"Weekly\", \"Daily\", \"almost daily\"],\n            [\"Never\", \"Less than monthly\", \"Monthly\", \"Weekly\", \"Daily\", \"almost daily\"],\n            [\"Never\", \"Less than monthly\", \"Monthly\", \"Weekly\", \"Daily\", \"almost daily\"],\n            [\"No\", \"Yes, but not in the last year\", \"Yes, during the last year\"],\n            [\"No\", \"Yes, but not in the last year\", \"Yes, during the last year\"]\n        ],\n        \"response_categories_match\": [\n            [\"Never\", \"Once a month\", \"two times a month\", \"three times a month\", \"four times a month\", \"two times a week\", \"three times a week\", \"more than three times a week\"],\n            [\"zero\", \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"more than ten\"], // Modified to add the 0 category and move to text numbers.\n            [\"Never\", \"Less than monthly\", \"Monthly\", \"Weekly\", \"Daily\", \"almost daily\"],\n            [\"Never\", \"Less than monthly\", \"Monthly\", \"Weekly\", \"Daily\", \"almost daily\"],\n            [\"Never\", \"Less than monthly\", \"Monthly\", \"Weekly\", \"Daily\", \"almost daily\"],\n            [\"Never\", \"Less than monthly\", \"Monthly\", \"Weekly\", \"Daily\", \"almost daily\"],\n            [\"Never\", \"Less than monthly\", \"Monthly\", \"Weekly\", \"Daily\", \"almost daily\"],\n            [\"Never\", \"Less than monthly\", \"Monthly\", \"Weekly\", \"Daily\", \"almost daily\"],\n            [\"No\", \"Yes, but not in the last year\", \"Yes, during the last year\"],\n            [\"No\", \"Yes, but not in the last year\", \"Yes, during the last year\"]\n        ],\n        \"scoring_categories\": [\n            [0, 1, 2, 2, 2, 3, 3, 4],\n            [0, 0, 0, 1, 1, 2, 2, 3, 3, 3, 4],\n            [0, 1, 2, 3, 4, 4],\n            [0, 1, 2, 3, 4, 4],\n            [0, 1, 2, 3, 4, 4],\n            [0, 1, 2, 3, 4, 4],\n            [0, 1, 2, 3, 4, 4],\n            [0, 1, 2, 3, 4, 4],\n            [0, 2, 4],\n            [0, 2, 4]\n        ]\n    }\n    const active_listening_audit_questionnaire = {\n        \"name\": \"AUDIT questionnaire\",\n        \"max_score\": 40,\n        \"introduction\": \"Let's start the AUDIT questionnaire. This questionnaire is used to determine if this program is worth your time and effort. Let's begin.\",\n        // \"introduction\": \"\",\n        \"questions\": [\n            \"How often do you have a drink containing alcohol?\",\n            \"How many drinks containing alcohol do you have on a typical day when you are drinking?\",\n            \"How often do you have four or more drinks on one occasion?\",\n            \"How often during the last year have you found that you were not able to stop drinking once you had started?\",\n            \"How often during the last year have you failed to do what was normally expected from you because of drinking?\",\n            \"How often during the last year have you needed a first drink in the morning to get yourself going after a heavy drinking session?\",\n            \"How often during the last year have you had a feeling of guilt or remorse after drinking?\",\n            \"How often during the last year have you been unable to remember what happened the night before because you had been drinking?\",\n            \"Have you or someone else been injured as a result of your drinking?\",\n            \"Has a relative or friend, or a doctor or other health worker been concerned about your drinking or suggested you cut down?\",\n        ],\n        \"target_questions\": [\n            \"How often do you have a drink containing alcohol?\",\n            \"How many drinks containing alcohol do you have on a typical day when you are drinking?\",\n            \"How often do you have four or more drinks on one occasion?\",\n            \"How often during the last year have you found that you were not able to stop drinking once you had started?\",\n            \"How often during the last year have you failed to do what was normally expected from you because of drinking?\",\n            \"How often during the last year have you needed a first drink in the morning to get yourself going after a heavy drinking session?\",\n            \"How often during the last year have you had a feeling of guilt or remorse after drinking?\",\n            \"How often during the last year have you been unable to remember what happened the night before because you had been drinking?\",\n            \"Have you or someone else been injured as a result of your drinking?\",\n            \"Has a relative or friend, or a doctor or other health worker been concerned about your drinking or suggested you cut down?\",\n        ],\n        \"response_categories\": [\n            [\"Never\", \"Once a month\", \"Two to four times a month\", \"Two to three times a week\", \"more than three times a week\"], // Modified to \"more than three times a week\".\n            [\"zero\", \"one or two\", \"three or four\", \"five or six\", \"seven to nine\", \"ten or more\"], // Modified to add the 0 category and move to text numbers.\n            [\"Never\", \"Less than monthly\", \"Monthly\", \"Weekly\", \"Daily\", \"almost daily\"],\n            [\"Never\", \"Less than monthly\", \"Monthly\", \"Weekly\", \"Daily\", \"almost daily\"],\n            [\"Never\", \"Less than monthly\", \"Monthly\", \"Weekly\", \"Daily\", \"almost daily\"],\n            [\"Never\", \"Less than monthly\", \"Monthly\", \"Weekly\", \"Daily\", \"almost daily\"],\n            [\"Never\", \"Less than monthly\", \"Monthly\", \"Weekly\", \"Daily\", \"almost daily\"],\n            [\"Never\", \"Less than monthly\", \"Monthly\", \"Weekly\", \"Daily\", \"almost daily\"],\n            [\"No\", \"Yes, but not in the last year\", \"Yes, during the last year\"],\n            [\"No\", \"Yes, but not in the last year\", \"Yes, during the last year\"]\n        ],\n        \"response_categories_match\": [\n            [\"Never\", \"Once a month\", \"two times a month\", \"three times a month\", \"four times a month\", \"two times a week\", \"three times a week\", \"more than three times a week\"],\n            [\"zero\", \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"more than ten\"], // Modified to add the 0 category and move to text numbers.\n            [\"Never\", \"Less than monthly\", \"Monthly\", \"Weekly\", \"Daily\", \"almost daily\"],\n            [\"Never\", \"Less than monthly\", \"Monthly\", \"Weekly\", \"Daily\", \"almost daily\"],\n            [\"Never\", \"Less than monthly\", \"Monthly\", \"Weekly\", \"Daily\", \"almost daily\"],\n            [\"Never\", \"Less than monthly\", \"Monthly\", \"Weekly\", \"Daily\", \"almost daily\"],\n            [\"Never\", \"Less than monthly\", \"Monthly\", \"Weekly\", \"Daily\", \"almost daily\"],\n            [\"Never\", \"Less than monthly\", \"Monthly\", \"Weekly\", \"Daily\", \"almost daily\"],\n            [\"No\", \"Yes, but not in the last year\", \"Yes, during the last year\"],\n            [\"No\", \"Yes, but not in the last year\", \"Yes, during the last year\"]\n        ],\n        \"scoring_categories\": [\n            [0, 1, 2, 2, 2, 3, 3, 4],\n            [0, 0, 0, 1, 1, 2, 2, 3, 3, 3, 4],\n            [0, 1, 2, 3, 4, 4],\n            [0, 1, 2, 3, 4, 4],\n            [0, 1, 2, 3, 4, 4],\n            [0, 1, 2, 3, 4, 4],\n            [0, 1, 2, 3, 4, 4],\n            [0, 1, 2, 3, 4, 4],\n            [0, 2, 4],\n            [0, 2, 4]\n        ]\n    }\n\n    const open_audit_questionnaire_dict = {\n        \"name\": \"Open AUDIT questionnaire\",\n        \"max_score\": 40,\n        \"introduction\": \"Let's start the Open AUDIT questionnaire. This questionnaire is used to determine if this program is worth your time and effort. Let's begin.\",\n        // \"introduction\": \"\",\n        \"questions\": [\n            \"Can you describe how frequently you typically have a drink containing alcohol?\",\n            \"What would you say is the usual number of alcoholic drinks you have on a day when you choose to drink?\",\n            \"How often do you find yourself drinking four or more drinks on one occasion?\",\n            \"Could you share any experiences from the past year where you started drinking and found it difficult to stop?\",\n            \"In the past year, have there been occasions where drinking affected your ability to meet your usual responsibilities? What was that like?\",\n            \"After a night of heavy drinking, have you ever felt the need to have a drink in the morning to get going? Could you tell me more about those situations?\",\n            \"What has your experience been with feelings of guilt or remorse after drinking over the past year?\",\n            \"Have you had times in the past year where you couldn't recall events from the night before due to drinking? How did that affect you?\",\n            \"Can you recall any situations where either you or someone else was injured as a result of your drinking within the last year?\",\n            \"Have there been any times when a relative, friend, doctor, or other health professional expressed concern about your drinking or suggested you consider cutting down within the last year? What was that experience like for you?\",\n        ],\n        \"target_questions\": [\n            \"How often do you have a drink containing alcohol?\",\n            \"How many drinks containing alcohol do you have on a typical day when you are drinking?\",\n            \"How often do you have four or more drinks on one occasion?\",\n            \"How often during the last year have you found that you were not able to stop drinking once you had started?\",\n            \"How often during the last year have you failed to do what was normally expected from you because of drinking?\",\n            \"How often during the last year have you needed a first drink in the morning to get yourself going after a heavy drinking session?\",\n            \"How often during the last year have you had a feeling of guilt or remorse after drinking?\",\n            \"How often during the last year have you been unable to remember what happened the night before because you had been drinking?\",\n            \"Have you or someone else been injured as a result of your drinking?\",\n            \"Has a relative or friend, or a doctor or other health worker been concerned about your drinking or suggested you cut down?\",\n        ],\n        \"response_categories\": [\n            [\"Never\", \"Once a month\", \"2 to 4 times a month\", \"2 to 3 times a week\", \"more than 3 times a week\"], // Modified to \"more than three times a week\".\n            [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"more than 10\"], // Modified to add the 0 category and move to text numbers.\n            [\"Never\", \"Less than monthly\", \"Monthly\", \"Weekly\", \"Daily\", \"almost daily\"],\n            [\"Never\", \"Less than monthly\", \"Monthly\", \"Weekly\", \"Daily\", \"almost daily\"],\n            [\"Never\", \"Less than monthly\", \"Monthly\", \"Weekly\", \"Daily\", \"almost daily\"],\n            [\"Never\", \"Less than monthly\", \"Monthly\", \"Weekly\", \"Daily\", \"almost daily\"],\n            [\"Never\", \"Less than monthly\", \"Monthly\", \"Weekly\", \"Daily\", \"almost daily\"],\n            [\"Never\", \"Less than monthly\", \"Monthly\", \"Weekly\", \"Daily\", \"almost daily\"],\n            [\"No\", \"Yes, but not in the last year\", \"Yes, during the last year\"],\n            [\"No\", \"Yes, but not in the last year\", \"Yes, during the last year\"]\n        ],\n        \"scoring_categories\": [\n            [0, 1, 2, 3, 4],\n            [0, 0, 0, 1, 1, 2, 2, 3, 3, 3, 4],\n            [0, 1, 2, 3, 4, 4],\n            [0, 1, 2, 3, 4, 4],\n            [0, 1, 2, 3, 4, 4],\n            [0, 1, 2, 3, 4, 4],\n            [0, 1, 2, 3, 4, 4],\n            [0, 1, 2, 3, 4, 4],\n            [0, 2, 4],\n            [0, 2, 4]\n        ]\n    }\n\n    const drinc_questionnaire_dict = {}\n\n    const simple_intervention_config = {\n        \"agent_greeting\": \"Hello, my name is eva.\",\n        \"introduction\": \"Let's go through some questionnaires to help review your drinking.\",\n        \"questionnaires\": [audit_questionnaire_dict]\n    }\n\n    const test_intervention_config = {\n        \"agent_greeting\": \"Hello, my name is eva.\",\n        \"introduction\": \"Let's go through some questionnaires to help review your drinking.\",\n        // \"introduction\": \"\",\n        // \"questionnaires\": [open_audit_questionnaire_dict]\n        \"questionnaires\": [active_listening_audit_questionnaire]\n    }\n\n    const complete_intervention_config = {\n        \"agent_greeting\": \"Hello, my name is eva.\",\n        \"introduction\": \"Let's go through some questionnaires to help review your drinking.\",\n        \"questionnaires\": [audit_questionnaire_dict, drinc_questionnaire_dict]\n    }\n\n    console.log('activeListeningTrigger:', activeListeningTrigger)\n\n    if (activeListeningTrigger == true) {\n        intervention_dict = test_intervention_config\n    }\n    else {\n        intervention_dict = simple_intervention_config\n    }\n\n    // intervention_dict = simple_intervention_config\n    // intervention_dict = test_intervention_config\n    // intervention_dict = complete_intervention_config\n\n    console.log('intervention_dict:', intervention_dict)\n}\n\nfunction captureUserResponse(toast, toastNotificationTrigger = false) {\n    return new Promise((resolve) => {\n        if ('webkitSpeechRecognition' in window) {\n            console.log('SpeechRecognition available!');\n            processingRequest = false;\n\n            console.log('recognitionStop:', recognitionStop)\n\n            if (!recognitionStop) {\n                recognitionAvailable = true;\n                recognitionStop = false;\n\n                // Initialize SpeechRecognition object\n                const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;\n                recognition = new SpeechRecognition();\n\n                recognition.continuous = false;\n                recognition.lang = 'en-US';\n                recognition.interimResults = true;\n                recognition.maxAlternatives = 1;\n\n                let silenceTimer;\n\n                lastTranscript = ''\n\n                if (toastNotificationTrigger)\n                    toast({\n                        title: 'eEVA Response',\n                        description: 'Started Listening...',\n                        status: 'success',\n                        duration: 0,\n                        isClosable: false,\n                    });\n\n                recognition.onresult = async function (event) {\n                    let transcript = '';\n                    let isFinal = false;\n\n                    for (let i = event.resultIndex; i < event.results.length; i++) {\n                        transcript += event.results[i][0].transcript;\n                        if (event.results[i].isFinal) {\n                            isFinal = true;\n                        }\n                    }\n\n                    console.log('processingRequest:', processingRequest);\n\n                    // Process only final results to avoid duplicates\n                    if (isFinal && transcript !== lastTranscript && !processingRequest) {\n                        lastTranscript = transcript;\n                        processingRequest = true;\n\n                        // Process transcript\n                        let processTranscript = transcript.toLowerCase().trim();\n                        console.log('Recognized Speech:', processTranscript);\n\n                        latestFacialEmotionTupleOnSpeak = null\n\n                        if (topEmotionTuple != undefined) {\n                            const currentTime = Date.now();\n                            const timeSinceLastEmotion = currentTime - lastEmotionTime;\n                            const timeSinceLastEmotionSeconds = timeSinceLastEmotion / 1000\n\n                            console.log('timeSinceLastEmotionSeconds:', timeSinceLastEmotionSeconds)\n\n                            if (timeSinceLastEmotionSeconds <= 1) {\n                                console.log('topEmotionTuple:', topEmotionTuple)\n\n                                const currentEmotion = topEmotionTuple[0]\n\n                                console.log('currentEmotion:', currentEmotion)\n\n                                latestFacialEmotionTupleOnSpeak = currentEmotion\n                            }\n                        }\n\n                        // Stop the timer and recognition when a final result is detected\n                        clearTimeout(silenceTimer);\n                        recognition.stop();\n\n                        // Call a function to handle the processed response\n                        processingRequest = false;\n                        resolve(transcript); // Resolve the promise to indicate processing is complete\n                    } else {\n                        // Reset silence timer if interim results are detected\n                        clearTimeout(silenceTimer);\n                        silenceTimer = setTimeout(() => {\n                            console.log('No speech detected, stopping recognition.');\n                            recognition.stop(); // Stop recognition after a period of silence\n                            resolve(transcript); // Resolve the promise to continue\n                        }, 2000); // Adjust the timeout duration as needed\n                    }\n                };\n\n                recognition.onend = () => {\n                    console.log('Speech recognition ended.');\n\n                    if (toastNotificationTrigger)\n                        toast({\n                            title: 'eEVA Response',\n                            description: 'Stopped Listening...',\n                            status: 'error',\n                            duration: 0,\n                            isClosable: false,\n                        });\n\n                    if (!lastTranscript) {\n                        resolve('')\n                    }\n                    else {\n                        console.log('lastTranscript:', lastTranscript)\n\n                        resolve(lastTranscript); // Resolve the promise when recognition ends\n                    }\n                };\n\n                recognition.onerror = (event) => {\n                    console.error('Speech recognition error:', event.error);\n                    clearTimeout(silenceTimer);\n                    resolve(lastTranscript); // Resolve even on error\n                };\n            }\n        } else {\n            console.error('SpeechRecognition not available in this browser!');\n            resolve(); // Resolve if not available\n        }\n\n        if (recognitionAvailable) {\n            recognition.start();\n            console.log(\"Speech recognition started!\");\n        }\n    });\n}\n\nasync function captureUserResponseAzure(toast, toastNotificationTrigger = false) {\n    try {\n        if (toastNotificationTrigger)\n            toast({\n                title: 'eEVA Response',\n                description: 'Started Listening...',\n                status: 'success',\n                duration: 0,\n                isClosable: false,\n            });\n\n        const result = await speechManager.recognizeSpeechUntilSilence();\n        //Shamim2\n        //clearBottomBox()\n        bottomBoxText = \"\";\n        updateBottomBoxText();\n\n\n        if (toastNotificationTrigger)\n            toast({\n                title: 'eEVA Response',\n                description: 'Stopped Listening...',\n                status: 'error',\n                duration: 0,\n                isClosable: false,\n            });\n\n        return result\n    }\n    catch (e) {\n        console.log(\"Error during speech recognition:\", e)\n\n        if (toastNotificationTrigger) {\n            toast({\n                title: \"Error\",\n                description: \"Speech recognition failed.\",\n                status: \"error\",\n                duration: 0,\n                isClosable: true,\n            });\n        }\n\n        throw e\n    }\n}\n\nfunction cleanResponse(response) {\n    let clean_response = response;\n\n    // Remove punctuation\n    clean_response = clean_response.replace(/[^\\w\\s]|_/g, \"\").replace(/\\s+/g, \" \")\n\n    // Set to lower case\n    clean_response = clean_response.toLowerCase()\n\n    return clean_response\n}\n\nasync function llmPrompt(llm_prompt, clean_response_trigger = false) {\n    let llm_response = await ollama.chat({\n        model: llm_model_name,\n        messages: [{\n            role: 'user',\n            content: llm_prompt\n        }]\n    })\n\n    let llm_response_msg = llm_response.message\n    let llm_response_msg_content = llm_response_msg.content\n\n    if (clean_response_trigger == true)\n        llm_response_msg_content = cleanResponse(llm_response_msg_content)\n\n    return llm_response_msg_content\n}\n\nasync function checkUserOKAnsweringQuestion(user_response, question) {\n    // let llm_prompt = `Please check if the user is comfortable answering the following question:  \\'${question}\\' given the following user response: \\'${user_response}\\'. Respond with yes if the user is comfortable, otherwise respond with no.`\n\n    // let llm_prompt = `Based on the user's response: '${user_response}' to the following question '${question}', assess if the user seems comfortable answering the question. Look for language that might indicate hesitation, discomfort, or reluctance. Respond with 'yes' if the user is confident and open, and 'no' if there are signs they feel hesitant or unwilling.`\n\n    let llm_prompt = `Based on the user's response '${user_response} to the following question '${question}', directly assess if the user is comfortable answering the question.  Respond with 'yes' if the user is open, and 'no' if there are signs they feel hesitant or unwilling.`\n\n    console.log(`llm ok prompt: ${llm_prompt}`)\n\n    let llm_response_msg_content = await llmPrompt(llm_prompt, true)\n\n    llm_response_msg_content = llm_response_msg_content.toLowerCase().trim();\n\n    console.log(`llm ok response: ${llm_response_msg_content}`)\n\n    let is_response_ok = llm_response_msg_content.includes('yes')\n\n    return is_response_ok\n}\n\nasync function checkResponseRelevancyOnce(user_response, question) {\n    // let llm_prompt = `Please check if the following user response: '${user_response}' answers or indirectly implies an answer to the following question: '${question}'. Consider if the response provides a relevant answer, even if its not stated directly. Respond with yes or no.`\n\n    let llm_prompt = `Please check if the following user response: '${user_response}' answers or indirectly implies an answer to the question: '${question}. Respond with \"yes\" if the response implies a relevant answer, even if it's not stated in the exact terms of the question. Respond with \"no\" only if it does not imply any relevant answer.`\n\n    console.log(`llm relevancy prompt: ${llm_prompt}`)\n\n    let llm_response_msg_content = await llmPrompt(llm_prompt, true)\n\n    llm_response_msg_content = llm_response_msg_content.toLowerCase().trim();\n\n    console.log(`llm relevancy response: ${llm_response_msg_content}`)\n\n    let is_response_relevant = llm_response_msg_content.includes('yes')\n\n    return is_response_relevant\n}\n\nasync function checkResponseRelevancyMultiple(user_response, question) {\n    let response_relevancy_list = []\n\n    for (let i = 0; i < llm_iter_relevancy_num; i++) {\n        response_relevancy_list.push(await checkResponseRelevancyOnce(user_response, question))\n    }\n\n    let response_relevancy_count = 0\n    for (let j = 0; j < response_relevancy_list.length; j++) {\n        let current_value = response_relevancy_list[j]\n\n        if (current_value == true) {\n            response_relevancy_count += 1\n        }\n    }\n\n    let threshold = parseInt(Math.floor(response_relevancy_list.length / 2))\n\n    let is_response_relevant = false\n    if (response_relevancy_count > threshold) {\n        is_response_relevant = true\n    }\n\n    return is_response_relevant\n}\n\nasync function checkResponseRelevancy(user_response, question) {\n    // return await checkResponseRelevancyOnce(user_response, question)\n\n    return await checkResponseRelevancyMultiple(user_response, question)\n}\n\nfunction stringifyArray(chosen_array) {\n    let array_string = ''\n\n    console.log(`chosen array type: ${typeof (chosen_array)}`)\n\n    for (let i = 0; i < chosen_array.length; i++) {\n        let element = chosen_array[i]\n\n        if (i == (chosen_array.length - 1)) {\n            array_string += `or ${element}.`\n        }\n        else {\n            array_string += `${element}, `\n        }\n    }\n\n    // chosen_array.forEach(element => {\n    //     array_string += `${element}, `\n    // });\n\n    return array_string\n}\n\nfunction stringifyArrayListFormat(chosen_array) {\n    let array_string = ''\n\n    chosen_array.forEach(element => {\n        array_string += `\\n*${element}`\n    });\n\n    return array_string\n}\n\nfunction mostFrequentNumber(arr) {\n    let frequencyMap = {}; // To store frequency of each number\n    let maxCount = 0;      // To store the highest frequency count\n    let mostFrequentNum = null; // To store the most frequent number\n\n    // Count the frequency of each number\n    for (let num of arr) {\n        if (num == null) continue\n\n        frequencyMap[num] = (frequencyMap[num] || 0) + 1;\n\n        // Update most frequent number if current number has a higher count\n        if (frequencyMap[num] > maxCount) {\n            maxCount = frequencyMap[num];\n            mostFrequentNum = num;\n        }\n    }\n\n    return mostFrequentNum;\n}\n\nasync function verifyLLMMatchResponse(user_response, valid_user_responses) {\n    let valid_user_responses_string = stringifyArrayListFormat(valid_user_responses)\n\n    valid_user_responses_string = valid_user_responses_string.toLowerCase().trim()\n\n    let llm_prompt = `Please check if the following user response: \\'${user_response}\\' can be directly or indirectly matched to the following valid responses: '${valid_user_responses_string}'. Respond with yes or no.`\n\n    console.log(`llm verify match prompt: ${llm_prompt}`)\n\n    let llm_response_msg_content = await llmPrompt(llm_prompt, true)\n\n    llm_response_msg_content = llm_response_msg_content.toLowerCase().trim();\n\n    console.log(`llm verify match response: ${llm_response_msg_content}`)\n\n    let is_response_match = llm_response_msg_content.includes('yes')\n\n    return is_response_match\n}\n\nasync function llmMatchResponseOnce(user_response, valid_user_responses) {\n    let valid_user_responses_string = stringifyArrayListFormat(valid_user_responses);\n\n    valid_user_responses_string = valid_user_responses_string.toLowerCase().trim();\n\n    let llm_prompt = `Please match the following user response: '${user_response}' to the following valid responses: \\n${valid_user_responses_string}\\nAlways match to the exact closest valid response. Be direct and succinct.`;\n\n    console.log(`llm match prompt: ${llm_prompt}`);\n\n    let llm_response = await ollama.chat({\n        model: llm_model_name,\n        messages: [{\n            role: 'user',\n            content: llm_prompt\n        }]\n    });\n\n    let llm_response_msg = llm_response.message;\n    let llm_response_msg_content = llm_response_msg.content;\n\n    llm_response_msg_content = llm_response_msg_content.toLowerCase().trim();\n\n    console.log(`llm match response: ${llm_response_msg_content}`);\n\n    let match_index = null\n\n    for (let i = 0; i < valid_user_responses.length; i++) {\n        let element = valid_user_responses[i].toLowerCase().trim()\n        if (llm_response_msg_content === element) {\n            console.log('EXACT MATCH!')\n\n            console.log('llm_response_msg_content:', llm_response_msg_content)\n            console.log('element:', element)\n\n            match_index = i\n            break\n        }\n        else if (llm_response_msg_content.includes(element)) {\n            console.log('SUBSTRING MATCH!')\n\n            console.log('llm_response_msg_content:', llm_response_msg_content)\n            console.log('element:', element)\n\n            match_index = i\n            break\n        }\n    }\n\n    console.log('match_index:', match_index);\n\n    return match_index;\n}\n\nasync function llmMatchResponseMultiple(user_response, valid_user_responses) {\n    let match_index_list = []\n    for (let i = 0; i < llm_iter_matching_num; i++) {\n        match_index_list.push(await llmMatchResponseOnce(user_response, valid_user_responses))\n    }\n\n    console.log('match_index_list:', match_index_list)\n    let match_index = mostFrequentNumber(match_index_list)\n\n    return match_index\n}\n\nasync function llmMatchResponse(user_response, valid_user_responses) {\n    let match_index = null\n\n    let match_count = 0\n    while (match_index == null && match_count < 2) {\n        // match_index = await llmMatchResponseOnce(user_response, valid_user_responses)\n\n        match_index = await llmMatchResponseMultiple(user_response, valid_user_responses)\n\n        match_count += 1\n    }\n\n    return match_index\n}\n\nasync function generateReflectiveListeningResponse(user_response, current_question, response_match) {\n    let llm_prompt = `Given the following relevant user response, '${user_response}' to '${current_question}', please provide a reflective listening response that acknowledges their response to the closest response match equivalent of ${response_match} and guides them to confirm your understanding of the user response.`\n\n    console.log('llm reflective listening response:', llm_prompt)\n\n    let llm_response_msg_content = await llmPrompt(llm_prompt)\n\n    llm_response_msg_content = llm_response_msg_content.toLowerCase().trim();\n\n    console.log(`llm followup response: ${llm_response_msg_content}`)\n\n    let reflective_listening_response = llm_response_msg_content\n\n    return reflective_listening_response\n}\n\nasync function llmConfirmResponse(user_response, current_statement) {\n    let llm_prompt = `Given the following user response, '${user_response}' to following agent response:'${current_statement}', check if the user is directly or indirectly responding with a form of 'yes'. Respond with 'yes' if they are and 'no' if they do not. Be direct and succinct by giving a 1 word response.`\n\n    console.log(`llm confirm prompt: ${llm_prompt}`)\n\n    let llm_response_msg_content = await llmPrompt(llm_prompt, true)\n\n    llm_response_msg_content = llm_response_msg_content.toLowerCase().trim();\n\n    console.log(`llm cofirm response: ${llm_response_msg_content}`)\n\n    let is_response_confirmed = llm_response_msg_content.includes('yes')\n\n    return is_response_confirmed\n}\n\nasync function generateFollowUpQuestion(user_response, current_question, target_question) {\n    let llm_prompt = `Given the following user response, '${user_response}' to '${current_question}', please provide a follow-up question that acknowledges their response and guides them to answer '${target_question}'.`\n\n    console.log(`llm followup prompt: ${llm_prompt}`)\n\n    let llm_response_msg_content = await llmPrompt(llm_prompt)\n\n    llm_response_msg_content = llm_response_msg_content.toLowerCase().trim();\n\n    console.log(`llm followup response: ${llm_response_msg_content}`)\n\n    let followup_question = llm_response_msg_content\n\n    return followup_question\n}\n\nasync function llmTextEmotionRecognitionOnce(text) {\n    let universal_emotions_string = stringifyArrayListFormat(universal_emotions_list)\n\n    universal_emotions_string = universal_emotions_string.toLowerCase().trim()\n\n    let llm_prompt = `Please match from the following text: \\'${text}\\' to one of the following emotions:\\n${universal_emotions_string}\\n`\n\n    console.log(`llm emotion prompt: ${llm_prompt}`)\n\n    let llm_response_msg_content = await llmPrompt(llm_prompt, true)\n\n    llm_response_msg_content = llm_response_msg_content.toLowerCase().trim();\n\n    console.log(`llm emotion response: ${llm_response_msg_content}`)\n\n    let match_index = null\n\n    for (let i = 0; i < universal_emotions_list.length; i++) {\n        let element = universal_emotions_list[i].toLowerCase().trim()\n\n        // console.log('element:', element)\n\n        if (llm_response_msg_content.includes(element)) {\n            console.log('MATCH!')\n\n            console.log('llm_response_msg_content:', llm_response_msg_content)\n            console.log('element:', element)\n\n            match_index = i\n            break\n        }\n    }\n\n    console.log('match_index:', match_index)\n\n    return match_index\n}\n\nasync function llmTextEmotionRecognitionMultiple(text) {\n    let match_index_list = []\n    for (let i = 0; i < llm_iter_emotion_num; i++) {\n        match_index_list.push(await llmTextEmotionRecognitionOnce(text))\n    }\n\n    console.log('match_index_list:', match_index_list)\n    let match_index = mostFrequentNumber(match_index_list)\n\n    return match_index\n}\n\nasync function llmTextEmotionRecognition(text) {\n    let match_index = null\n\n    let match_count = 0\n    while (match_index == null && match_count < 2) {\n        // match_index = await llmTextEmotionRecognitionOnce(text)\n\n        match_index = await llmTextEmotionRecognitionMultiple(text)\n\n        match_count += 1\n    }\n\n    return match_index\n}\n\nfunction generateLogID() {\n    return crypto.randomUUID()\n}\n\nfunction convertToDateTimeString(timestamp) {\n    return new Date(timestamp).toLocaleString('en-US', {\n        year: 'numeric',\n        month: '2-digit',\n        day: '2-digit',\n        hour: '2-digit',\n        minute: '2-digit',\n        second: '2-digit',\n    });\n}\n\nfunction initDataCapture() {\n    conversation_log = {\n        \"id\": generateLogID(),\n        \"datetime\": convertToDateTimeString(Date.now()),\n        \"user_data\": {},\n        \"transcript\": {\n            'introduction': '',\n            'questionnaires': [],\n        }\n    }\n\n    console.log('init conversation_log:', JSON.stringify(conversation_log, null, 2))\n}\n\nasync function activeListeningAgentDrivenIntervention(toast, toastNotificationTrigger = false) {\n    loadIntervention()\n    initDataCapture()\n    createBottomBox()\n    let agent_greeting = intervention_dict['agent_greeting']\n    //changed-correct\n    await agentSpeak(agent_greeting, toast, false)\n    //changed-correct\n    await agentSpeak(intervention_dict['introduction'], toast, false)\n\n    conversation_log['transcript']['introduction'] = `${agent_greeting} ${intervention_dict['introduction']}`\n\n    let questionnaires = intervention_dict['questionnaires']\n\n    let current_questionnaire_dict = {}\n\n    for (let i = 0; i < questionnaires.length; i++) {\n        if (interventionFullStop) break\n\n        let current_questionnaire = questionnaires[i]\n\n        let questionnaire_name = current_questionnaire['name']\n        console.log('current_questionnaire:', questionnaire_name)\n\n        current_questionnaire_dict['name'] = questionnaire_name\n        current_questionnaire_dict['transactions'] = {}\n\n        let max_questionnaire_score = current_questionnaire['max_score']\n\n        let introduction = current_questionnaire['introduction']\n        //changed-correct\n        await agentSpeak(introduction, toast, false)\n\n        let questions = current_questionnaire['questions']\n        let target_questions = current_questionnaire['target_questions']\n        let valid_user_responses = current_questionnaire['response_categories']\n        let valid_match_user_responses = current_questionnaire['response_categories_match']\n        let scoring_categories = current_questionnaire['scoring_categories']\n\n        console.log('questions:', questions)\n        console.log('scoring_categories:', scoring_categories)\n\n        let questionnaire_score = 0\n\n        current_questionnaire_dict['total_questionnaire_score'] = questionnaire_score\n\n\n        for (let j = 0; j < questions.length; j++) {\n            if (interventionFullStop) break\n\n            let transaction_pair_count = 0\n\n            let current_question = questions[j]\n            let current_target_question = target_questions[j]\n            let current_valid_user_responses = valid_user_responses[j]\n            let current_valid_match_user_responses = valid_match_user_responses[j]\n            let current_scoring_categories = scoring_categories[j]\n\n            current_questionnaire_dict['transactions'][`Q${j}`] = {}\n\n            current_questionnaire_dict['transactions'][`Q${j}`]['start_question'] = current_question\n\n            current_questionnaire_dict['transactions'][`Q${j}`]['target_question'] = current_target_question\n\n            current_questionnaire_dict['transactions'][`Q${j}`]['text_emotions'] = {}\n\n            current_questionnaire_dict['transactions'][`Q${j}`]['facial_emotions'] = {}\n\n            current_questionnaire_dict['transactions'][`Q${j}`]['responses'] = {}\n\n            let current_question_question = current_question\n\n            let current_question_options = stringifyArray(current_valid_user_responses).toLowerCase().trim()\n\n            current_question = `${current_question_question} ${current_question_options}`\n\n            // current_question += (' ' + stringifyArray(current_valid_user_responses).toLowerCase().trim())\n\n            //changed-correct\n            await agentSpeak(current_question_question, toast, false)\n\n            await agentSpeak(current_question_options, toast, false)\n\n            current_questionnaire_dict['transactions'][`Q${j}`]['responses'][`${transaction_pair_count}: agent`] = current_question // 0th pair count, in this case, will always be the question being asked.\n            current_questionnaire_dict['transactions'][`Q${j}`]['responses'][`${transaction_pair_count}: user`] = \"\"\n\n            current_questionnaire_dict['transactions'][`Q${j}`]['text_emotions'][`${transaction_pair_count}: agent`] = \"\"\n            current_questionnaire_dict['transactions'][`Q${j}`]['text_emotions'][`${transaction_pair_count}: user`] = \"\"\n\n            current_questionnaire_dict['transactions'][`Q${j}`]['facial_emotions'][`${transaction_pair_count}: agent`] = \"\"\n            current_questionnaire_dict['transactions'][`Q${j}`]['facial_emotions'][`${transaction_pair_count}: user`] = \"\"\n\n            current_questionnaire_dict['transactions'][`Q${j}`]['user_valid_responses'] = current_valid_user_responses // 0th pair count, in this case, will always be the question being asked.\n            current_questionnaire_dict['transactions'][`Q${j}`]['user_valid_match_responses'] = current_valid_match_user_responses\n\n            if (interventionFullStop) break\n\n            let is_user_ok_answering_question = false\n            let is_response_relevant = false\n            let is_response_relevant_target = false\n            let is_response_match = false\n            let likert_matched_response_index = null\n            let repeat_count = 0\n            let max_repeat_count = 2\n\n            while (!is_response_relevant || (likert_matched_response_index == null)) {\n                if (interventionFullStop) break\n\n                let user_response = ''\n\n                while (!user_response) {\n                    if (interventionFullStop) break\n\n                    captureEmotionTrigger = true\n                    //changed-correct\n                    // user_response = await captureUserResponse(toast, true)\n                    user_response = await captureUserResponseAzure(toast, false)\n\n                    let user_response_timestamp = user_response.timestamp\n                    console.log('user_response_timestamp:', user_response_timestamp)\n\n                    captureEmotionTrigger = false\n                    console.log('before - capturedEmotionsList:', capturedEmotionsList)\n\n                    if (capturedEmotionsList.length > 0) {\n                        latestFacialEmotionTupleOnSpeak = getLatestFacialEmotionTuple(user_response_timestamp)\n                        console.log('latestFacialEmotionTupleOnSpeak:', latestFacialEmotionTupleOnSpeak)\n                    }\n\n                    capturedEmotionsList = []\n\n                    console.log('after - capturedEmotionsList:', capturedEmotionsList)\n\n                    user_response = user_response.text\n\n                    console.log('user_response:', user_response)\n\n                    if (interventionFullStop) break\n\n                    if (!user_response) {\n                        console.log('USER RESPONSE NOT PROVIDED!')\n\n                        console.log('repeat_count - 1:', repeat_count)\n\n                        if (repeat_count >= max_repeat_count) {\n                            repeat_count = 0\n\n                            // Allow user to move on to the next question.\n                            // await agentSpeak(\"Let's move on to the next question.\", toast, true)\n                            // break\n\n                            // Terminate intervention due to exhausted attempts\n                            //changed-correct\n                            await agentSpeak(\"It seems you've used all your attempts to answer the current questions. Unfortunately, all questions must be answered as part of the intervention procedure to provide a valid evaluation. When you're ready to answer all the questions in the intervention, feel free to restart this module. Thank you for your time and participation.\", toast, false)\n                            stop()\n                            return\n                        }\n                        else {\n                            repeat_count += 1\n                            //changed-correct\n                            let speak_text = \"It seems I wasn't able to hear what you said. Let me repeat the question.\"\n                            await agentSpeak(speak_text, toast, false)\n\n                            await agentSpeak(current_question_question, toast, false)\n\n                            await agentSpeak(current_question_options, toast, false)\n                        }\n                    }\n\n                    if (interventionFullStop) break\n                }\n\n                if (!user_response) {\n                    break\n                }\n\n                bottomBoxText = 'Processing response. Please wait...';\n                updateBottomBoxText();\n\n                // console.log(`current_questionnaire_dict - 1: ${JSON.stringify(current_questionnaire_dict, null, 2)}`)\n\n                await delay(500);\n\n                if (toastNotificationTrigger) {\n                    toast({\n                        title: 'eEVA Response',\n                        description: 'Processing response. Please wait...',\n                        status: 'warning',\n                        duration: 0,\n                        isClosable: false,\n                    });\n                }\n\n                if (interventionFullStop) break\n\n                let cleaned_user_response = cleanResponse(user_response)\n                console.log('cleaned_user_response:', cleaned_user_response)\n\n                current_questionnaire_dict['transactions'][`Q${j}`]['text_emotions'][`${transaction_pair_count}: agent`] = \"\"\n                current_questionnaire_dict['transactions'][`Q${j}`]['text_emotions'][`${transaction_pair_count}: user`] = \"\"\n\n                current_questionnaire_dict['transactions'][`Q${j}`]['facial_emotions'][`${transaction_pair_count}: agent`] = \"\"\n                current_questionnaire_dict['transactions'][`Q${j}`]['facial_emotions'][`${transaction_pair_count}: user`] = \"\"\n\n                current_questionnaire_dict['transactions'][`Q${j}`]['responses'][`${transaction_pair_count}: agent`] = current_question\n                current_questionnaire_dict['transactions'][`Q${j}`]['responses'][`${transaction_pair_count}: user`] = cleaned_user_response\n\n                // let is_user_answering_question = await verifyLLMMatchResponse(user_response, current_valid_user_responses)\n\n                // console.log('is_user_answering_question:', is_user_answering_question)\n\n                // if(!is_user_answering_question){\n\n                //     is_user_ok_answering_question = await checkUserOKAnsweringQuestion(cleaned_user_response, current_question)\n\n                //     console.log('is_user_ok_answering_question:', is_user_ok_answering_question)\n\n                //     if(!is_user_ok_answering_question){\n                //         await agentSpeak(`I understand if you prefer not to answer the current question right now. Let's move on to the next question.`)\n\n                //         likert_matched_response_index = await llmMatchResponse(cleaned_user_response, current_valid_user_responses)\n\n                //         console.log('likert_matched_response_index:', likert_matched_response_index)\n\n                //         if (interventionFullStop) break\n\n                //         break\n                //     }\n                // }\n\n                if (interventionFullStop) break\n\n                is_response_relevant = await checkResponseRelevancy(cleaned_user_response, current_question)\n\n                console.log(\"is_response_relevant:\", is_response_relevant)\n\n                if (interventionFullStop) break\n\n                // This does a verification to make sure that the user is ok answering the current question.\n                // Addendum: No longer needed needed under the assumption that the user is ok with answering all questions of the study.\n                // if (!is_response_relevant){\n                //     is_user_ok_answering_question = await checkUserOKAnsweringQuestion(cleaned_user_response, current_question)\n\n                //     console.log('is_user_ok_answering_question:', is_user_ok_answering_question)\n\n                //     if(!is_user_ok_answering_question){\n                //         await agentSpeak(`I understand if you prefer not to answer the current question right now. Let's move on to the next question.`, toast, true)\n\n                //         likert_matched_response_index = await llmMatchResponse(cleaned_user_response, current_valid_user_responses)\n\n                //         console.log('likert_matched_response_index:', likert_matched_response_index)\n\n                //         if (interventionFullStop) break\n\n                //         break\n                //     }\n                // }\n\n                is_response_relevant_target = await checkResponseRelevancy(cleaned_user_response, current_target_question)\n\n                console.log(\"is_response_relevant_target:\", is_response_relevant_target)\n\n                if (interventionFullStop) break\n\n                let reflective_listening_trigger = (j % 2 == 0) // Reflective listening on every odd question.\n\n                if (is_response_relevant && is_response_relevant_target) {\n                    likert_matched_response_index = await llmMatchResponse(cleaned_user_response, current_valid_match_user_responses)\n                    console.log('likert_matched_response_index:', likert_matched_response_index)\n\n                    let response_match = current_valid_match_user_responses[likert_matched_response_index]\n\n                    if (interventionFullStop) break\n\n                    let agent_matched_text_emotion_response_index = await llmTextEmotionRecognition(current_question)\n                    let user_matched_text_emotion_response_index = await llmTextEmotionRecognition(cleaned_user_response)\n\n                    let agent_matched_text_emotion = universal_emotions_list[agent_matched_text_emotion_response_index]\n                    let user_matched_text_emotion = universal_emotions_list[user_matched_text_emotion_response_index]\n\n                    console.log(`agent_matched_text_emotion_${j}:`, agent_matched_text_emotion)\n                    console.log(`user_matched_text_emotion:_${j}:`, user_matched_text_emotion)\n\n                    current_questionnaire_dict['transactions'][`Q${j}`]['text_emotions'][`${transaction_pair_count}: agent`] = agent_matched_text_emotion\n                    current_questionnaire_dict['transactions'][`Q${j}`]['text_emotions'][`${transaction_pair_count}: user`] = user_matched_text_emotion\n\n                    current_questionnaire_dict['transactions'][`Q${j}`]['responses'][`${transaction_pair_count}: agent`] = current_question\n                    current_questionnaire_dict['transactions'][`Q${j}`]['responses'][`${transaction_pair_count}: user`] = cleaned_user_response\n\n                    if (latestFacialEmotionTupleOnSpeak) {\n                        current_questionnaire_dict['transactions'][`Q${j}`]['facial_emotions'][`${transaction_pair_count}: user`] = latestFacialEmotionTupleOnSpeak[0]\n                    }\n\n                    transaction_pair_count += 1\n\n                    if (interventionFullStop) break\n\n                    if (likert_matched_response_index != null) {\n                        if (interventionFullStop) break\n\n                        if (reflective_listening_trigger) {\n                            // Reaffirm user response through a reflection.\n                            let reflective_listening_response = await generateReflectiveListeningResponse(cleaned_user_response, current_question, response_match)\n                            //changed-correct\n                            await agentSpeak(reflective_listening_response, toast, false)\n\n                            let confirmation_response = ''\n                            let is_response_confirmed = false\n\n                            if (interventionFullStop) break\n\n                            while (!confirmation_response) {\n                                if (interventionFullStop) break\n                                //changed-correct\n                                confirmation_response = await captureUserResponseAzure(toast, false)\n\n                                if (toastNotificationTrigger) {\n                                    toast({\n                                        title: 'eEVA Response',\n                                        description: 'Processing response. Please wait...',\n                                        status: 'warning',\n                                        duration: 0,\n                                        isClosable: false,\n                                    });\n                                }\n\n                                confirmation_response = cleanResponse(confirmation_response.text)\n\n                                if (interventionFullStop) break\n\n                                let agent_matched_text_emotion_response_index = await llmTextEmotionRecognition(current_question)\n                                let user_matched_text_emotion_response_index = await llmTextEmotionRecognition(cleaned_user_response)\n\n                                let agent_matched_text_emotion = universal_emotions_list[agent_matched_text_emotion_response_index]\n                                let user_matched_text_emotion = universal_emotions_list[user_matched_text_emotion_response_index]\n\n                                console.log(`agent_matched_text_emotion_${j}:`, agent_matched_text_emotion)\n                                console.log(`user_matched_text_emotion:_${j}:`, user_matched_text_emotion)\n\n                                current_questionnaire_dict['transactions'][`Q${j}`]['text_emotions'][`${transaction_pair_count}: agent`] = agent_matched_text_emotion\n                                current_questionnaire_dict['transactions'][`Q${j}`]['text_emotions'][`${transaction_pair_count}: user`] = user_matched_text_emotion\n\n                                current_questionnaire_dict['transactions'][`Q${j}`]['responses'][`${transaction_pair_count}: agent`] = reflective_listening_response\n                                current_questionnaire_dict['transactions'][`Q${j}`]['responses'][`${transaction_pair_count}: user`] = confirmation_response\n\n                                transaction_pair_count += 1\n\n                                if (interventionFullStop) break\n\n                                if (!confirmation_response) {\n                                    console.log('CONFIRMATION RESPONSE NOT PROVIDED!')\n\n                                    console.log('repeat_count - 2:', repeat_count)\n\n                                    if (repeat_count >= max_repeat_count) {\n                                        repeat_count = 0\n\n                                        // Allow user to move on to the next question.\n                                        // await agentSpeak(\"Let's move on to the next question.\", toast, true)\n                                        // break\n\n                                        // Terminate intervention due to exhausted attempts\n                                        //changed-correct\n                                        await agentSpeak(\"It seems you've used all your attempts to answer the current questions. Unfortunately, all questions must be answered as part of the intervention procedure to provide a valid evaluation. When you're ready to answer all the questions in the intervention, feel free to restart this module. Thank you for your time and participation.\", toast, false)\n                                        stop()\n                                        return\n                                    }\n                                    else {\n                                        repeat_count += 1\n                                        //changed-correct\n                                        let speak_text = \"It seems I wasn't able to hear what you said. Let me repeat the question.\"\n                                        await agentSpeak(speak_text, toast, false)\n            \n                                        await agentSpeak(current_question_question, toast, false)\n            \n                                        await agentSpeak(current_question_options, toast, false)\n                                    }\n                                }\n                            }\n\n                            if (!confirmation_response) break\n\n                            console.log('confirmation_response:', confirmation_response)\n\n                            is_response_confirmed = await llmConfirmResponse(confirmation_response, reflective_listening_response)\n\n                            console.log('is_response_confirmed:', is_response_confirmed)\n\n                            let rand_i = Math.floor(Math.random() * acknowledge_phrases.length)\n                            let random_acknowledgement = acknowledge_phrases[rand_i]\n\n                            if (!is_response_confirmed) {\n                                //changed-correct\n                                await agentSpeak(`${random_acknowledgement}. Let me repeat the question.`, toast, false)\n                                await agentSpeak(current_question, toast, false)\n\n                                current_questionnaire_dict['transactions'][`Q${j}`]['responses'][`${transaction_pair_count}: agent`] = reflective_listening_response\n\n                                // Reset\n                                is_response_relevant = false\n                                likert_matched_response_index = null\n\n                                continue\n                            }\n                            else {\n                                // await agentSpeak(`${random_acknowledgement}. Let's move on to the next question.`, toast, true)\n\n                                headNodAffirmation()\n\n                                await delay(3000)\n                            }\n                        }\n                    }\n\n                    // let agent_matched_text_emotion_response_index = await llmTextEmotionRecognition(current_question)\n                    // let user_matched_text_emotion_response_index = await llmTextEmotionRecognition(cleaned_user_response)\n\n                    // let agent_matched_text_emotion = universal_emotions_list[agent_matched_text_emotion_response_index]\n                    // let user_matched_text_emotion = universal_emotions_list[user_matched_text_emotion_response_index]\n\n                    // console.log(`agent_matched_text_emotion_${j}:`, agent_matched_text_emotion)\n                    // console.log(`user_matched_text_emotion:_${j}:`, user_matched_text_emotion)\n\n                    // current_questionnaire_dict['transactions'][`Q${j}`]['text_emotions'][`${transaction_pair_count}: agent`] = agent_matched_text_emotion\n                    // current_questionnaire_dict['transactions'][`Q${j}`]['text_emotions'][`${transaction_pair_count}: user`] = user_matched_text_emotion\n\n                    // if (latestFacialEmotionTupleOnSpeak){\n                    //     current_questionnaire_dict['transactions'][`Q${j}`]['facial_emotions'][`${transaction_pair_count}: user`] = latestFacialEmotionTupleOnSpeak[0]\n                    // }\n\n                    // likert_matched_response_index = await llmMatchResponse(cleaned_user_response, current_valid_user_responses)\n\n                    // DEBUG\n                    // likert_matched_response_index = null\n\n                    // console.log('likert_matched_response_index:', likert_matched_response_index)\n\n                    if (interventionFullStop) break\n\n                    if (likert_matched_response_index != null) {\n                        let response_score = current_scoring_categories[likert_matched_response_index]\n\n                        console.log('response_score:', response_score)\n\n                        current_questionnaire_dict['transactions'][`Q${j}`]['match_index'] = likert_matched_response_index\n                        current_questionnaire_dict['transactions'][`Q${j}`]['response_match'] = response_match\n                        current_questionnaire_dict['transactions'][`Q${j}`]['response_score'] = response_score\n\n                        // console.log(`current_questionnaire_dict - 2: ${JSON.stringify(current_questionnaire_dict, null, 2)}`)\n\n                        if (conversation_log['transcript']['questionnaires'][i] == undefined)\n                            conversation_log['transcript']['questionnaires'].push(current_questionnaire_dict)\n                        else\n                            conversation_log['transcript']['questionnaires'][i] = current_questionnaire_dict\n\n                        questionnaire_score += response_score\n                        current_questionnaire_dict['total_questionnaire_score'] = questionnaire_score\n\n                        console.log(`conversation_log: ${JSON.stringify(conversation_log, null, 2)}`)\n                    }\n\n                    // console.log(`current_questionnaire_dict - 1: ${JSON.stringify(current_questionnaire_dict, null, 2)}`)\n                }\n\n                transaction_pair_count += 1\n\n                if (likert_matched_response_index == null) {\n                    console.log('RESPONSE IS CANNOT BE MATCHED!')\n                    console.log('CONTINUE WITH FOLLOWUP!')\n\n                    let followup_question = await generateFollowUpQuestion(cleaned_user_response, current_question, current_target_question)\n\n                    let followup_first_time = true\n\n                    if (interventionFullStop) break\n\n                    while (!is_response_relevant || (likert_matched_response_index == null)) {\n                        if (followup_first_time) {\n                            //changed-correct\n                            await agentSpeak(followup_question, toast, false)\n                            followup_first_time = false\n\n                            current_question = followup_question\n                        }\n                        else {\n                            //changed-correct\n                            await agentSpeak(current_target_question, toast, false)\n\n                            current_question = current_target_question\n                        }\n\n                        if (interventionFullStop) break\n\n                        current_questionnaire_dict['transactions'][`Q${j}`]['responses'][`${transaction_pair_count}: agent`] = current_question\n\n                        user_response = ''\n\n                        while (!user_response) {\n                            if (interventionFullStop) break\n\n                            captureEmotionTrigger = true\n\n                            // user_response = await captureUserResponse(toast, true)\n                            //changed-correct\n                            user_response = await captureUserResponseAzure(toast, false)\n\n                            let user_response_timestamp = user_response.timestamp\n                            console.log('user_response_timestamp:', user_response_timestamp)\n\n                            captureEmotionTrigger = false\n                            console.log('before - capturedEmotionsList:', capturedEmotionsList)\n\n                            if (capturedEmotionsList.length > 0) {\n                                latestFacialEmotionTupleOnSpeak = getLatestFacialEmotionTuple(user_response_timestamp)\n                                console.log('latestFacialEmotionTupleOnSpeak:', latestFacialEmotionTupleOnSpeak)\n                            }\n\n                            capturedEmotionsList = []\n\n                            console.log('after - capturedEmotionsList:', capturedEmotionsList)\n\n                            user_response = user_response.text\n\n                            console.log('user_response:', user_response)\n\n                            if (interventionFullStop) break\n\n                            if (!user_response) {\n                                console.log('USER RESPONSE NOT PROVIDED!')\n\n                                console.log('repeat_count - 2:', repeat_count)\n\n                                if (repeat_count >= max_repeat_count) {\n                                    repeat_count = 0\n\n                                    // Allow user to move on to the next question.\n                                    // await agentSpeak(\"Let's move on to the next question.\", toast, true)\n                                    // break\n\n                                    // Terminate intervention due to exhausted attempts\n                                    //changed-correct\n                                    await agentSpeak(\"It seems you've used all your attempts to answer the current questions. Unfortunately, all questions must be answered as part of the intervention procedure to provide a valid evaluation. When you're ready to answer all the questions in the intervention, feel free to restart this module. Thank you for your time and participation.\", toast, false)\n                                    stop()\n                                    return\n                                }\n                                else {\n                                    repeat_count += 1\n                                    //changed-correct\n                                    let speak_text = \"It seems I wasn't able to hear what you said. Let me repeat the question.\"\n                                    await agentSpeak(speak_text, toast, false)\n        \n                                    await agentSpeak(current_question_question, toast, false)\n        \n                                    await agentSpeak(current_question_options, toast, false)\n                                }\n                            }\n\n                            if (interventionFullStop) break\n                        }\n\n                        if (!user_response) {\n                            break\n                        }\n\n                        // console.log(`current_questionnaire_dict - 1: ${JSON.stringify(current_questionnaire_dict, null, 2)}`)\n\n                        await delay(500);\n\n                        if (toastNotificationTrigger) {\n                            toast({\n                                title: 'eEVA Response',\n                                description: 'Processing response. Please wait...',\n                                status: 'warning',\n                                duration: 0,\n                                isClosable: false,\n                            });\n                        }\n\n                        if (interventionFullStop) break\n\n                        cleaned_user_response = cleanResponse(user_response)\n                        console.log('cleaned_user_response:', cleaned_user_response)\n\n                        current_questionnaire_dict['transactions'][`Q${j}`]['text_emotions'][`${transaction_pair_count}: agent`] = \"\"\n                        current_questionnaire_dict['transactions'][`Q${j}`]['text_emotions'][`${transaction_pair_count}: user`] = \"\"\n\n                        current_questionnaire_dict['transactions'][`Q${j}`]['facial_emotions'][`${transaction_pair_count}: agent`] = \"\"\n                        current_questionnaire_dict['transactions'][`Q${j}`]['facial_emotions'][`${transaction_pair_count}: user`] = \"\"\n\n                        current_questionnaire_dict['transactions'][`Q${j}`]['responses'][`${transaction_pair_count}: user`] = cleaned_user_response\n\n                        // is_user_ok_answering_question = await checkUserOKAnsweringQuestion(cleaned_user_response, current_question)\n\n                        // console.log('is_user_ok_answering_question:', is_user_ok_answering_question)\n\n                        // if(!is_user_ok_answering_question){\n                        //     await agentSpeak(`I understand if you prefer not to answer the current question. Let's move on to the next question.`)\n\n                        //     break\n                        // }\n\n                        if (interventionFullStop) break\n\n                        is_response_relevant = await checkResponseRelevancy(cleaned_user_response, current_question)\n\n                        console.log(\"is_response_relevant:\", is_response_relevant)\n\n                        is_response_relevant_target = await checkResponseRelevancy(cleaned_user_response, current_target_question)\n\n                        console.log(\"is_response_relevant_target:\", is_response_relevant_target)\n\n                        if (interventionFullStop) break\n\n                        if (is_response_relevant || is_response_relevant_target) {\n                            let agent_matched_text_emotion_response_index = await llmTextEmotionRecognition(current_question)\n                            let user_matched_text_emotion_response_index = await llmTextEmotionRecognition(cleaned_user_response)\n\n                            let agent_matched_text_emotion = universal_emotions_list[agent_matched_text_emotion_response_index]\n                            let user_matched_text_emotion = universal_emotions_list[user_matched_text_emotion_response_index]\n\n                            console.log(`agent_matched_text_emotion_${j}:`, agent_matched_text_emotion)\n                            console.log(`user_matched_text_emotion:_${j}:`, user_matched_text_emotion)\n\n                            current_questionnaire_dict['transactions'][`Q${j}`]['text_emotions'][`${transaction_pair_count}: agent`] = agent_matched_text_emotion\n                            current_questionnaire_dict['transactions'][`Q${j}`]['text_emotions'][`${transaction_pair_count}: user`] = user_matched_text_emotion\n\n                            if (latestFacialEmotionTupleOnSpeak) {\n                                current_questionnaire_dict['transactions'][`Q${j}`]['facial_emotions'][`${transaction_pair_count}: user`] = latestFacialEmotionTupleOnSpeak[0]\n                            }\n\n\n                            likert_matched_response_index = await llmMatchResponse(cleaned_user_response, current_valid_user_responses)\n\n                            // DEBUG\n                            // likert_matched_response_index = null\n\n                            console.log('likert_match_response_index:', likert_matched_response_index)\n\n                            if (interventionFullStop) break\n\n                            if (likert_matched_response_index != null) {\n                                let response_score = current_scoring_categories[likert_matched_response_index]\n\n                                console.log('response_score:', response_score)\n\n                                current_questionnaire_dict['transactions'][`Q${j}`]['match_index'] = likert_matched_response_index\n                                current_questionnaire_dict['transactions'][`Q${j}`]['response_match'] = current_valid_user_responses[likert_matched_response_index]\n                                current_questionnaire_dict['transactions'][`Q${j}`]['response_score'] = response_score\n\n                                // console.log(`current_questionnaire_dict - 2: ${JSON.stringify(current_questionnaire_dict, null, 2)}`)\n\n                                if (conversation_log['transcript']['questionnaires'][i] == undefined)\n                                    conversation_log['transcript']['questionnaires'].push(current_questionnaire_dict)\n                                else\n                                    conversation_log['transcript']['questionnaires'][i] = current_questionnaire_dict\n\n                                questionnaire_score += response_score\n                                current_questionnaire_dict['total_questionnaire_score'] = questionnaire_score\n\n                                console.log(`conversation_log: ${JSON.stringify(conversation_log, null, 2)}`)\n                            }\n                        }\n                        if (likert_matched_response_index == null) {\n                            console.log('RESPONSE IS NOT RELEVANT!')\n                            console.log('is_response_relevant:', is_response_relevant)\n\n                            console.log('repeat_count - 3:', repeat_count)\n\n                            if (repeat_count >= max_repeat_count) {\n                                repeat_count = 0\n\n                                // Allow user to move on to the next question.\n                                // await agentSpeak(\"Let's move on to the next question.\", toast, true)\n                                // break\n\n                                // Terminate intervention due to exhausted attempts\n                                //changed-correct\n                                await agentSpeak(\"It seems you've used all your attempts to answer the current questions. Unfortunately, all questions must be answered as part of the intervention procedure to provide a valid evaluation. When you're ready to answer all the questions in the intervention, feel free to restart this module. Thank you for your time and participation.\", toast, false)\n                                stop()\n                                return\n                            }\n                            else {\n                                repeat_count += 1\n                                //changed-correct\n                                let speak_text = \"May you please rephrase and elaborate your response? Let me repeat the question.\"\n\n                                await agentSpeak(speak_text, toast, false)\n        \n                                await agentSpeak(current_question_question, toast, false)\n        \n                                await agentSpeak(current_question_options, toast, false)\n                            }\n                        }\n                    }\n                    break\n                }\n            }\n            // transaction_pair_count += 1\n        }\n        //All questions for the current questionnaire have been answered here.\n        if (!interventionFullStop) {\n            //changed-correct\n            await agentSpeak(`It appears you've answered all the questions from the ${questionnaire_name}. Let me evaluate your score.`, toast, false)\n            //changed-correct\n            await agentSpeak(`You've score a ${questionnaire_score} out of ${max_questionnaire_score}.`, toast, false)\n\n            if (questionnaire_name == 'AUDIT questionnaire') {\n                console.log('Evaluating AUDIT questionnaire score...')\n                console.log('questionnaire_score:', questionnaire_score)\n                switch (true) {\n                    case questionnaire_score >= 0 && questionnaire_score <= 7:\n                        //changed-correct\n                        await agentSpeak('This means you fall under risk level 1 and are to be provided some alcohol education.', toast, false);\n                        break;\n                    case questionnaire_score >= 8 && questionnaire_score <= 15:\n                        //changed-correct\n                        await agentSpeak('This means you fall under risk level 2 and are to be provided some simple advice.', toast, false);\n                        break;\n                    case questionnaire_score >= 16 && questionnaire_score <= 19:\n                        //changed-correct\n                        await agentSpeak('This means you fall under risk level 3 and are to be provided some simple advice plus brief counseling and continued monitoring.', toast, false);\n                        break;\n                    case questionnaire_score >= 20:\n                        //changed-correct\n                        await agentSpeak('This means you fall under risk level 4 and are to be referred to a specialist for diagnostic evaluation and treatment.', toast, false);\n                        break;\n                }\n            }\n        }\n    }\n    //changed-correct\n    await agentSpeak(`Ending intervention procedure.`, toast, false)\n    bottomBoxText = \"\"\n    updateBottomBoxText()\n}\n\nfunction getLatestFacialEmotionTuple(speech_response_timestamp) {\n    let closestEmotion = null;\n    let closestTimestampDiff = Infinity;  // Initialize with a large number to find the minimum difference\n\n    // Loop through the captured emotions list\n    for (let i = 0; i < capturedEmotionsList.length; i++) {\n        const current_emotion_tuple = capturedEmotionsList[i];\n        const current_timestamp = current_emotion_tuple[2]; // Assuming timestamp is at index 2\n\n        // Calculate the absolute difference between the current emotion timestamp and the speech response timestamp\n        const timestampDiff = Math.abs(current_timestamp - speech_response_timestamp);\n\n        // Check if this emotion timestamp is closer than the previous closest\n        if (timestampDiff < closestTimestampDiff) {\n            closestTimestampDiff = timestampDiff;\n            closestEmotion = current_emotion_tuple;\n        }\n    }\n\n    return closestEmotion;\n}\n\nasync function agentDrivenIntervention(toast, toastNotificationTrigger = false) {\n    loadIntervention()\n    initDataCapture()\n    createBottomBox()\n\n    let agent_greeting = intervention_dict['agent_greeting']\n    //changed-correct\n    await agentSpeak(intervention_dict['agent_greeting'], toast, false)\n\n    // console.log('introduction:', intervention_dict['introduction'])\n    //changed-correct\n    await agentSpeak(intervention_dict['introduction'], toast, false)\n   \n    conversation_log['transcript']['introduction'] = `${agent_greeting} ${intervention_dict['introduction']}`\n\n    let questionnaires = intervention_dict['questionnaires']\n\n    let current_questionnaire_dict = {}\n\n    for (let i = 0; i < questionnaires.length; i++) {\n        if (interventionFullStop) break\n\n        let current_questionnaire = questionnaires[i]\n\n        let questionnaire_name = current_questionnaire['name']\n        console.log('current_questionnaire:', questionnaire_name)\n\n        current_questionnaire_dict['name'] = questionnaire_name\n        current_questionnaire_dict['transactions'] = {}\n\n        let max_questionnaire_score = current_questionnaire['max_score']\n\n        let introduction = current_questionnaire['introduction']\n        //changed-correct\n        await agentSpeak(introduction, toast, false)\n       \n        let questions = current_questionnaire['questions']\n        let valid_user_responses = current_questionnaire['response_categories']\n        let valid_match_user_responses = current_questionnaire['response_categories_match']\n        let scoring_categories = current_questionnaire['scoring_categories']\n\n        console.log('questions:', questions)\n        console.log('valid_user_responses', valid_user_responses)\n        console.log('valid_match_user_responses:', valid_match_user_responses)\n        console.log('scoring_categories:', scoring_categories)\n\n        let questionnaire_score = 0\n\n        current_questionnaire_dict['total_questionnaire_score'] = questionnaire_score\n\n        for (let j = 0; j < questions.length; j++) {\n            if (interventionFullStop) break\n\n            let transaction_pair_count = 0\n\n            let current_question = questions[j]\n            let current_valid_user_responses = valid_user_responses[j]\n            let current_valid_match_user_responses = valid_match_user_responses[j]\n            let current_scoring_categories = scoring_categories[j]\n\n            current_questionnaire_dict['transactions'][`Q${j}`] = {}\n\n            current_questionnaire_dict['transactions'][`Q${j}`]['question'] = current_question\n\n            current_questionnaire_dict['transactions'][`Q${j}`]['text_emotions'] = {}\n\n            current_questionnaire_dict['transactions'][`Q${j}`]['facial_emotions'] = {}\n\n            current_questionnaire_dict['transactions'][`Q${j}`]['responses'] = {}\n\n            let current_question_question = current_question\n\n            let current_question_options = stringifyArray(current_valid_user_responses).toLowerCase().trim()\n\n            current_question = `${current_question_question} ${current_question_options}`\n\n            // current_question += (' ' + stringifyArray(current_valid_user_responses).toLowerCase().trim())\n\n            //changed-correct\n            await agentSpeak(current_question_question, toast, false)\n\n            await agentSpeak(current_question_options, toast, false)\n\n            current_questionnaire_dict['transactions'][`Q${j}`]['responses'][`${transaction_pair_count}: agent`] = current_question // 0th pair count, in this case, will always be the question being asked.\n            current_questionnaire_dict['transactions'][`Q${j}`]['responses'][`${transaction_pair_count}: user`] = \"\"\n\n            current_questionnaire_dict['transactions'][`Q${j}`]['text_emotions'][`${transaction_pair_count}: agent`] = \"\"\n            current_questionnaire_dict['transactions'][`Q${j}`]['text_emotions'][`${transaction_pair_count}: user`] = \"\"\n\n            current_questionnaire_dict['transactions'][`Q${j}`]['facial_emotions'][`${transaction_pair_count}: agent`] = \"\"\n            current_questionnaire_dict['transactions'][`Q${j}`]['facial_emotions'][`${transaction_pair_count}: user`] = \"\"\n\n            current_questionnaire_dict['transactions'][`Q${j}`]['user_valid_responses'] = current_valid_user_responses // 0th pair count, in this case, will always be the question being asked.\n            current_questionnaire_dict['transactions'][`Q${j}`]['user_valid_match_responses'] = current_valid_match_user_responses\n\n            let is_response_relevant = false\n            let likert_matched_response_index = null\n            let repeat_count = 0\n            let max_repeat_count = 2\n\n            while (!is_response_relevant || (likert_matched_response_index == null)) {\n                if (interventionFullStop) break\n\n                let user_response = ''\n\n                while (!user_response) {\n                    if (interventionFullStop) break\n\n                    captureEmotionTrigger = true\n\n                    // user_response = await captureUserResponse(toast, true)\n                    //changed-correct\n                    user_response = await captureUserResponseAzure(toast, false)\n\n                    let user_response_timestamp = user_response.timestamp\n                    console.log('user_response_timestamp:', user_response_timestamp)\n\n                    captureEmotionTrigger = false\n                    console.log('before - capturedEmotionsList:', capturedEmotionsList)\n\n                    if (capturedEmotionsList.length > 0) {\n                        latestFacialEmotionTupleOnSpeak = getLatestFacialEmotionTuple(user_response_timestamp)\n                        console.log('latestFacialEmotionTupleOnSpeak:', latestFacialEmotionTupleOnSpeak)\n                    }\n\n                    capturedEmotionsList = []\n\n                    console.log('after - capturedEmotionsList:', capturedEmotionsList)\n\n                    user_response = user_response.text\n\n                    console.log('user_response:', user_response)\n\n                    if (interventionFullStop) break\n\n                    if (!user_response) {\n                        console.log('USER RESPONSE NOT PROVIDED!')\n                        console.log('user_response:', user_response)\n\n                        if (repeat_count >= max_repeat_count) {\n                            repeat_count = 0\n\n                            // Allow user to move on to the next question.\n                            // await agentSpeak(\"Let's move on to the next question.\", toast, true)\n                            // break\n\n                            // Terminate intervention due to exhausted attempts\n                            //changed-correct\n                            let speak_text = \"It seems you've used all your attempts to answer the current questions. Unfortunately, all questions must be answered as part of the intervention procedure to provide a valid evaluation. When you're ready to answer all the questions in the intervention, feel free to restart this module. Thank you for your time and participation.\"\n                            await agentSpeak(speak_text, toast, false)\n\n                            stop()\n                            return\n                        }\n                        else {\n                            repeat_count += 1\n                            //changed-correct\n\n                            let speak_text = \"It seems I wasn't able to hear what you said. Let me repeat the question.\"\n                            await agentSpeak(speak_text, toast, false)\n\n                            await agentSpeak(current_question_question, toast, false)\n\n                            await agentSpeak(current_question_options, toast, false)\n                        }\n                    }\n\n                    if (interventionFullStop) break\n                }\n\n                if (!user_response) {\n                    break\n                }\n\n                // console.log(`current_questionnaire_dict - 1: ${JSON.stringify(current_questionnaire_dict, null, 2)}`)\n\n                await delay(500);\n\n                bottomBoxText = 'Processing response. Please wait...';\n                updateBottomBoxText();\n\n                if (toastNotificationTrigger) {\n                    toast({\n                        title: 'eEVA Response',\n                        description: 'Processing response. Please wait...',\n                        status: 'warning',\n                        duration: 5000,\n                        isClosable: false,\n                    });\n                }\n\n                if (interventionFullStop) break\n\n                let cleaned_user_response = cleanResponse(user_response)\n                console.log('cleaned_user_response:', cleaned_user_response)\n\n                current_questionnaire_dict['transactions'][`Q${j}`]['text_emotions'][`${transaction_pair_count}: agent`] = \"\"\n                current_questionnaire_dict['transactions'][`Q${j}`]['text_emotions'][`${transaction_pair_count}: user`] = \"\"\n\n                current_questionnaire_dict['transactions'][`Q${j}`]['facial_emotions'][`${transaction_pair_count}: agent`] = \"\"\n                current_questionnaire_dict['transactions'][`Q${j}`]['facial_emotions'][`${transaction_pair_count}: user`] = \"\"\n\n                current_questionnaire_dict['transactions'][`Q${j}`]['responses'][`${transaction_pair_count}: agent`] = current_question\n                current_questionnaire_dict['transactions'][`Q${j}`]['responses'][`${transaction_pair_count}: user`] = cleaned_user_response\n\n                is_response_relevant = await checkResponseRelevancy(cleaned_user_response, current_question)\n\n                console.log(\"is_response_relevant:\", is_response_relevant)\n\n                if (interventionFullStop) break\n\n                let acknowledgement_trigger = (j % 2 == 0) // Reflective listening on every odd question.\n\n                if (is_response_relevant) {\n                    // console.log('cleaned_user_response:', cleaned_user_response)\n                    console.log('current_valid_match_user_responses', current_valid_match_user_responses)\n\n                    likert_matched_response_index = await llmMatchResponse(cleaned_user_response, current_valid_match_user_responses)\n                    console.log('likert_matched_response_index:', likert_matched_response_index)\n\n                    let response_match = current_valid_match_user_responses[likert_matched_response_index]\n\n                    if (interventionFullStop) break\n\n                    let agent_matched_text_emotion_response_index = await llmTextEmotionRecognition(current_question)\n                    let user_matched_text_emotion_response_index = await llmTextEmotionRecognition(cleaned_user_response)\n\n                    let agent_matched_text_emotion = universal_emotions_list[agent_matched_text_emotion_response_index]\n                    let user_matched_text_emotion = universal_emotions_list[user_matched_text_emotion_response_index]\n\n                    console.log(`agent_matched_text_emotion_${j}:`, agent_matched_text_emotion)\n                    console.log(`user_matched_text_emotion:_${j}:`, user_matched_text_emotion)\n\n                    current_questionnaire_dict['transactions'][`Q${j}`]['text_emotions'][`${transaction_pair_count}: agent`] = agent_matched_text_emotion\n                    current_questionnaire_dict['transactions'][`Q${j}`]['text_emotions'][`${transaction_pair_count}: user`] = user_matched_text_emotion\n\n                    if (latestFacialEmotionTupleOnSpeak) {\n                        current_questionnaire_dict['transactions'][`Q${j}`]['facial_emotions'][`${transaction_pair_count}: user`] = latestFacialEmotionTupleOnSpeak[0]\n                    }\n\n                    transaction_pair_count += 1\n\n                    if (interventionFullStop) break\n\n                    if (likert_matched_response_index != null) {\n                        if (interventionFullStop) break\n\n                        // console.log('acknowledgment_trigger:', acknowledgement_trigger)\n\n                        let rand_i = Math.floor(Math.random() * acknowledge_phrases.length)\n                        let random_acknowledgement = acknowledge_phrases[rand_i]\n\n                        if (acknowledgement_trigger) {\n                            //changed-correct\n                            await agentSpeak(`${random_acknowledgement}. Let's move on to the next question.`, toast, false)\n                        }\n\n                        let response_score = current_scoring_categories[likert_matched_response_index]\n\n                        console.log('response_score:', response_score)\n\n                        current_questionnaire_dict['transactions'][`Q${j}`]['match_index'] = likert_matched_response_index\n                        current_questionnaire_dict['transactions'][`Q${j}`]['response_match'] = response_match\n                        current_questionnaire_dict['transactions'][`Q${j}`]['response_score'] = response_score\n\n                        // console.log(`current_questionnaire_dict - 2: ${JSON.stringify(current_questionnaire_dict, null, 2)}`)\n\n                        if (conversation_log['transcript']['questionnaires'][i] == undefined)\n                            conversation_log['transcript']['questionnaires'].push(current_questionnaire_dict)\n                        else\n                            conversation_log['transcript']['questionnaires'][i] = current_questionnaire_dict\n\n                        questionnaire_score += response_score\n                        current_questionnaire_dict['total_questionnaire_score'] = questionnaire_score\n\n                        console.log(`conversation_log: ${JSON.stringify(conversation_log, null, 2)}`)\n                    }\n                    else {\n                        console.log('LIKERT_MATCHED_RESPONSE_INDEX NOT PRODUCED!')\n                        console.log('likert_matched_response_index:', likert_matched_response_index)\n\n                        if (repeat_count >= max_repeat_count) {\n                            repeat_count = 0\n\n                            // Allow user to move on to the next question.\n                            // await agentSpeak(\"Let's move on to the next question.\", toast, true)\n                            // break\n\n                            // Terminate intervention due to exhausted attempts\n                            //changed-correct\n                            let speak_text = \"It seems you've used all your attempts to answer the current questions. Unfortunately, all questions must be answered as part of the intervention procedure to provide a valid evaluation. When you're ready to answer all the questions in the intervention, feel free to restart this module. Thank you for your time and participation.\"\n\n                            await agentSpeak(speak_text, toast, false)\n\n                            stop()\n                            return\n                        }\n                        else {\n                            repeat_count += 1\n                            //changed-correct\n                            let speak_text = \"May you please rephrase and elaborate your response? Let me repeat the question.\"\n\n                            await agentSpeak(speak_text, toast, false)\n\n                            await agentSpeak(current_question_question, toast, false)\n\n                            await agentSpeak(current_question_options, toast, false)\n                        }\n                    }\n                }\n                else {\n                    console.log('RESPONSE IS NOT RELEVANT!')\n                    console.log('is_response_relevant:', is_response_relevant)\n\n                    if (repeat_count >= max_repeat_count) {\n                        repeat_count = 0\n\n                        // Allow user to move on to the next question.\n                        // await agentSpeak(\"Let's move on to the next question.\", toast, true)\n                        // break\n\n                        // Terminate intervention due to exhausted attempts\n                        //changed-correct\n                        let speak_text = \"It seems you've used all your attempts to answer the current questions. Unfortunately, all questions must be answered as part of the intervention procedure to provide a valid evaluation. When you're ready to answer all the questions in the intervention, feel free to restart this module. Thank you for your time and participation.\"\n\n                        await agentSpeak(speak_text, toast, false)\n\n                        stop()\n                        return\n                    }\n                    else {\n                        repeat_count += 1\n                        //changed-correct\n                        let speak_text = \"May you please rephrase and elaborate your response? Let me repeat the question.\"\n\n                        await agentSpeak(speak_text, toast, false)\n\n                        await agentSpeak(current_question_question, toast, false)\n\n                        await agentSpeak(current_question_options, toast, false)\n                    }\n                }\n\n                if (interventionFullStop) break\n            }\n        }\n\n        if (!interventionFullStop) {\n            //changed-correct\n            await agentSpeak(`It appears you've answered all the questions from the ${questionnaire_name}. Let me evaluate your score.`, toast, false)\n            //changed-correct\n            await agentSpeak(`You've score a ${questionnaire_score} out of ${max_questionnaire_score}.`, toast, false)\n\n            if (questionnaire_name == 'AUDIT questionnaire') {\n                console.log('Evaluating AUDIT questionnaire score...')\n                console.log('questionnaire_score:', questionnaire_score)\n                switch (true) {\n                    case questionnaire_score >= 0 && questionnaire_score <= 7:\n                        //changed-correct\n                        await agentSpeak('This means you fall under risk level 1 and are to be provided some alcohol education.', toast, false);\n                        break;\n                    case questionnaire_score >= 8 && questionnaire_score <= 15:\n                        await agentSpeak('This means you fall under risk level 2 and are to be provided some simple advice.', toast, false);\n                        break;\n                    case questionnaire_score >= 16 && questionnaire_score <= 19:\n                        //changed-correct\n                        await agentSpeak('This means you fall under risk level 3 and are to be provided some simple advice plus brief counseling and continued monitoring.', toast, false);\n                        break;\n                    case questionnaire_score >= 20:\n                        //changed-correct\n                        await agentSpeak('This means you fall under risk level 4 and are to be referred to a specialist for diagnostic evaluation and treatment.', toast, false);\n                        break;\n                }\n            }\n        }\n    }\n    if (!interventionFullStop) {\n        //changed-correct\n        await agentSpeak(`It seems we've covered all the questionnaires.`, toast, false)\n    }\n    //changed-correct\n    await agentSpeak(`Ending intervention procedure.`, toast, false)\n    bottomBoxText = \"\"\n    updateBottomBoxText()\n}\n\nasync function startProcedure(toast) {\n    console.log('IN START PROCEDURE!')\n    console.log('activeListeningTrigger:', activeListeningTrigger)\n\n    interventionFullStop = false\n\n    // Use this to demo the tech stack.\n    // userDrivenDemo()\n\n    await delay(3000)\n\n    if (activeListeningTrigger == true) {\n        console.log('RUNNING ACTIVE LISTENING AGENT DRIVEN INTERVENTION!!!')\n\n        toast({\n            title: 'eEVA Procedure',\n            description: 'RUNNING ACTIVE LISTENING AGENT DRIVEN INTERVENTION!!!',\n            status: 'success',\n            duration: 0,\n            isClosable: false,\n        });\n        //changed-correct\n        activeListeningAgentDrivenIntervention(toast, false)\n    }\n    else {\n        console.log('RUNNING AGENT DRIVEN INTERVENTION!!!')\n\n        toast({\n            title: 'eEVA Procedure',\n            description: 'RUNNING AGENT DRIVEN INTERVENTION!!!',\n            status: 'success',\n            duration: 0,\n            isClosable: false,\n        });\n        //changed-correct\n        agentDrivenIntervention(toast, false)\n    }\n}\n\nfunction saveJSON(json_dict) {\n    // Sample JSON object\n    const data = json_dict\n\n    // Convert JSON object to string\n    const jsonData = JSON.stringify(data);\n\n    // Create a Blob object from the JSON string\n    const blob = new Blob([jsonData], { type: \"application/json\" });\n\n    // Create a link element\n    const link = document.createElement('a');\n\n    // Set the download attribute with the desired file name\n    link.download = 'data.json';\n\n    // Create an object URL for the Blob\n    link.href = window.URL.createObjectURL(blob);\n\n    // Programmatically click the link to trigger the download\n    link.click();\n}\n\nfunction initAzure(animationManager, settings) {\n    // Azure Key\n    console.log('azure key object:', settings.azure_api_key)\n    console.log('azure key object type:', typeof settings.azure_api_key)\n    console.log('azure key default:', settings.azure_api_key.default)\n\n    // Initialize SpeechManager object\n    if (settings.azure_api_key && typeof settings.azure_api_key == 'string') {\n        speechManager = new SpeechManager(animationManager, settings.azure_api_key, 'eastus')\n    }\n    else {\n        speechManager = new SpeechManager(animationManager, settings.azure_api_key.default, 'eastus')\n    }\n}\n\nfunction setFace(au_data) {\n    // Extrapolated from AnimationManager.js\n    au_data.forEach(({ id, intensity, duration, explanation = \"\" }) => {\n        animationManager.scheduleChange(id, intensity * 90, duration, 0, explanation);\n    });\n}\n\nfunction setHeadToFaceUser() {\n    let au_data = [\n        {\n            \"id\": \"51\",\n            \"intensity\": 0.12222222222222222,\n            \"duration\": 750,\n            \"explanation\": \"\"\n        },\n        {\n            \"id\": \"53\",\n            \"intensity\": 0.06666666666666667,\n            \"duration\": 750,\n            \"explanation\": \"\"\n        },\n        {\n            \"id\": \"55\",\n            \"intensity\": 0.16666666666666666,\n            \"duration\": 750,\n            \"explanation\": \"\"\n        }\n    ]\n\n    setFace(au_data = au_data)\n}\n\nfunction neutralFace(skipList = [], smoothTime = 0.5) {\n    // animationManager.setFaceToNeutral()\n\n    console.log('skipList:', skipList)\n\n    // Set all AUs to neutral positions.\n    ActionUnitsList.forEach((AU) => {\n        console.log('current AU.id:', AU.id)\n        if (skipList.includes(AU.id)) {\n            console.log('skipping AU.id:', AU.id)\n            return\n        }\n        //   animationManager.scheduleChange(AU.id, 0, 750, 0, \"\");\n        animationManager.applyAUChange(AU.id, 0, 0, \"\", smoothTime, \"\")\n    });\n    // Set all visemes to neutral positions.\n    VisemesList.forEach((viseme) => {\n        animationManager.scheduleVisemeChange(viseme.id, 0, 750);\n    });\n\n    setHeadToFaceUser()\n}\n\nasync function headNodAffirmation() {\n    neutralFace([\"45\", \"51\", \"53\", \"55\"])\n\n    await (delay(1000))\n\n    animationManager.applyAUChange(\"54\", 15, 0, \"\", 2, \"\") //headDownAffirmation\n\n    await (delay(1000))\n\n    neutralFace([\"45\", \"51\", \"53\", \"55\"], 3)\n}\n\nfunction startBlinking(animationManager) {\n    // Clear any existing interval to avoid overlapping blinks\n    if (blinkInterval) {\n        clearInterval(blinkInterval);\n    }\n\n    // Set up the blink interval\n    blinkInterval = setInterval(() => {\n        // Close eyes\n        animationManager.scheduleChange(\"45\", 200, 100, 0); // Close eyes\n        // Open eyes after 200ms delay (after the close animation)\n        setTimeout(() => animationManager.scheduleChange(\"45\", 0, 100, 0), 600); // Open300 eyes\n    }, blinkSpeed + 400);\n}\n\nasync function start(animationManager, settings, containerRef) {\n    console.clear()\n\n    console.log('Calling start!')\n\n    neutralFace([\"45\", \"51\", \"53\", \"55\"])\n\n    if (blinkInterval) {\n        clearInterval(blinkInterval);\n    }\n\n    if (!containerRef || !containerRef.current) {\n        console.error('Invalid container reference');\n        return;\n    }\n\n    if (!root) {\n        root = ReactDOMClient.createRoot(containerRef.current);\n    }\n\n    recognitionStop = false\n\n    console.log('settings:', settings)\n\n    activeListeningTrigger = settings.active_listening_trigger;\n\n    // Debug of latest FER detection.\n    // monitorFER()\n\n    initAzure(animationManager, settings)\n\n    const MyComponent = () => {\n        const toast = useToast()\n\n        enableFER(animationManager, settings, root)\n        startBlinking(animationManager)\n        startProcedure(toast)\n    }\n\n    root.render(<MyComponent />);\n}\n\nfunction stop(animationManager, settings, containerRef) {\n    console.log('Calling stop!')\n\n    stopFER(animationManager, settings, containerRef)\n\n    bottomBoxText = \"\"\n    updateBottomBoxText()\n\n    if (blinkInterval) {\n        clearInterval(blinkInterval);\n    }\n\n    if (recognitionAvailable) {\n        recognitionStop = true\n        recognition.stop()\n        console.log(\"speech recognition stopped!\")\n    }\n\n    interventionFullStop = true\n}\n\nexport { start, stop }\n\n//Shamim implemented\nfunction createBottomBox() {\n    if (!document.getElementById(\"bottomBox\")) {\n        const box = document.createElement(\"div\");\n        box.id = \"bottomBox\";\n        box.style.position = \"fixed\";\n        box.style.bottom = \"20px\"; // Adjust distance from the bottom\n        box.style.left = \"50%\"; // Center horizontally\n        box.style.transform = \"translateX(-50%)\"; // Center horizontally\n        box.style.width = \"80%\"; // Adjust width\n        box.style.backgroundColor = \"#333\";\n        box.style.color = \"#fff\";\n        box.style.padding = \"8px\";\n        box.style.textAlign = \"center\";\n        box.style.zIndex = \"1000\";\n        box.style.fontSize = \"18px\";\n        box.style.borderRadius = \"8px\"; // Optional: Rounded corners\n        document.body.appendChild(box);\n    }\n}\n\nfunction updateBottomBoxText() {\n    const box = document.getElementById(\"bottomBox\");\n    if (box) {\n        if (bottomBoxText.trim() === \"\") {\n            box.innerText = \"\"; // Clear the text but keep the box visible\n            box.style.opacity = \"0.5\"; // Dim the box to indicate no active content\n        } else {\n            box.innerText = bottomBoxText; // Update the text from the global variable\n            box.style.opacity = \"1\"; // Restore full opacity\n        }\n    }\n}\n"],"names":["constructor","animationManager","apiKey","region","this","queue","isSpeaking","audioInterrupt","speechConfig","SpeechConfig","fromSubscription","initSynthesizer","recognizeSpeechUntilSilence","Promise","resolve","reject","audioConfig","AudioConfig","fromDefaultMicrophoneInput","recognizer","SpeechRecognizer","recognizedText","silenceTimer","finalRecognitionTimestamp","setTimeout","console","log","stopContinuousRecognitionAsync","text","trim","timestamp","recognizing","s","e","concat","result","clearTimeout","recognized","reason","ResultReason","RecognizedSpeech","Date","now","NoMatch","canceled","error","Error","sessionStopped","startContinuousRecognitionAsync","err","speechSynthesisVoiceName","player","SpeakerAudioDestination","fromSpeakerOutput","synthesizer","SpeechSynthesizer","visemeReceived","scheduleVisemeApplication","visemeId","audioOffset","enqueueText","push","processQueue","length","shift","synthesizeSpeech","then","speakTextAsync","applyVisemeToCharacter","facsLib","setFaceToNeutral","setNeutralViseme","setTargetViseme","updateEngine","stopSpeech","close","interruptSpeech","_ref","onEmotionDetected","stopVideoRef","videoRef","useRef","canvasRef","intervalRef","streamRef","useEffect","async","MODEL_URL","process","faceapi","tinyFaceDetector","loadFromUri","faceLandmark68Net","faceRecognitionNet","faceExpressionNet","startVideo","loadModels","navigator","mediaDevices","getUserMedia","video","stream","current","srcObject","catch","stopVideo","getTracks","forEach","track","stop","detectFace","paused","ended","detections","withFaceLandmarks","withFaceExpressions","canvas","displaySize","width","videoWidth","height","videoHeight","resizedDetections","getContext","clearRect","expressions","sortedEmotionTuples","Object","entries","map","_ref2","emotion","confidence","sort","a","b","handlePlay","setInterval","videoElement","addEventListener","clearInterval","removeEventListener","_jsxs","style","position","children","_jsx","ref","autoPlay","muted","left","top","conversation_log","universal_emotions_list","lastEmotionTime","topEmotionTuple","latestFacialEmotionTupleOnSpeak","blinkInterval","root","captureEmotionTrigger","capturedEmotionsList","blinkSpeed","llm_model_name","llm_iter_relevancy_num","llm_iter_matching_num","llm_iter_emotion_num","acknowledge_phrases","recognition","speechManager","intervention_dict","activeListeningTrigger","recognitionAvailable","recognitionStop","interventionFullStop","bottomBoxText","handleEmotionDetected","sortedEmotionsTuples","delay","ms","agentSpeak","toast","toastNotificationTrigger","arguments","undefined","synthesize_result","delayTime","audioDuration","updateBottomBoxText","title","description","status","duration","isClosable","message","loadIntervention","audit_questionnaire_dict","simple_intervention_config","test_intervention_config","captureUserResponseAzure","cleanResponse","response","clean_response","replace","toLowerCase","llmPrompt","llm_prompt","clean_response_trigger","llm_response_msg_content","ollama","chat","model","messages","role","content","checkResponseRelevancyOnce","user_response","question","includes","checkResponseRelevancy","response_relevancy_list","i","response_relevancy_count","j","is_response_relevant","parseInt","Math","floor","checkResponseRelevancyMultiple","stringifyArray","chosen_array","array_string","element","stringifyArrayListFormat","mostFrequentNumber","arr","frequencyMap","maxCount","mostFrequentNum","num","llmMatchResponseOnce","valid_user_responses","valid_user_responses_string","match_index","llmMatchResponseMultiple","match_index_list","llmMatchResponse","match_count","generateReflectiveListeningResponse","current_question","response_match","llmConfirmResponse","current_statement","generateFollowUpQuestion","target_question","llmTextEmotionRecognitionOnce","universal_emotions_string","llmTextEmotionRecognitionMultiple","llmTextEmotionRecognition","initDataCapture","crypto","randomUUID","toLocaleString","year","month","day","hour","minute","second","JSON","stringify","getLatestFacialEmotionTuple","speech_response_timestamp","closestEmotion","closestTimestampDiff","Infinity","current_emotion_tuple","current_timestamp","timestampDiff","abs","startProcedure","createBottomBox","agent_greeting","questionnaires","current_questionnaire_dict","current_questionnaire","questionnaire_name","max_questionnaire_score","introduction","questions","target_questions","valid_match_user_responses","scoring_categories","questionnaire_score","transaction_pair_count","current_target_question","current_valid_user_responses","current_valid_match_user_responses","current_scoring_categories","current_question_question","current_question_options","is_response_relevant_target","likert_matched_response_index","repeat_count","max_repeat_count","user_response_timestamp","speak_text","cleaned_user_response","reflective_listening_trigger","agent_matched_text_emotion_response_index","user_matched_text_emotion_response_index","agent_matched_text_emotion","user_matched_text_emotion","reflective_listening_response","confirmation_response","is_response_confirmed","rand_i","random","random_acknowledgement","headNodAffirmation","response_score","followup_question","followup_first_time","activeListeningAgentDrivenIntervention","acknowledgement_trigger","agentDrivenIntervention","setHeadToFaceUser","au_data","id","intensity","explanation","scheduleChange","setFace","neutralFace","skipList","smoothTime","ActionUnitsList","AU","applyAUChange","VisemesList","viseme","scheduleVisemeChange","start","settings","containerRef","clear","ReactDOMClient","active_listening_trigger","azure_api_key","default","SpeechManager","initAzure","MyComponent","useToast","React","render","FaceDetection","enableFER","startBlinking","unmount","document","getElementById","box","createElement","bottom","transform","backgroundColor","color","padding","textAlign","zIndex","fontSize","borderRadius","body","appendChild","innerText","opacity"],"sourceRoot":""}