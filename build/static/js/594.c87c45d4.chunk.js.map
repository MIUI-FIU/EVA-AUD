{"version":3,"file":"static/js/594.c87c45d4.chunk.js","mappings":"sIA8HA,QA5HA,MACIA,WAAAA,CAAYC,EAAkBC,EAAQC,GAClCC,KAAKH,iBAAmBA,EACxBG,KAAKF,OAASA,EACdE,KAAKD,OAASA,EACdC,KAAKC,MAAQ,GACbD,KAAKE,YAAa,EAElBF,KAAKG,OACLH,KAAKI,YACLJ,KAAKK,gBAAiB,EAGtBL,KAAKM,iBACT,CAEAA,eAAAA,GACI,MAAMR,EAASE,KAAKF,OACdC,EAASC,KAAKD,OACdQ,EAAeC,EAAAA,aAAaC,iBAAiBX,EAAQC,GAC3DQ,EAAaG,yBAA2B,oBAExC,MAAMP,EAAS,IAAIQ,EAAAA,wBACbP,EAAcQ,EAAAA,YAAYC,kBAAkBV,GAElDH,KAAKG,OAASA,EACdH,KAAKI,YAAcA,EAEnBJ,KAAKc,YAAc,IAAIC,EAAAA,kBAAkBR,EAAcP,KAAKI,aAE5DJ,KAAKc,YAAYE,eAAiB,CAACC,EAAGC,KAClClB,KAAKmB,0BAA0BD,EAAEE,SAAUF,EAAEG,YAAY,CAEjE,CAEAC,WAAAA,CAAYC,GACRvB,KAAKC,MAAMuB,KAAKD,GACXvB,KAAKE,YACNF,KAAKyB,cAEb,CAEAA,YAAAA,GACI,GAA0B,IAAtBzB,KAAKC,MAAMyB,OAEX,YADA1B,KAAKE,YAAa,GAItBF,KAAKE,YAAa,EAClB,MAAMqB,EAAOvB,KAAKC,MAAM0B,QACxB3B,KAAK4B,iBAAiBL,GAAMM,MAAK,IAAM7B,KAAKyB,gBAChD,CAEAG,gBAAAA,CAAiBL,GACb,OAAO,IAAIO,SAAQ,CAACC,EAASC,KACzBhC,KAAKc,YAAYmB,eAAeV,GAC5BW,IACIC,QAAQC,IAAI,+BACZL,EAAQG,EAAO,IAEnBG,IACIF,QAAQC,IAAI,6CACZJ,EAAOK,EAAM,GACf,GAEd,CAEAlB,yBAAAA,CAA0BC,EAAUC,GAEhCiB,YAAW,KACPtC,KAAKuC,uBAAuBnB,EAAS,GAFZC,EAAc,IAI/C,CAEAkB,sBAAAA,CAAuBnB,GACnB,MAAMoB,EAAUxC,KAAKH,iBAAiB2C,QAEjCxC,KAAKK,gBASNL,KAAKH,iBAAiB4C,mBACtBD,EAAQE,oBATS,IAAbtB,EACAoB,EAAQE,iBAAiB,IAEzBtB,GAAY,EACZoB,EAAQG,gBAAgBvB,EAAU,GAAI,IAQ9CoB,EAAQI,cACZ,CAEAC,UAAAA,GAEI7C,KAAKc,YAAYgC,QAGjB9C,KAAKC,MAAQ,GACbD,KAAKE,YAAa,EAGlBF,KAAKM,kBAEL6B,QAAQC,IAAI,4BAChB,CAEAW,eAAAA,CAAgBxB,GAEZvB,KAAKc,YAAYgC,QAGjB9C,KAAKC,MAAQ,GACbD,KAAKE,YAAa,EAClBF,KAAKM,kBAEDiB,GACAvB,KAAKsB,YAAYC,GAGrBY,QAAQC,IAAI,gCAChB,E,gDCpEF,QAvDF,MACIxC,WAAAA,CAAYE,GACV,IAAKA,GAA4B,kBAAXA,EACpB,MAAM,IAAIkD,MAAM,4CAElBhD,KAAKF,OAASA,EACdE,KAAKiD,OAAS,4CAChB,CAQA,iBAAMC,CAAY3B,GAChB,MAAM4B,EAAU,CACdC,MAAO,gBACPC,SAAU,CACR,CAAEC,KAAM,SAAUC,QAJWC,UAAA9B,OAAA,QAAA+B,IAAAD,UAAA,GAAAA,UAAA,GAAG,oCAKhC,CAAEF,KAAM,OAAQC,QAAShC,IAE3BmC,WAAY,IACZC,YAAa,IAGf,IAAK,IAADC,EAAAC,EAAAC,EAAAC,EACF,MAAMC,QAAiBC,MAAMjE,KAAKiD,OAAQ,CACxCiB,OAAQ,OACRC,QAAS,CACP,cAAgB,UAADC,OAAYpE,KAAKF,QAChC,eAAgB,oBAElBuE,KAAMC,KAAKC,UAAUpB,KAGvB,IAAKa,EAASQ,GACZ,MAAM,IAAIxB,MAAM,mCAADoB,OAAoCJ,EAASS,SAG9D,MACMC,EAA0B,QAAfd,SADEI,EAASW,QACHC,eAAO,IAAAhB,GAAK,QAALC,EAAZD,EAAe,UAAE,IAAAC,GAAS,QAATC,EAAjBD,EAAmBgB,eAAO,IAAAf,GAAS,QAATC,EAA1BD,EAA4BP,eAAO,IAAAQ,OAAvB,EAAZA,EAAqCe,OAEzD,IAAKJ,EACH,MAAM,IAAI1B,MAAM,+CAGlB,OAAO0B,CACT,CAAE,MAAOrC,GAEP,MADAF,QAAQE,MAAM,kCAAmCA,GAC3CA,CACR,CACF,E,gGC7CJ,IACI0C,EAGAC,EAEAC,EAKAC,EAXAC,GAAuB,EAEvBC,EAAiB,GAMjBC,GAAoB,EACpBC,GAAkB,EAItB,SAASC,EAAQC,GAEbA,EAAQC,SAAQC,IAAoD,IAAnD,GAAEC,EAAE,UAAEC,EAAS,SAAEC,EAAQ,YAAEC,EAAc,IAAIJ,EAC1D7F,iBAAiBkG,eAAeJ,EAAgB,GAAZC,EAAgBC,EAAU,EAAGC,EAAY,GAErF,CAEA,SAASE,IACL,IAAIR,EAAU,CACV,CACE,GAAM,IACN,UAAa,GACb,SAAY,IACZ,YAAe,IAEjB,CACE,GAAM,KACN,UAAa,GACb,SAAY,IACZ,YAAe,KAIrBD,EAAgBC,EACpB,CAeA,SAASS,IACLpG,iBAAiB4C,kBACrB,CAGA,SAASyD,EAAMC,GACX,OAAO,IAAIrE,SAAQC,GAAWO,WAAWP,EAASoE,IACtD,CAEAC,eAAeC,EAAW9E,GACtB0D,EAAc3E,kBACd,IAAIgG,QAA0BrB,EAAcrD,iBAAiBL,GAE7DY,QAAQC,IAAI,4BAA6BkE,GACzCnE,QAAQC,IAAI,gCAAiCb,GAC7CY,QAAQC,IAAI,mCAAoCkE,EAAkBC,eAElEpE,QAAQC,IAAI,iBAEZ,UACU8D,EAAMI,EAAkBC,cAAgB,KAC9CpE,QAAQC,IAAI,eAChB,CAAE,MAAOC,GACiB,kBAAlBA,EAAMwC,QACN1C,QAAQC,IAAI,qBAEZD,QAAQE,MAAM,sBAAuBA,EAE7C,CACJ,CA4BA+D,eAAeI,EAAM3G,EAAkB4G,GA8BnC,GA7BAtE,QAAQC,IAAI,kBAEZD,QAAQC,IAAI,qBAAsBqE,EAASC,gBAC3CvE,QAAQC,IAAI,oBAAqBqE,EAASE,eAE1CxE,QAAQC,IAAI,iCAAkCqE,EAASC,gBACvDvE,QAAQC,IAAI,gCAAiCqE,EAASE,eAEtDxE,QAAQC,IAAI,sBAAuBqE,EAASC,eAAeE,SAC3DzE,QAAQC,IAAI,qBAAsBqE,EAASE,cAAcC,SAEzD1B,EAAkB,EAIdD,EADAwB,EAASE,eAAkD,iBAA1BF,EAASE,cAC1B,IAAIE,EAAAA,EAAchH,EAAkB4G,EAASE,cAAe,UAG5D,IAAIE,EAAAA,EAAchH,EAAkB4G,EAASE,cAAcC,QAAS,UAGxFZ,UAEMK,EAAW,4FAADjC,OAA6FqC,EAASK,YAAW,uCAEjIb,IAEA9D,QAAQC,IAAI,mBAAoBkD,GAE5B,4BAA6ByB,OAAQ,CACrC5E,QAAQC,IAAI,gCACZiD,GAAoB,EAEpB,IAAI2B,EAAe,CAAC,EAGpB,IAAI1B,EAAgB,CAChBH,GAAuB,EACvBG,GAAkB,EAGlB,MAAM2B,EAAoBF,OAAOE,mBAAqBF,OAAOG,wBAC7DnC,EAAc,IAAIkC,EAIdjC,EADAyB,EAASC,gBAAoD,iBAA3BD,EAASC,eAC9B,IAAIS,EAAAA,EAAoBV,EAASC,gBAGjC,IAAIS,EAAAA,EAAoBV,EAASC,eAAeE,SAGjE7B,EAAYqC,YAAa,EACzBrC,EAAYsC,KAAO,QACnBtC,EAAYuC,gBAAiB,EAC7BvC,EAAYwC,gBAAkB,EAE9BxC,EAAYyC,SAAWpB,eAAeqB,GAClC,IAAIC,EAAa,GACbC,GAAU,EAEVC,EAAsB,CAAC,EAE3B,IAAK,IAAIC,EAAIJ,EAAMK,YAAaD,EAAIJ,EAAMM,QAAQrG,OAAQmG,IACtDH,GAAcD,EAAMM,QAAQF,GAAG,GAAGH,WAC9BD,EAAMM,QAAQF,GAAGF,UACjBA,GAAU,GAOlB,GAHAxF,QAAQC,IAAI,qBAAsBiD,GAG9BsC,GAAWD,IAAetC,IAAmBC,EAAmB,CAChED,EAAiBsC,EACjBrC,GAAoB,EAGpB,IAAI2C,EAAoBN,EAAWO,cAAcnD,OAWjD,GATA3C,QAAQC,IAAI,qBAAsB4F,GAS9BA,EAAkB,CAElB7F,QAAQC,IAAI,cAAeqE,EAASC,gBACpCvE,QAAQC,IAAI,aAAcqE,EAASE,eAEnCiB,EAAiC,YAAII,EAErC,IAAIE,EAAgB,GAiBpB,GAhBAA,QA5HxB9B,eAA6BsB,EAAY1C,GACrC,IAAIN,EAAc,GAElB,IACIA,QAAoBM,EAAW9B,YAAYwE,GAI3CvF,QAAQC,IAAI,gBAAiBsC,EACjC,CACA,MAAOxD,GACHiB,QAAQE,MAAM,iCAAkCnB,EACpD,CACA,IAMI,OALGwD,UACO2B,EAAW3B,GACjBQ,GAAmB,GAGhBR,CACX,CACA,MAAOxD,GACHiB,QAAQE,MAAM,iCAAkCnB,EACpD,CACJ,CAoG8CiH,CAAcH,EAAmBhD,GAEvD4C,EAAoC,eAAIM,EAIxC/F,QAAQC,IAAI,qBACZD,QAAQC,IAAI,mBAAoB8C,GAChC/C,QAAQC,IAAI,wBAAyBqE,EAASK,aAE9CE,EAAa9B,EAAgB,GAAK0C,EAElCvC,GAAoB,EAEpBlD,QAAQC,IAAI,qBAAsBiD,GAE9BH,EAAkBuB,EAASK,YAAY,CACvCzB,GAAoB,QACdgB,EAAW,+BAADjC,OAAgCqC,EAASK,YAAW,sBAC9DT,EAAW,gGACXA,EAAW,sBAEjBL,IApLxBT,EATc,CACV,CACI,GAAM,MACN,UAAa,GACb,SAAY,IACZ,YAAe,YA0LOW,EAAM,KACZD,IAEA,IAAImC,EAAc9D,KAAKC,UAAUyC,EAAc,KAAM,GAErD7E,QAAQC,IAAI,yBAADgC,OAA0BgE,IAErC9C,GAAkB,EAClBP,EAAYsD,MAChB,CACJ,CACJ,CACJ,EACAtD,EAAYuD,MAAQ,WACXhD,IACDnD,QAAQC,IAAI,oCACR+C,IACAJ,EAAYyB,QACZrE,QAAQC,IAAI,kCAGxB,CACJ,CACJ,MAEID,QAAQE,MAAM,oDAGU,GAAxB8C,IACAJ,EAAYyB,QACZrE,QAAQC,IAAI,+BAEpB,CAEA,SAASiG,IACLlG,QAAQC,IAAI,iBAEZkD,GAAkB,EAClBnD,QAAQC,IAAI,mBAAoBkD,GAO5BH,IACAJ,EAAYsD,OACZlG,QAAQC,IAAI,gCAGhBD,QAAQC,IAAI,oBAChB,C","sources":["VISOS/action/verbalizers/SpeechManager.js","VISOS/cognition/TextToGptReconciler.js","modules/evaLLM.js"],"sourcesContent":["import { AudioConfig, SpeakerAudioDestination, SpeechConfig, SpeechSynthesizer } from \"microsoft-cognitiveservices-speech-sdk\";\n\nclass SpeechManager {\n    constructor(animationManager, apiKey, region) {\n        this.animationManager = animationManager;\n        this.apiKey = apiKey,\n        this.region = region,\n        this.queue = [];\n        this.isSpeaking = false;\n\n        this.player;\n        this.audioConfig;\n        this.audioInterrupt = false;\n\n        // Initialize synthesizer\n        this.initSynthesizer();\n    }\n\n    initSynthesizer() {\n        const apiKey = this.apiKey;\n        const region = this.region;\n        const speechConfig = SpeechConfig.fromSubscription(apiKey, region);\n        speechConfig.speechSynthesisVoiceName = \"en-US-JennyNeural\";\n\n        const player = new SpeakerAudioDestination();;\n        const audioConfig = AudioConfig.fromSpeakerOutput(player);\n\n        this.player = player\n        this.audioConfig = audioConfig\n\n        this.synthesizer = new SpeechSynthesizer(speechConfig, this.audioConfig);\n\n        this.synthesizer.visemeReceived = (s, e) => {\n            this.scheduleVisemeApplication(e.visemeId, e.audioOffset);\n        };\n    }\n\n    enqueueText(text) {\n        this.queue.push(text);\n        if (!this.isSpeaking) {\n            this.processQueue();\n        }\n    }\n\n    processQueue() {\n        if (this.queue.length === 0) {\n            this.isSpeaking = false;\n            return;\n        }\n\n        this.isSpeaking = true;\n        const text = this.queue.shift();\n        this.synthesizeSpeech(text).then(() => this.processQueue());\n    }\n\n    synthesizeSpeech(text) {\n        return new Promise((resolve, reject) => {\n            this.synthesizer.speakTextAsync(text,\n                result => {\n                    console.log(\"Speech synthesis completed.\");\n                    resolve(result);\n                },\n                error => {\n                    console.log(\"Error during speech synthesis, restarting\");\n                    reject(error);\n                });\n        });\n    }\n\n    scheduleVisemeApplication(visemeId, audioOffset) {\n        const offsetInMilliseconds = audioOffset / 10000;\n        setTimeout(() => {\n            this.applyVisemeToCharacter(visemeId);\n        }, offsetInMilliseconds);\n    }\n\n    applyVisemeToCharacter(visemeId) {\n        const facsLib = this.animationManager.facsLib;\n        // console.log('audioInterrupt:', this.audioInterrupt)\n        if (!this.audioInterrupt){\n            if (visemeId === 0) {\n                facsLib.setNeutralViseme(0.0);\n            } else {\n                visemeId -= 1;\n                facsLib.setTargetViseme(visemeId, 70, 0);\n            }\n        }\n        else{\n            this.animationManager.setFaceToNeutral()\n            facsLib.setNeutralViseme()\n        }\n\n        facsLib.updateEngine();\n    }\n\n    stopSpeech() {\n        // Dispose of the current synthesizer to stop speech\n        this.synthesizer.close();\n\n        // Clear the queue and reset the speaking state\n        this.queue = [];\n        this.isSpeaking = false;\n\n        // Reinitialize the synthesizer for future use\n        this.initSynthesizer();\n\n        console.log(\"Speech synthesis stopped.\");\n    }\n\n    interruptSpeech(text) {\n        // Dispose of the current synthesizer to stop speech\n        this.synthesizer.close();\n\n        // Clear the queue, reset the speaking state, and reinitialize the synthesizer\n        this.queue = [];\n        this.isSpeaking = false;\n        this.initSynthesizer();\n\n        if (text) {\n            this.enqueueText(text); // Enqueue and play the new text\n        }\n\n        console.log(\"Speech synthesis interrupted.\");\n    }\n}\n\nexport default SpeechManager;\n","class TextToGptReconciler {\n    constructor(apiKey) {\n      if (!apiKey || typeof apiKey !== 'string') {\n        throw new Error('A valid OpenAI API key must be provided.');\n      }\n      this.apiKey = apiKey;\n      this.apiUrl = 'https://api.openai.com/v1/chat/completions';\n    }\n  \n    /**\n     * Processes the provided text by sending it to the OpenAI API.\n     * @param {string} text - The input text to process.\n     * @param {string} instruction - Instruction or system prompt for the AI.\n     * @returns {Promise<string>} - The GPT response.\n     */\n    async processText(text, instruction = 'Answer in a professional manner:') {\n      const payload = {\n        model: 'gpt-3.5-turbo',  // You can switch this model based on your requirement\n        messages: [\n          { role: 'system', content: instruction },\n          { role: 'user', content: text }\n        ],\n        max_tokens: 150,  // Adjust as necessary\n        temperature: 0.7\n      };\n  \n      try {\n        const response = await fetch(this.apiUrl, {\n          method: 'POST',\n          headers: {\n            'Authorization': `Bearer ${this.apiKey}`,\n            'Content-Type': 'application/json'\n          },\n          body: JSON.stringify(payload)\n        });\n  \n        if (!response.ok) {\n          throw new Error(`API request failed with status: ${response.status}`);\n        }\n  \n        const data = await response.json();\n        const gptResponse = data.choices?.[0]?.message?.content?.trim();\n  \n        if (!gptResponse) {\n          throw new Error('Failed to get a valid response from the API');\n        }\n  \n        return gptResponse;\n      } catch (error) {\n        console.error('Error processing text with GPT:', error);\n        throw error;\n      }\n    }\n  }\n  \n  export default TextToGptReconciler;","// This module does STT -> QA (LLM) -> TTS.\nimport SpeechManager from \"../VISOS/action/verbalizers/SpeechManager\";\nimport TextToGptReconciler from \"../VISOS/cognition/TextToGptReconciler\";\n\n// This doesn't work requires CLI access which is not available on github pages.\n// import ollama from 'ollama'\n\nlet recognitionAvailable = false;\nlet recognition;\nlet lastTranscript = '';\n\nlet gptManager;\n\nlet speechManager;\n\nlet processingRequest = false;\nlet recognitionStop = false;\n\nlet numTransactions;\n\nfunction setFace(au_data){\n    // Extrapolated from AnimationManager.js\n    au_data.forEach(({ id, intensity, duration, explanation = \"\" }) => {\n        animationManager.scheduleChange(id, intensity * 90, duration, 0, explanation);\n    });\n}\n\nfunction slightSmileFace(){\n    let au_data = [\n        {\n          \"id\": \"6\",\n          \"intensity\": 0.4,\n          \"duration\": 750,\n          \"explanation\": \"\"\n        },\n        {\n          \"id\": \"12\",\n          \"intensity\": 0.5,\n          \"duration\": 750,\n          \"explanation\": \"\"\n        }\n      ]\n\n    setFace(au_data=au_data)\n}\n\nfunction wink(){\n    let au_data = [\n        {\n            \"id\": \"46L\",\n            \"intensity\": 0.9,\n            \"duration\": 300,\n            \"explanation\": \"\"\n        }\n    ]\n\n    setFace(au_data=au_data)\n}\n\nfunction neutralFace(){\n    animationManager.setFaceToNeutral()\n}\n\n\nfunction delay(ms) {\n    return new Promise(resolve => setTimeout(resolve, ms));\n}\n\nasync function agentSpeak(text) {\n    speechManager.initSynthesizer();\n    let synthesize_result = await speechManager.synthesizeSpeech(text);\n\n    console.log('synthesize_result object:', synthesize_result);\n    console.log('synthesize result transcript:', text)\n    console.log('synthesize result audioDuration:', synthesize_result.audioDuration);\n\n    console.log('before delay!');\n\n    try {\n        await delay(synthesize_result.audioDuration / 10000);\n        console.log('after delay!');\n    } catch (error) {\n        if (error.message === 'Delay aborted') {\n            console.log('Delay was aborted');\n        } else {\n            console.error('Error during delay:', error);\n        }\n    }\n}\n\nasync function processSpeech(transcript, gptManager){\n    let gptResponse = ''\n\n    try{\n        gptResponse = await gptManager.processText(transcript)\n        \n        // gptResponse = transcript\n\n        console.log('gptResponse: ', gptResponse)\n    }\n    catch (e){\n        console.error('Error processing GPT response:', e)\n    }\n    try{\n        if(gptResponse){\n            await agentSpeak(gptResponse)\n            numTransactions += 1\n        }\n\n        return gptResponse\n    }\n    catch (e) {\n        console.error('Error processing TTS response:', e)\n    }\n}\n\nasync function start(animationManager, settings) {\n    console.log('Calling start!')\n\n    console.log('openai key object:', settings.openai_api_key)\n    console.log('azure key object:', settings.azure_api_key)\n\n    console.log('openai key object type:', typeof settings.openai_api_key)\n    console.log('azure key object type:', typeof settings.azure_api_key)\n\n    console.log('openai key default:', settings.openai_api_key.default)\n    console.log('azure key default:', settings.azure_api_key.default)\n\n    numTransactions = 1\n\n    // Initialize SpeechManager object\n    if (settings.azure_api_key && typeof settings.azure_api_key == 'string'){\n        speechManager = new SpeechManager(animationManager, settings.azure_api_key, 'eastus')\n    }\n    else{\n        speechManager = new SpeechManager(animationManager, settings.azure_api_key.default, 'eastus')\n    }\n\n    slightSmileFace()\n\n    await agentSpeak(`Hello, my name is eva. I am powered by ChatGPT. I am programmed to provide a response to ${settings.num_prompts} questions. Feel free to ask away!`)\n\n    neutralFace()\n\n    console.log('recognitionStop:', recognitionStop)\n\n    if ('webkitSpeechRecognition' in window) {\n        console.log('SpeechRecognition available!')\n        processingRequest = false\n\n        let transactions = {}\n\n\n        if(!recognitionStop){\n            recognitionAvailable = true\n            recognitionStop = false;\n        \n            // Initialize SpeechRecognition object\n            const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;\n            recognition = new SpeechRecognition();\n\n            // Initialize TextToGptReconciler object\n            if (settings.openai_api_key && typeof settings.openai_api_key == 'string'){\n                gptManager = new TextToGptReconciler(settings.openai_api_key)\n            }\n            else{\n                gptManager = new TextToGptReconciler(settings.openai_api_key.default)\n            }\n\n            recognition.continuous = true;\n            recognition.lang = 'en-US'\n            recognition.interimResults = true\n            recognition.maxAlternatives = 1\n        \n            recognition.onresult = async function(event) {\n                let transcript = '';\n                let isFinal = false;\n\n                let current_transaction = {}\n        \n                for (let i = event.resultIndex; i < event.results.length; i++) {\n                    transcript += event.results[i][0].transcript;\n                    if (event.results[i].isFinal) {\n                        isFinal = true;\n                    }\n                }\n\n                console.log('processingRequest:', processingRequest)\n        \n                // Process only final results to avoid duplicates\n                if (isFinal && transcript !== lastTranscript && !processingRequest) {\n                    lastTranscript = transcript;\n                    processingRequest = true;\n\n                    // Process transcript\n                    let processTranscript = transcript.toLowerCase().trim();\n\n                    console.log('Recognized Speech:', processTranscript)\n\n                    // const response = await ollama.chat({\n                    //     model: 'phi3',\n                    //     messages: [{role: 'user', content: transcript}]\n                    // })\n\n                    // console.log(\"llama response:\", response)\n\n                    if (processTranscript){\n\n                        console.log('openai key:', settings.openai_api_key)\n                        console.log('azure key:', settings.azure_api_key)\n\n                        current_transaction[\"user_prompt\"] = processTranscript\n\n                        let agentResponse = ''\n                        agentResponse = await processSpeech(processTranscript, gptManager)\n\n                        current_transaction['agent_response'] = agentResponse\n\n                        // console.log('returned speechResponse:', speechResponse)\n\n                        console.log('finished request!')\n                        console.log('numTransactions:', numTransactions)\n                        console.log('settings.num_prompts:', settings.num_prompts)\n\n                        transactions[numTransactions-1] = current_transaction\n\n                        processingRequest = false\n\n                        console.log('processingRequest:', processingRequest)\n                        \n                        if (numTransactions > settings.num_prompts){\n                            processingRequest = true;\n                            await agentSpeak(`It appears I've answered my ${settings.num_prompts} questions.`)\n                            await agentSpeak(\"If you'd like to review this conversation, a transcript is available in the inspector.\")\n                            await agentSpeak('Talk to you later!')\n\n                            slightSmileFace()\n                            wink()\n                            await delay(1000)\n                            neutralFace()\n\n                            let json_string = JSON.stringify(transactions, null, 2)\n\n                            console.log(`Transaction History:\\n${json_string}`)\n\n                            recognitionStop = true\n                            recognition.stop()\n                        }\n                    }\n                }\n            };\n            recognition.onend = function() {\n                if (!recognitionStop){\n                    console.log('Speech recognition restarting...');\n                    if (recognitionAvailable) {\n                        recognition.start();  // Restart recognition\n                        console.log('Speech recognition restarted!');\n                    }\n                }\n            };\n        }\n    }\n    else {\n        console.error('SpeechRecognition not available in this browser!')\n    }\n\n    if (recognitionAvailable == true) {\n        recognition.start()\n        console.log(\"speech recognition started!\")\n    }\n}\n\nfunction stop() {\n    console.log('Calling stop!')\n    \n    recognitionStop = true\n    console.log('recognitionStop:', recognitionStop)\n\n    // Interrupt Speech (TBD)\n    // speechManager.player.pause()\n    // speechManager.audioInterrupt = true\n    // setWaitCondition(speechManager.audioInterrupt)\n    \n    if (recognitionAvailable){\n        recognition.stop()    \n        console.log(\"speech recognition stopped!\")\n    }\n\n    console.log('Reached end stop!')\n}\n\nexport { start, stop }"],"names":["constructor","animationManager","apiKey","region","this","queue","isSpeaking","player","audioConfig","audioInterrupt","initSynthesizer","speechConfig","SpeechConfig","fromSubscription","speechSynthesisVoiceName","SpeakerAudioDestination","AudioConfig","fromSpeakerOutput","synthesizer","SpeechSynthesizer","visemeReceived","s","e","scheduleVisemeApplication","visemeId","audioOffset","enqueueText","text","push","processQueue","length","shift","synthesizeSpeech","then","Promise","resolve","reject","speakTextAsync","result","console","log","error","setTimeout","applyVisemeToCharacter","facsLib","setFaceToNeutral","setNeutralViseme","setTargetViseme","updateEngine","stopSpeech","close","interruptSpeech","Error","apiUrl","processText","payload","model","messages","role","content","arguments","undefined","max_tokens","temperature","_data$choices","_data$choices$","_data$choices$$messag","_data$choices$$messag2","response","fetch","method","headers","concat","body","JSON","stringify","ok","status","gptResponse","json","choices","message","trim","recognition","gptManager","speechManager","numTransactions","recognitionAvailable","lastTranscript","processingRequest","recognitionStop","setFace","au_data","forEach","_ref","id","intensity","duration","explanation","scheduleChange","slightSmileFace","neutralFace","delay","ms","async","agentSpeak","synthesize_result","audioDuration","start","settings","openai_api_key","azure_api_key","default","SpeechManager","num_prompts","window","transactions","SpeechRecognition","webkitSpeechRecognition","TextToGptReconciler","continuous","lang","interimResults","maxAlternatives","onresult","event","transcript","isFinal","current_transaction","i","resultIndex","results","processTranscript","toLowerCase","agentResponse","processSpeech","json_string","stop","onend"],"sourceRoot":""}