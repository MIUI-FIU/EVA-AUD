{"version":3,"file":"static/js/394.e141c71d.chunk.js","mappings":"wJAGA,MAiIA,EAjIsBA,IAA0C,IAAzC,kBAAEC,EAAiB,aAAEC,GAAcF,EACxD,MAAMG,GAAWC,EAAAA,EAAAA,UACXC,GAAYD,EAAAA,EAAAA,UACZE,GAAcF,EAAAA,EAAAA,UACdG,GAAYH,EAAAA,EAAAA,WAGlBI,EAAAA,EAAAA,YAAU,KACWC,WACjB,IACE,MAAMC,EAAYC,0BACZC,EAAAA,GAAaC,iBAAiBC,YAAYJ,SAC1CE,EAAAA,GAAaG,kBAAkBD,YAAYJ,SAC3CE,EAAAA,GAAaI,mBAAmBF,YAAYJ,SAC5CE,EAAAA,GAAaK,kBAAkBH,YAAYJ,GAEjDQ,GACF,CAAE,MAAOC,GACPC,QAAQD,MAAM,wBAAyBA,EACzC,GAGFE,EAAY,GACX,IAGH,MAAMH,EAAaA,KACjBI,UAAUC,aACPC,aAAa,CAAEC,MAAO,CAAC,IACvBC,MAAMC,IACLpB,EAAUqB,QAAUD,EACpBxB,EAASyB,QAAQC,UAAYF,CAAM,IAEpCG,OAAOC,GAAQX,QAAQD,MAAM,0BAA2BY,IAAK,EAI5DC,EAAYA,KACZzB,EAAUqB,SACZrB,EAAUqB,QAAQK,YAAYC,SAASC,GAAUA,EAAMC,QACzD,EAIIC,EAAa5B,UACjB,IAAKN,EAASyB,SAAWzB,EAASyB,QAAQU,QAAUnC,EAASyB,QAAQW,MACnE,OAGF,MAAMC,QAAmB5B,EAAAA,EAAuBT,EAASyB,QAAS,IAAIhB,EAAAA,IACnE6B,oBACAC,sBAEGC,EAAStC,EAAUuB,QACzB,IAAKe,EAEH,YADAvB,QAAQD,MAAM,2BAIhB,MAAMyB,EAAc,CAAEC,MAAO1C,EAASyB,QAAQkB,WAAYC,OAAQ5C,EAASyB,QAAQoB,aACnFpC,EAAAA,GAAwB+B,EAAQC,GAChC,MAAMK,EAAoBrC,EAAAA,GAAsB4B,EAAYI,GAQ5D,GANAD,EAAOO,WAAW,MAAMC,UAAU,EAAG,EAAGR,EAAOE,MAAOF,EAAOI,QAC7DnC,EAAAA,GAAAA,eAA4B+B,EAAQM,GACpCrC,EAAAA,GAAAA,kBAA+B+B,EAAQM,GACvCrC,EAAAA,GAAAA,oBAAiC+B,EAAQM,GAGrCT,EAAWY,OAAS,EAAG,CACzB,MAAMC,EAAcb,EAAW,GAAGa,YAS5BC,EALgBC,OAAOC,QAAQH,GAAaI,KAAIC,IAAA,IAAEC,EAASC,GAAWF,EAAA,MAAK,CAACC,EAASC,EAAYC,KAAKC,MAAM,IAKxEC,MAAK,CAACC,EAAGC,IAAMA,EAAE,GAAKD,EAAE,KAa9D/D,GACFA,EAAkBqD,EAEtB,GA0BF,OAtBA9C,EAAAA,EAAAA,YAAU,KACR,MAAM0D,EAAaA,KACZ5D,EAAYsB,UACftB,EAAYsB,QAAUuC,YAAY9B,EAAY,KAChD,EAGI+B,EAAejE,EAASyB,QAQ9B,OAPY,OAAZwC,QAAY,IAAZA,GAAAA,EAAcC,iBAAiB,OAAQH,GAGnChE,IACFA,EAAa0B,QAAUI,GAGlB,KACLsC,cAAchE,EAAYsB,SAC1BtB,EAAYsB,QAAU,KACV,OAAZwC,QAAY,IAAZA,GAAAA,EAAcG,oBAAoB,OAAQL,EAAW,CACtD,GACA,KAGDM,EAAAA,EAAAA,MAAA,OAAKC,MAAO,CAAEC,SAAU,YAAaC,SAAA,EACnCC,EAAAA,EAAAA,KAAA,SAAOC,IAAK1E,EAAU2E,UAAQ,EAACC,OAAK,EAAClC,MAAM,MAAME,OAAO,MAAM0B,MAAO,CAAEC,SAAU,eACjFE,EAAAA,EAAAA,KAAA,UAAQC,IAAKxE,EAAWoE,MAAO,CAAEC,SAAU,WAAYM,KAAM,EAAGC,IAAK,OACjE,C,sHC5HV,IAAIC,EAAO,KACPhF,EAAe,KAEZ,MAAMiF,EAAQA,CAACC,EAAkBC,EAAUC,KAChD,IAAKA,IAAiBA,EAAa1D,QAEjC,YADAR,QAAQD,MAAM,kFAIX+D,IACHA,EAAOK,EAAAA,WAA0BD,EAAa1D,UAahD1B,EAAesF,EAAAA,YAEfN,EAAKO,QACHb,EAAAA,EAAAA,KAACc,EAAAA,EAAa,CACZN,iBAAkBA,EAClBC,SAAUA,EACVpF,kBAf2B0F,IAC7BvE,QAAQwE,IAAI,iCAAkCD,GAE9C,IAAIE,EAAkBF,EAAqB,GAE3CvE,QAAQwE,IAAI,qBAAsBC,EAAgB,EAWhD3F,aAAcA,IAEjB,EAGUkC,EAAOA,KACdlC,GAAgBA,EAAa0B,SAC/B1B,EAAa0B,UAGXsD,IACFA,EAAKY,UACLZ,EAAO,KACT,C","sources":["components/FaceDetection.jsx","modules/faceDetectionApp.js"],"sourcesContent":["import React, { useRef, useEffect } from 'react';\nimport * as faceapi from 'face-api.js';\n\nconst FaceDetection = ({ onEmotionDetected, stopVideoRef }) => {\n  const videoRef = useRef(); // Reference to the video element\n  const canvasRef = useRef(); // Reference to the canvas element\n  const intervalRef = useRef(); // Reference to store the interval ID\n  const streamRef = useRef(); // Reference to store the video stream\n\n  // Function to load Face API models\n  useEffect(() => {\n    const loadModels = async () => {\n      try {\n        const MODEL_URL = process.env.PUBLIC_URL + '/models';\n        await faceapi.nets.tinyFaceDetector.loadFromUri(MODEL_URL);\n        await faceapi.nets.faceLandmark68Net.loadFromUri(MODEL_URL);\n        await faceapi.nets.faceRecognitionNet.loadFromUri(MODEL_URL);\n        await faceapi.nets.faceExpressionNet.loadFromUri(MODEL_URL);\n\n        startVideo(); // Start webcam after models are loaded\n      } catch (error) {\n        console.error('Error loading models:', error);\n      }\n    };\n\n    loadModels();\n  }, []);\n\n  // Function to start the webcam stream\n  const startVideo = () => {\n    navigator.mediaDevices\n      .getUserMedia({ video: {} })\n      .then((stream) => {\n        streamRef.current = stream; // Store the stream reference\n        videoRef.current.srcObject = stream; // Attach the webcam stream to the video element\n      })\n      .catch((err) => console.error('Error accessing webcam:', err));\n  };\n\n  // Function to stop the webcam stream\n  const stopVideo = () => {\n    if (streamRef.current) {\n      streamRef.current.getTracks().forEach((track) => track.stop()); // Stop all tracks of the stream\n    }\n  };\n\n  // Detect faces and draw bounding boxes\n  const detectFace = async () => {\n    if (!videoRef.current || videoRef.current.paused || videoRef.current.ended) {\n      return; // Stop detection if video is not ready\n    }\n\n    const detections = await faceapi.detectAllFaces(videoRef.current, new faceapi.TinyFaceDetectorOptions())\n      .withFaceLandmarks()\n      .withFaceExpressions();\n\n    const canvas = canvasRef.current;\n    if (!canvas) {\n      console.error('Canvas is not available');\n      return;\n    }\n\n    const displaySize = { width: videoRef.current.videoWidth, height: videoRef.current.videoHeight };\n    faceapi.matchDimensions(canvas, displaySize);\n    const resizedDetections = faceapi.resizeResults(detections, displaySize);\n\n    canvas.getContext('2d').clearRect(0, 0, canvas.width, canvas.height);\n    faceapi.draw.drawDetections(canvas, resizedDetections);\n    faceapi.draw.drawFaceLandmarks(canvas, resizedDetections);\n    faceapi.draw.drawFaceExpressions(canvas, resizedDetections);\n\n    // Log the detected emotions\n    if (detections.length > 0) {\n      const expressions = detections[0].expressions;\n      // console.log('expressions:', expressions);\n    \n      // Create a list of tuples (emotion, confidence, timestamp)\n      const emotionTuples = Object.entries(expressions).map(([emotion, confidence]) => [emotion, confidence, Date.now()]);\n    \n      // console.log('Emotion tuples:', emotionTuples);\n    \n      // Sort the emotion tuples based on confidence in descending order\n      const sortedEmotionTuples = emotionTuples.sort((a, b) => b[1] - a[1]);\n    \n      // Create a list of emotion strings ordered by confidence\n      // const orderedEmotions = sortedEmotionTuples.map(tuple => tuple[0]);\n    \n      // console.log('Ordered emotions based on confidence:', orderedEmotions);\n    \n      // Get the emotion with the highest confidence\n      // const [emotion, confidence] = sortedEmotionTuples[0];\n    \n      // console.log('FaceDetection current emotion:', emotion, 'with confidence:', confidence);\n    \n      // Trigger the callback if provided\n      if (onEmotionDetected) {\n        onEmotionDetected(sortedEmotionTuples); // Pass both emotion and confidence to the callback\n      }\n    }\n  };\n\n  // Detect faces every 100 milliseconds\n  useEffect(() => {\n    const handlePlay = () => {\n      if (!intervalRef.current) {\n        intervalRef.current = setInterval(detectFace, 100);\n      }\n    };\n\n    const videoElement = videoRef.current;\n    videoElement?.addEventListener('play', handlePlay);\n\n    // Pass stopVideo function back to parent through ref\n    if (stopVideoRef) {\n      stopVideoRef.current = stopVideo;\n    }\n\n    return () => {\n      clearInterval(intervalRef.current); // Cleanup interval on unmount\n      intervalRef.current = null;\n      videoElement?.removeEventListener('play', handlePlay);\n    };\n  }, []);\n\n  return (\n    <div style={{ position: 'relative' }}>\n      <video ref={videoRef} autoPlay muted width=\"480\" height=\"270\" style={{ position: 'relative' }} />\n      <canvas ref={canvasRef} style={{ position: 'absolute', left: 0, top: 0 }} />\n    </div>\n  );\n};\n\nexport default FaceDetection;\n","import ReactDOMClient from 'react-dom/client';\nimport React, { useRef } from 'react';\nimport FaceDetection from '../components/FaceDetection';\n\nlet root = null;\nlet stopVideoRef = null; // Reference to store stopVideo function\n\nexport const start = (animationManager, settings, containerRef) => {\n  if (!containerRef || !containerRef.current) {\n    console.error('Container reference not provided or invalid. Unable to start FaceDetectionApp.');\n    return;\n  }\n\n  if (!root) {\n    root = ReactDOMClient.createRoot(containerRef.current);\n  }\n\n  // Callback function to handle detected emotions\n  const handleEmotionDetected = (sortedEmotionsTuples) => {\n    console.log('Detected sortedEmotionsTuples:', sortedEmotionsTuples);\n\n    let topEmotionTuple = sortedEmotionsTuples[0]\n\n    console.log('Top Emotion Tuple:', topEmotionTuple)\n  };\n\n  // Create a ref to pass to FaceDetection for stopping video\n  stopVideoRef = React.createRef();\n\n  root.render(\n    <FaceDetection\n      animationManager={animationManager}\n      settings={settings}\n      onEmotionDetected={handleEmotionDetected}\n      stopVideoRef={stopVideoRef} // Pass the stopVideoRef to FaceDetection\n    />\n  );\n};\n\nexport const stop = () => {\n  if (stopVideoRef && stopVideoRef.current) {\n    stopVideoRef.current(); // Call stopVideo from FaceDetection\n  }\n\n  if (root) {\n    root.unmount();\n    root = null;\n  }\n};"],"names":["_ref","onEmotionDetected","stopVideoRef","videoRef","useRef","canvasRef","intervalRef","streamRef","useEffect","async","MODEL_URL","process","faceapi","tinyFaceDetector","loadFromUri","faceLandmark68Net","faceRecognitionNet","faceExpressionNet","startVideo","error","console","loadModels","navigator","mediaDevices","getUserMedia","video","then","stream","current","srcObject","catch","err","stopVideo","getTracks","forEach","track","stop","detectFace","paused","ended","detections","withFaceLandmarks","withFaceExpressions","canvas","displaySize","width","videoWidth","height","videoHeight","resizedDetections","getContext","clearRect","length","expressions","sortedEmotionTuples","Object","entries","map","_ref2","emotion","confidence","Date","now","sort","a","b","handlePlay","setInterval","videoElement","addEventListener","clearInterval","removeEventListener","_jsxs","style","position","children","_jsx","ref","autoPlay","muted","left","top","root","start","animationManager","settings","containerRef","ReactDOMClient","React","render","FaceDetection","sortedEmotionsTuples","log","topEmotionTuple","unmount"],"sourceRoot":""}