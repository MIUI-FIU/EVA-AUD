{"version":3,"file":"static/js/713.3911a410.chunk.js","mappings":"kIAyMA,QAvMA,MACIA,WAAAA,CAAYC,EAAkBC,EAAQC,GAClCC,KAAKH,iBAAmBA,EACxBG,KAAKF,OAASA,EACdE,KAAKD,OAASA,EACdC,KAAKC,MAAQ,GACbD,KAAKE,YAAa,EAClBF,KAAKG,gBAAiB,EAEtBH,KAAKI,aAAeC,EAAAA,aAAaC,iBAAiBN,KAAKF,OAAQE,KAAKD,QAGpEC,KAAKO,iBACT,CAEAC,2BAAAA,GACI,OAAO,IAAIC,SAAQ,CAACC,EAASC,KACzB,MAAMC,EAAcC,EAAAA,YAAYC,6BAC1BC,EAAa,IAAIC,EAAAA,iBAAiBhB,KAAKI,aAAcQ,GAE3D,IAAIK,EAAiB,GACjBC,EAAe,KAEnB,IAAIC,EAA4B,KAGhCD,EAAeE,YAAW,KACtBC,QAAQC,IAAI,kEACZP,EAAWQ,gCAA+B,KACtCb,EAAQ,CACJc,KAAMP,EAAeQ,OACrBC,UAAWP,GACb,GACJ,GAXmB,KAezBJ,EAAWY,YAAc,CAACC,EAAGC,KACzBR,QAAQC,IAAI,+BACZD,QAAQC,IAAI,mBAADQ,OAAoBD,EAAEE,OAAOP,OAGxCQ,aAAad,GACbA,EAAeE,YAAW,KACtBC,QAAQC,IAAI,oDACZP,EAAWQ,gCAA+B,KACtCb,EAAQ,CACJc,KAAMP,EAAeQ,OACrBC,UAAWP,GACb,GACJ,GA5Be,IA6BD,EAIxBJ,EAAWkB,WAAa,CAACL,EAAGC,KACxBR,QAAQC,IAAI,8BACRO,EAAEE,OAAOG,SAAWC,EAAAA,aAAaC,kBACjCf,QAAQC,IAAI,iBAADQ,OAAkBD,EAAEE,OAAOP,OACtCP,GAAkBY,EAAEE,OAAOP,KAAO,IAClCL,EAA4BkB,KAAKC,OAC1BT,EAAEE,OAAOG,SAAWC,EAAAA,aAAaI,SACxClB,QAAQC,IAAI,kBAChB,EAIJP,EAAWyB,SAAW,CAACZ,EAAGC,KACtBR,QAAQoB,MAAM,4BACdpB,QAAQoB,MAAM,yBAADX,OAA0BD,EAAEK,SACzCnB,EAAWQ,gCAA+B,KACtCZ,EAAO,IAAI+B,MAAM,yBAADZ,OAA0BD,EAAEK,SAAU,GACxD,EAINnB,EAAW4B,eAAiB,CAACf,EAAGC,KAC5BR,QAAQC,IAAI,kCACZP,EAAWQ,gCAA+B,KACtCb,EAAQ,CACJc,KAAMP,EAAeQ,OACrBC,UAAWP,GACb,GACJ,EAGNE,QAAQC,IAAI,sCACZP,EAAW6B,iCACP,IAAMvB,QAAQC,IAAI,sCACjBuB,IACGxB,QAAQoB,MAAM,8BAA+BI,GAC7ClC,EAAOkC,EAAI,GAElB,GAET,CAEAtC,eAAAA,GACIP,KAAKI,aAAa0C,yBAA2B,oBAE7C,MAAMC,EAAS,IAAIC,EAAAA,wBACbpC,EAAcC,EAAAA,YAAYoC,kBAAkBF,GAElD/C,KAAKkD,YAAc,IAAIC,EAAAA,kBAAkBnD,KAAKI,aAAcQ,GAE5DZ,KAAKkD,YAAYE,eAAiB,CAACxB,EAAGC,KAClC7B,KAAKqD,0BAA0BxB,EAAEyB,SAAUzB,EAAE0B,YAAY,CAEjE,CAEAC,WAAAA,CAAYhC,GACRxB,KAAKC,MAAMwD,KAAKjC,GACXxB,KAAKE,YACNF,KAAK0D,cAEb,CAEAA,YAAAA,GACI,GAA0B,IAAtB1D,KAAKC,MAAM0D,OAEX,YADA3D,KAAKE,YAAa,GAItBF,KAAKE,YAAa,EAClB,MAAMsB,EAAOxB,KAAKC,MAAM2D,QACxB5D,KAAK6D,iBAAiBrC,GAAMsC,MAAK,IAAM9D,KAAK0D,gBAChD,CAEAG,gBAAAA,CAAiBrC,GACb,OAAO,IAAIf,SAAQ,CAACC,EAASC,KACzBX,KAAKkD,YAAYa,eAAevC,GAC5BO,IACIV,QAAQC,IAAI,+BACZZ,EAAQqB,EAAO,IAEnBU,IACIpB,QAAQC,IAAI,6CACZX,EAAO8B,EAAM,GACf,GAEd,CAEAY,yBAAAA,CAA0BC,EAAUC,GAEhCnC,YAAW,KACPpB,KAAKgE,uBAAuBV,EAAS,GAFZC,EAAc,IAI/C,CAEAS,sBAAAA,CAAuBV,GACnB,MAAMW,EAAUjE,KAAKH,iBAAiBoE,QAEjCjE,KAAKG,gBASNH,KAAKH,iBAAiBqE,mBACtBD,EAAQE,oBATS,IAAbb,EACAW,EAAQE,iBAAiB,IAEzBb,GAAY,EACZW,EAAQG,gBAAgBd,EAAU,GAAI,IAQ9CW,EAAQI,cACZ,CAEAC,UAAAA,GAEItE,KAAKkD,YAAYqB,QAGjBvE,KAAKC,MAAQ,GACbD,KAAKE,YAAa,EAGlBF,KAAKO,kBAELc,QAAQC,IAAI,4BAChB,CAEAkD,eAAAA,CAAgBhD,GAEZxB,KAAKkD,YAAYqB,QAGjBvE,KAAKC,MAAQ,GACbD,KAAKE,YAAa,EAClBF,KAAKO,kBAEDiB,GACAxB,KAAKwD,YAAYhC,GAGrBH,QAAQC,IAAI,gCAChB,E,gDC/IF,QAvDF,MACI1B,WAAAA,CAAYE,GACV,IAAKA,GAA4B,kBAAXA,EACpB,MAAM,IAAI4C,MAAM,4CAElB1C,KAAKF,OAASA,EACdE,KAAKyE,OAAS,4CAChB,CAQA,iBAAMC,CAAYlD,GAChB,MAAMmD,EAAU,CACdC,MAAO,gBACPC,SAAU,CACR,CAAEC,KAAM,SAAUC,QAJWC,UAAArB,OAAA,QAAAsB,IAAAD,UAAA,GAAAA,UAAA,GAAG,oCAKhC,CAAEF,KAAM,OAAQC,QAASvD,IAE3B0D,WAAY,IACZC,YAAa,IAGf,IAAK,IAADC,EAAAC,EAAAC,EAAAC,EACF,MAAMC,QAAiBC,MAAMzF,KAAKyE,OAAQ,CACxCiB,OAAQ,OACRC,QAAS,CACP,cAAgB,UAAD7D,OAAY9B,KAAKF,QAChC,eAAgB,oBAElB8F,KAAMC,KAAKC,UAAUnB,KAGvB,IAAKa,EAASO,GACZ,MAAM,IAAIrD,MAAM,mCAADZ,OAAoC0D,EAASQ,SAG9D,MACMC,EAA0B,QAAfb,SADEI,EAASU,QACHC,eAAO,IAAAf,GAAK,QAALC,EAAZD,EAAe,UAAE,IAAAC,GAAS,QAATC,EAAjBD,EAAmBe,eAAO,IAAAd,GAAS,QAATC,EAA1BD,EAA4BP,eAAO,IAAAQ,OAAvB,EAAZA,EAAqC9D,OAEzD,IAAKwE,EACH,MAAM,IAAIvD,MAAM,+CAGlB,OAAOuD,CACT,CAAE,MAAOxD,GAEP,MADApB,QAAQoB,MAAM,kCAAmCA,GAC3CA,CACR,CACF,E,gGCjDJ,IACI4D,EAGAC,EAEAC,EAKAC,EAXAC,GAAuB,EAEvBC,EAAiB,GAMjBC,GAAoB,EACpBC,GAAkB,EAItB,SAASC,EAAQC,GAEbA,EAAQC,SAAQC,IAAoD,IAAnD,GAAEC,EAAE,UAAEC,EAAS,SAAEC,EAAQ,YAAEC,EAAc,IAAIJ,EAC1DnH,iBAAiBwH,eAAeJ,EAAgB,GAAZC,EAAgBC,EAAU,EAAGC,EAAY,GAErF,CAEA,SAASE,IACL,IAAIR,EAAU,CACV,CACE,GAAM,IACN,UAAa,GACb,SAAY,IACZ,YAAe,IAEjB,CACE,GAAM,KACN,UAAa,GACb,SAAY,IACZ,YAAe,KAIrBD,EAAgBC,EACpB,CAeA,SAASS,IACL1H,iBAAiBqE,kBACrB,CAyFA,SAASsD,EAAMC,GACX,OAAO,IAAIhH,SAAQC,GAAWU,WAAWV,EAAS+G,IACtD,CAEAC,eAAeC,EAAWnG,GACtB+E,EAAchG,kBACd,IAAIqH,QAA0BrB,EAAc1C,iBAAiBrC,GAE7DH,QAAQC,IAAI,4BAA6BsG,GACzCvG,QAAQC,IAAI,gCAAiCE,GAC7CH,QAAQC,IAAI,mCAAoCsG,EAAkBC,eAElExG,QAAQC,IAAI,iBAEZ,IAGI,aAFMkG,EAAMI,EAAkBC,cAAgB,KAC9CxG,QAAQC,IAAI,gBACLE,CACX,CAAE,MAAOiB,GACiB,kBAAlBA,EAAM2D,QACN/E,QAAQC,IAAI,qBAEZD,QAAQoB,MAAM,sBAAuBA,EAE7C,CACJ,CAEAiF,eAAeI,EAAcC,EAAYzB,GACrC,IAAIL,EAAc,GAElB,IACIA,QAAoBK,EAAW5B,YAAYqD,GAI3C1G,QAAQC,IAAI,gBAAiB2E,EACjC,CACA,MAAOpE,GACHR,QAAQoB,MAAM,iCAAkCZ,EACpD,CACA,IAMI,OALGoE,UACO0B,EAAW1B,GACjBO,GAAmB,GAGhBP,CACX,CACA,MAAOpE,GACHR,QAAQoB,MAAM,iCAAkCZ,EACpD,CACJ,CAEA6F,eAAeM,EAAMnI,EAAkBoI,GACnCrB,GAAkB,EAIdL,EADA0B,EAASC,eAAkD,iBAA1BD,EAASC,cAC1B,IAAIC,EAAAA,EAActI,EAAkBoI,EAASC,cAAe,UAG5D,IAAIC,EAAAA,EAActI,EAAkBoI,EAASC,cAAcE,QAAS,UAKpF9B,EADA2B,EAASI,gBAAoD,iBAA3BJ,EAASI,eAC9B,IAAIC,EAAAA,EAAoBL,EAASI,gBAGjC,IAAIC,EAAAA,EAAoBL,EAASI,eAAeD,SAGjE,IAAIG,EAAY,CACZ,EAAK,6BACL,EAAK,iCACL,EAAK,uCACL,EAAK,oDACL,EAAK,iCACL,EAAK,kCAGTlH,QAAQC,IAAI,cAADQ,OAAeyG,IAE1B,IAAIC,EAAgBC,OAAOC,KAAKH,GAAW5E,OAE3C2D,UAEMK,EAAW,yCAAD7F,OAA0C0G,EAAa,6DAEvEjB,IAEA,IAAIoB,EAAe,CAAC,EAEpB,IAAI,IAAIC,EAAI,EAAGA,GAAKJ,EAAeI,IAAI,CACnCvH,QAAQC,IAAI,qBAADQ,OAAsB8G,IAEjC,IAAIC,EAAsB,CAAC,EAEvBC,QAAuBnB,EAAWY,EAAUK,IAChDC,EAAoC,eAAIC,EACxCzH,QAAQC,IAAI,kBAAmBwH,GAE/B,IAAIC,QA5LD,IAAItI,SAASC,IAChB,GAAI,4BAA6BsI,QAM7B,GALA3H,QAAQC,IAAI,gCACZqF,GAAoB,EAEpBtF,QAAQC,IAAI,mBAAoBsF,IAE3BA,EAAiB,CAClBH,GAAuB,EACvBG,GAAkB,EAGlB,MAAMqC,EAAoBD,OAAOC,mBAAqBD,OAAOE,wBAQ7D,IAAIhI,EAPJmF,EAAc,IAAI4C,EAElB5C,EAAY8C,YAAa,EACzB9C,EAAY+C,KAAO,QACnB/C,EAAYgD,gBAAiB,EAC7BhD,EAAYiD,gBAAkB,EAI9BjD,EAAYkD,SAAW7B,eAAe8B,GAClC,IAAIzB,EAAa,GACb0B,GAAU,EAEd,IAAK,IAAIb,EAAIY,EAAME,YAAad,EAAIY,EAAMG,QAAQhG,OAAQiF,IACtDb,GAAcyB,EAAMG,QAAQf,GAAG,GAAGb,WAC9ByB,EAAMG,QAAQf,GAAGa,UACjBA,GAAU,GAOlB,GAHApI,QAAQC,IAAI,qBAAsBqF,GAG9B8C,GAAW1B,IAAerB,IAAmBC,EAAmB,CAChED,EAAiBqB,EACjBpB,GAAoB,EAGpB,IAAIiD,EAAoB7B,EAAW8B,cAAcpI,OACjDJ,QAAQC,IAAI,qBAAsBsI,GAGlC5H,aAAad,GACbmF,EAAYyD,OAGZnD,GAAoB,EACpBjG,EAAQqH,EACZ,MAEI/F,aAAad,GACbA,EAAeE,YAAW,KACtBC,QAAQC,IAAI,6CACZ+E,EAAYyD,OACZpJ,EAAQqH,EAAW,GACpB,IAEX,EAEA1B,EAAY0D,MAAQ,KAChB1I,QAAQC,IAAI,6BACZZ,EAAQgG,EAAe,EAG3BL,EAAY2D,QAAWR,IACnBnI,QAAQoB,MAAM,4BAA6B+G,EAAM/G,OACjDT,aAAad,GACbR,EAAQgG,EAAe,CAE/B,OAEArF,QAAQoB,MAAM,oDACd/B,IAGA+F,IACAJ,EAAY2B,QACZ3G,QAAQC,IAAI,+BAChB,IA4GAuH,EAAmC,cAAIE,EACvC1H,QAAQC,IAAI,iBAAkByH,GAE9B,IAAIkB,EAAiB,wJAAAnI,OAA2JgH,EAAc,oBAAAhH,OAAmBiH,GAE7MmB,QAAuBpC,EAAcmC,EAAmB3D,GAC5DuC,EAAoC,eAAIqB,EACxC7I,QAAQC,IAAI,kBAAmB4I,GAE/BvB,EAAaC,GAAKC,CACtB,OAEMlB,EAAW,iDACXA,EAAW,gGACXA,EAAW,0CAEjBL,IArNAT,EATc,CACV,CACI,GAAM,MACN,UAAa,GACb,SAAY,IACZ,YAAe,YA2NjBW,EAAM,KACZD,IAEA,IAAI4C,EAActE,KAAKC,UAAU6C,EAAc,KAAM,GAErDtH,QAAQC,IAAI,yBAADQ,OAA0BqI,GACzC,CAEA,SAASL,IACLzI,QAAQC,IAAI,iBAEZsF,GAAkB,EAClBvF,QAAQC,IAAI,mBAAoBsF,GAE5BH,IACAJ,EAAYyD,OACZzI,QAAQC,IAAI,gCAGhBD,QAAQC,IAAI,oBAChB,C","sources":["VISOS/action/verbalizers/SpeechManager.js","VISOS/cognition/TextToGptReconciler.js","modules/evaSimpleDialog.js"],"sourcesContent":["import { AudioConfig, PropertyId, ResultReason, SpeakerAudioDestination, SpeechConfig, SpeechRecognizer, SpeechSynthesizer } from \"microsoft-cognitiveservices-speech-sdk\";\n\nclass SpeechManager {\n    constructor(animationManager, apiKey, region) {\n        this.animationManager = animationManager;\n        this.apiKey = apiKey,\n        this.region = region,\n        this.queue = [];\n        this.isSpeaking = false;\n        this.audioInterrupt = false;\n\n        this.speechConfig = SpeechConfig.fromSubscription(this.apiKey, this.region)\n\n        // Initialize synthesizer\n        this.initSynthesizer();\n    }\n\n    recognizeSpeechUntilSilence() {\n        return new Promise((resolve, reject) => {\n            const audioConfig = AudioConfig.fromDefaultMicrophoneInput();\n            const recognizer = new SpeechRecognizer(this.speechConfig, audioConfig);\n    \n            let recognizedText = '';\n            let silenceTimer = null;\n            const silenceThreshold = 3000; // Silence threshold in milliseconds (3 seconds)\n            let finalRecognitionTimestamp = null; // To store the timestamp of the final recognition event\n            \n            // Start the silence timer immediately\n            silenceTimer = setTimeout(() => {\n                console.log('Silence detected (before recognition), stopping recognition...');\n                recognizer.stopContinuousRecognitionAsync(() => {\n                    resolve({\n                        text: recognizedText.trim(), \n                        timestamp: finalRecognitionTimestamp\n                    }); // Resolve with any recognized text (may be empty if no recognition)\n                });\n            }, silenceThreshold);\n    \n            // Handle partial results\n            recognizer.recognizing = (s, e) => {\n                console.log('Recognizing event triggered');\n                console.log(`Partial result: ${e.result.text}`);\n                \n                // If there's speech, reset the silence timer\n                clearTimeout(silenceTimer);\n                silenceTimer = setTimeout(() => {\n                    console.log('Silence detected during recognition, stopping...');\n                    recognizer.stopContinuousRecognitionAsync(() => {\n                        resolve({\n                            text: recognizedText.trim(),\n                            timestamp: finalRecognitionTimestamp\n                        }); // Resolve with the recognized text and timestamp\n                    });\n                }, silenceThreshold);\n            };\n    \n            // Handle final recognition results\n            recognizer.recognized = (s, e) => {\n                console.log('Recognized event triggered');\n                if (e.result.reason === ResultReason.RecognizedSpeech) {\n                    console.log(`Final result: ${e.result.text}`);\n                    recognizedText += e.result.text + ' ';\n                    finalRecognitionTimestamp = Date.now(); // Capture the timestamp of the final result\n                } else if (e.result.reason === ResultReason.NoMatch) {\n                    console.log('No match found.');\n                }\n            };\n    \n            // Handle cancellation events (e.g., when the recognition is interrupted)\n            recognizer.canceled = (s, e) => {\n                console.error('Canceled event triggered');\n                console.error(`Recognition canceled: ${e.reason}`);\n                recognizer.stopContinuousRecognitionAsync(() => {\n                    reject(new Error(`Recognition canceled: ${e.reason}`));\n                });\n            };\n    \n            // Handle session stopped event (e.g., when recognition ends)\n            recognizer.sessionStopped = (s, e) => {\n                console.log('Session stopped due to silence');\n                recognizer.stopContinuousRecognitionAsync(() => {\n                    resolve({\n                        text: recognizedText.trim(), \n                        timestamp: finalRecognitionTimestamp\n                    }); // Return the accumulated text and final timestamp\n                });\n            };\n    \n            console.log('Starting continuous recognition...');\n            recognizer.startContinuousRecognitionAsync(\n                () => console.log('Recognition started successfully'),\n                (err) => {\n                    console.error('Error starting recognition:', err);\n                    reject(err);\n                }\n            );\n        });\n    }\n    \n    initSynthesizer() {\n        this.speechConfig.speechSynthesisVoiceName = \"en-US-JennyNeural\";\n\n        const player = new SpeakerAudioDestination();;\n        const audioConfig = AudioConfig.fromSpeakerOutput(player);\n\n        this.synthesizer = new SpeechSynthesizer(this.speechConfig, audioConfig);\n\n        this.synthesizer.visemeReceived = (s, e) => {\n            this.scheduleVisemeApplication(e.visemeId, e.audioOffset);\n        };\n    }\n\n    enqueueText(text) {\n        this.queue.push(text);\n        if (!this.isSpeaking) {\n            this.processQueue();\n        }\n    }\n\n    processQueue() {\n        if (this.queue.length === 0) {\n            this.isSpeaking = false;\n            return;\n        }\n\n        this.isSpeaking = true;\n        const text = this.queue.shift();\n        this.synthesizeSpeech(text).then(() => this.processQueue());\n    }\n\n    synthesizeSpeech(text) {\n        return new Promise((resolve, reject) => {\n            this.synthesizer.speakTextAsync(text,\n                result => {\n                    console.log(\"Speech synthesis completed.\");\n                    resolve(result);\n                },\n                error => {\n                    console.log(\"Error during speech synthesis, restarting\");\n                    reject(error);\n                });\n        });\n    }\n\n    scheduleVisemeApplication(visemeId, audioOffset) {\n        const offsetInMilliseconds = audioOffset / 10000;\n        setTimeout(() => {\n            this.applyVisemeToCharacter(visemeId);\n        }, offsetInMilliseconds);\n    }\n\n    applyVisemeToCharacter(visemeId) {\n        const facsLib = this.animationManager.facsLib;\n        // console.log('audioInterrupt:', this.audioInterrupt)\n        if (!this.audioInterrupt){\n            if (visemeId === 0) {\n                facsLib.setNeutralViseme(0.0);\n            } else {\n                visemeId -= 1;\n                facsLib.setTargetViseme(visemeId, 70, 0);\n            }\n        }\n        else{\n            this.animationManager.setFaceToNeutral()\n            facsLib.setNeutralViseme()\n        }\n\n        facsLib.updateEngine();\n    }\n\n    stopSpeech() {\n        // Dispose of the current synthesizer to stop speech\n        this.synthesizer.close();\n\n        // Clear the queue and reset the speaking state\n        this.queue = [];\n        this.isSpeaking = false;\n\n        // Reinitialize the synthesizer for future use\n        this.initSynthesizer();\n\n        console.log(\"Speech synthesis stopped.\");\n    }\n\n    interruptSpeech(text) {\n        // Dispose of the current synthesizer to stop speech\n        this.synthesizer.close();\n\n        // Clear the queue, reset the speaking state, and reinitialize the synthesizer\n        this.queue = [];\n        this.isSpeaking = false;\n        this.initSynthesizer();\n\n        if (text) {\n            this.enqueueText(text); // Enqueue and play the new text\n        }\n\n        console.log(\"Speech synthesis interrupted.\");\n    }\n}\n\nexport default SpeechManager;\n","class TextToGptReconciler {\n    constructor(apiKey) {\n      if (!apiKey || typeof apiKey !== 'string') {\n        throw new Error('A valid OpenAI API key must be provided.');\n      }\n      this.apiKey = apiKey;\n      this.apiUrl = 'https://api.openai.com/v1/chat/completions';\n    }\n  \n    /**\n     * Processes the provided text by sending it to the OpenAI API.\n     * @param {string} text - The input text to process.\n     * @param {string} instruction - Instruction or system prompt for the AI.\n     * @returns {Promise<string>} - The GPT response.\n     */\n    async processText(text, instruction = 'Answer in a professional manner:') {\n      const payload = {\n        model: 'gpt-3.5-turbo',  // You can switch this model based on your requirement\n        messages: [\n          { role: 'system', content: instruction },\n          { role: 'user', content: text }\n        ],\n        max_tokens: 150,  // Adjust as necessary\n        temperature: 0.7\n      };\n  \n      try {\n        const response = await fetch(this.apiUrl, {\n          method: 'POST',\n          headers: {\n            'Authorization': `Bearer ${this.apiKey}`,\n            'Content-Type': 'application/json'\n          },\n          body: JSON.stringify(payload)\n        });\n  \n        if (!response.ok) {\n          throw new Error(`API request failed with status: ${response.status}`);\n        }\n  \n        const data = await response.json();\n        const gptResponse = data.choices?.[0]?.message?.content?.trim();\n  \n        if (!gptResponse) {\n          throw new Error('Failed to get a valid response from the API');\n        }\n  \n        return gptResponse;\n      } catch (error) {\n        console.error('Error processing text with GPT:', error);\n        throw error;\n      }\n    }\n  }\n  \n  export default TextToGptReconciler;","import SpeechManager from \"../VISOS/action/verbalizers/SpeechManager\";\nimport TextToGptReconciler from \"../VISOS/cognition/TextToGptReconciler\";\n\nlet recognitionAvailable = false;\nlet recognition;\nlet lastTranscript = '';\n\nlet gptManager;\n\nlet speechManager;\n\nlet processingRequest = false;\nlet recognitionStop = false;\n\nlet numTransactions;\n\nfunction setFace(au_data){\n    // Extrapolated from AnimationManager.js\n    au_data.forEach(({ id, intensity, duration, explanation = \"\" }) => {\n        animationManager.scheduleChange(id, intensity * 90, duration, 0, explanation);\n    });\n}\n\nfunction slightSmileFace(){\n    let au_data = [\n        {\n          \"id\": \"6\",\n          \"intensity\": 0.4,\n          \"duration\": 750,\n          \"explanation\": \"\"\n        },\n        {\n          \"id\": \"12\",\n          \"intensity\": 0.5,\n          \"duration\": 750,\n          \"explanation\": \"\"\n        }\n      ]\n\n    setFace(au_data=au_data)\n}\n\nfunction wink(){\n    let au_data = [\n        {\n            \"id\": \"46L\",\n            \"intensity\": 0.9,\n            \"duration\": 300,\n            \"explanation\": \"\"\n        }\n    ]\n\n    setFace(au_data=au_data)\n}\n\nfunction neutralFace(){\n    animationManager.setFaceToNeutral()\n}\n\nfunction captureUserResponse() {\n    return new Promise((resolve) => {\n        if ('webkitSpeechRecognition' in window) {\n            console.log('SpeechRecognition available!');\n            processingRequest = false;\n\n            console.log('recognitionStop:', recognitionStop)\n\n            if (!recognitionStop) {\n                recognitionAvailable = true;\n                recognitionStop = false;\n\n                // Initialize SpeechRecognition object\n                const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;\n                recognition = new SpeechRecognition();\n\n                recognition.continuous = false;\n                recognition.lang = 'en-US';\n                recognition.interimResults = true;\n                recognition.maxAlternatives = 1;\n\n                let silenceTimer;\n\n                recognition.onresult = async function(event) {\n                    let transcript = '';\n                    let isFinal = false;\n\n                    for (let i = event.resultIndex; i < event.results.length; i++) {\n                        transcript += event.results[i][0].transcript;\n                        if (event.results[i].isFinal) {\n                            isFinal = true;\n                        }\n                    }\n\n                    console.log('processingRequest:', processingRequest);\n\n                    // Process only final results to avoid duplicates\n                    if (isFinal && transcript !== lastTranscript && !processingRequest) {\n                        lastTranscript = transcript;\n                        processingRequest = true;\n\n                        // Process transcript\n                        let processTranscript = transcript.toLowerCase().trim();\n                        console.log('Recognized Speech:', processTranscript);\n\n                        // Stop the timer and recognition when a final result is detected\n                        clearTimeout(silenceTimer);\n                        recognition.stop();\n\n                        // Call a function to handle the processed response\n                        processingRequest = false;\n                        resolve(transcript); // Resolve the promise to indicate processing is complete\n                    } else {\n                        // Reset silence timer if interim results are detected\n                        clearTimeout(silenceTimer);\n                        silenceTimer = setTimeout(() => {\n                            console.log('No speech detected, stopping recognition.');\n                            recognition.stop(); // Stop recognition after a period of silence\n                            resolve(transcript); // Resolve the promise to continue\n                        }, 2000); // Adjust the timeout duration as needed\n                    }\n                };\n\n                recognition.onend = () => {\n                    console.log('Speech recognition ended.');\n                    resolve(lastTranscript); // Resolve the promise when recognition ends\n                };\n\n                recognition.onerror = (event) => {\n                    console.error('Speech recognition error:', event.error);\n                    clearTimeout(silenceTimer);\n                    resolve(lastTranscript); // Resolve even on error\n                };\n            }\n        } else {\n            console.error('SpeechRecognition not available in this browser!');\n            resolve(); // Resolve if not available\n        }\n\n        if (recognitionAvailable) {\n            recognition.start();\n            console.log(\"Speech recognition started!\");\n        }\n    });\n}\n\n\nfunction delay(ms) {\n    return new Promise(resolve => setTimeout(resolve, ms));\n}\n\nasync function agentSpeak(text) {\n    speechManager.initSynthesizer();\n    let synthesize_result = await speechManager.synthesizeSpeech(text);\n\n    console.log('synthesize_result object:', synthesize_result);\n    console.log('synthesize result transcript:', text)\n    console.log('synthesize result audioDuration:', synthesize_result.audioDuration);\n\n    console.log('before delay!');\n\n    try {\n        await delay(synthesize_result.audioDuration / 10000);\n        console.log('after delay!');\n        return text\n    } catch (error) {\n        if (error.message === 'Delay aborted') {\n            console.log('Delay was aborted');\n        } else {\n            console.error('Error during delay:', error);\n        }\n    }\n}\n\nasync function processSpeech(transcript, gptManager){\n    let gptResponse = ''\n\n    try{\n        gptResponse = await gptManager.processText(transcript)\n        \n        // gptResponse = transcript\n\n        console.log('gptResponse: ', gptResponse)\n    }\n    catch (e){\n        console.error('Error processing GPT response:', e)\n    }\n    try{\n        if(gptResponse){\n            await agentSpeak(gptResponse)\n            numTransactions += 1\n        }\n\n        return gptResponse\n    }\n    catch (e) {\n        console.error('Error processing TTS response:', e)\n    }\n}\n\nasync function start(animationManager, settings){\n    recognitionStop = false;\n\n    // Initialize SpeechManager object\n    if (settings.azure_api_key && typeof settings.azure_api_key == 'string'){\n        speechManager = new SpeechManager(animationManager, settings.azure_api_key, 'eastus')\n    }\n    else{\n        speechManager = new SpeechManager(animationManager, settings.azure_api_key.default, 'eastus')\n    }\n\n    // Initialize TextToGptReconciler object\n    if (settings.openai_api_key && typeof settings.openai_api_key == 'string'){\n        gptManager = new TextToGptReconciler(settings.openai_api_key)\n    }\n    else{\n        gptManager = new TextToGptReconciler(settings.openai_api_key.default)\n    }\n\n    let questions = {\n        \"1\": \"Do you like attending FIU?\",\n        \"2\": \"Where are you originally from?\",\n        \"3\": \"Do you like living in South Florida?\",\n        \"4\": \"Where would you like to go for you next vacation?\",\n        \"5\": \"Do you enjoy speaking with me?\",\n        \"6\": \"Would you speak with me again?\"\n    }\n\n    console.log(`questions: ${questions}`)\n\n    let num_questions = Object.keys(questions).length\n\n    slightSmileFace()\n\n    await agentSpeak(`Hello, my name is eva. I will ask you ${num_questions} questions. Feel free to answer as you feel appropriate!`)\n\n    neutralFace()\n\n    let transactions = {}\n\n    for(let i = 1; i <= num_questions; i++){\n        console.log(`current question: ${i}`)\n\n        let current_transaction = {}\n\n        let agent_question = await agentSpeak(questions[i])\n        current_transaction['agent_question'] = agent_question\n        console.log('agent_question:', agent_question)\n\n        let user_response = await captureUserResponse()\n        current_transaction['user_response'] = user_response\n        console.log('user_response:', user_response)\n\n        let agent_instruction = `You are a helpful agent providing simple one sentence responses to user replies to your questions. Do not ask follow-up questions.\\nAgent Questions: ${agent_question}\\nUser Response:${user_response}`\n\n        let agent_response = await processSpeech(agent_instruction, gptManager)\n        current_transaction['agent_response'] = agent_response\n        console.log('agent_response:', agent_response)\n\n        transactions[i] = current_transaction\n    }\n\n    await agentSpeak(`It appears I've asked all my questions.`)\n    await agentSpeak(\"If you'd like to review this conversation, a transcript is available in the inspector.\")\n    await agentSpeak('Thanks for answering all my questions!')\n\n    slightSmileFace()\n    wink()\n    await delay(1000)\n    neutralFace()\n\n    let json_string = JSON.stringify(transactions, null, 2)\n\n    console.log(`Transaction History:\\n${json_string}`)\n}\n\nfunction stop(){\n    console.log('Calling stop!')\n    \n    recognitionStop = true\n    console.log('recognitionStop:', recognitionStop)\n    \n    if (recognitionAvailable){\n        recognition.stop()    \n        console.log(\"speech recognition stopped!\")\n    }\n\n    console.log('Reached end stop!')\n}\n\nexport { start, stop }"],"names":["constructor","animationManager","apiKey","region","this","queue","isSpeaking","audioInterrupt","speechConfig","SpeechConfig","fromSubscription","initSynthesizer","recognizeSpeechUntilSilence","Promise","resolve","reject","audioConfig","AudioConfig","fromDefaultMicrophoneInput","recognizer","SpeechRecognizer","recognizedText","silenceTimer","finalRecognitionTimestamp","setTimeout","console","log","stopContinuousRecognitionAsync","text","trim","timestamp","recognizing","s","e","concat","result","clearTimeout","recognized","reason","ResultReason","RecognizedSpeech","Date","now","NoMatch","canceled","error","Error","sessionStopped","startContinuousRecognitionAsync","err","speechSynthesisVoiceName","player","SpeakerAudioDestination","fromSpeakerOutput","synthesizer","SpeechSynthesizer","visemeReceived","scheduleVisemeApplication","visemeId","audioOffset","enqueueText","push","processQueue","length","shift","synthesizeSpeech","then","speakTextAsync","applyVisemeToCharacter","facsLib","setFaceToNeutral","setNeutralViseme","setTargetViseme","updateEngine","stopSpeech","close","interruptSpeech","apiUrl","processText","payload","model","messages","role","content","arguments","undefined","max_tokens","temperature","_data$choices","_data$choices$","_data$choices$$messag","_data$choices$$messag2","response","fetch","method","headers","body","JSON","stringify","ok","status","gptResponse","json","choices","message","recognition","gptManager","speechManager","numTransactions","recognitionAvailable","lastTranscript","processingRequest","recognitionStop","setFace","au_data","forEach","_ref","id","intensity","duration","explanation","scheduleChange","slightSmileFace","neutralFace","delay","ms","async","agentSpeak","synthesize_result","audioDuration","processSpeech","transcript","start","settings","azure_api_key","SpeechManager","default","openai_api_key","TextToGptReconciler","questions","num_questions","Object","keys","transactions","i","current_transaction","agent_question","user_response","window","SpeechRecognition","webkitSpeechRecognition","continuous","lang","interimResults","maxAlternatives","onresult","event","isFinal","resultIndex","results","processTranscript","toLowerCase","stop","onend","onerror","agent_instruction","agent_response","json_string"],"sourceRoot":""}