{"version":3,"file":"static/js/713.7ddd3eb9.chunk.js","mappings":"sIA8HA,QA5HA,MACIA,WAAAA,CAAYC,EAAkBC,EAAQC,GAClCC,KAAKH,iBAAmBA,EACxBG,KAAKF,OAASA,EACdE,KAAKD,OAASA,EACdC,KAAKC,MAAQ,GACbD,KAAKE,YAAa,EAElBF,KAAKG,OACLH,KAAKI,YACLJ,KAAKK,gBAAiB,EAGtBL,KAAKM,iBACT,CAEAA,eAAAA,GACI,MAAMR,EAASE,KAAKF,OACdC,EAASC,KAAKD,OACdQ,EAAeC,EAAAA,aAAaC,iBAAiBX,EAAQC,GAC3DQ,EAAaG,yBAA2B,oBAExC,MAAMP,EAAS,IAAIQ,EAAAA,wBACbP,EAAcQ,EAAAA,YAAYC,kBAAkBV,GAElDH,KAAKG,OAASA,EACdH,KAAKI,YAAcA,EAEnBJ,KAAKc,YAAc,IAAIC,EAAAA,kBAAkBR,EAAcP,KAAKI,aAE5DJ,KAAKc,YAAYE,eAAiB,CAACC,EAAGC,KAClClB,KAAKmB,0BAA0BD,EAAEE,SAAUF,EAAEG,YAAY,CAEjE,CAEAC,WAAAA,CAAYC,GACRvB,KAAKC,MAAMuB,KAAKD,GACXvB,KAAKE,YACNF,KAAKyB,cAEb,CAEAA,YAAAA,GACI,GAA0B,IAAtBzB,KAAKC,MAAMyB,OAEX,YADA1B,KAAKE,YAAa,GAItBF,KAAKE,YAAa,EAClB,MAAMqB,EAAOvB,KAAKC,MAAM0B,QACxB3B,KAAK4B,iBAAiBL,GAAMM,MAAK,IAAM7B,KAAKyB,gBAChD,CAEAG,gBAAAA,CAAiBL,GACb,OAAO,IAAIO,SAAQ,CAACC,EAASC,KACzBhC,KAAKc,YAAYmB,eAAeV,GAC5BW,IACIC,QAAQC,IAAI,+BACZL,EAAQG,EAAO,IAEnBG,IACIF,QAAQC,IAAI,6CACZJ,EAAOK,EAAM,GACf,GAEd,CAEAlB,yBAAAA,CAA0BC,EAAUC,GAEhCiB,YAAW,KACPtC,KAAKuC,uBAAuBnB,EAAS,GAFZC,EAAc,IAI/C,CAEAkB,sBAAAA,CAAuBnB,GACnB,MAAMoB,EAAUxC,KAAKH,iBAAiB2C,QAEjCxC,KAAKK,gBASNL,KAAKH,iBAAiB4C,mBACtBD,EAAQE,oBATS,IAAbtB,EACAoB,EAAQE,iBAAiB,IAEzBtB,GAAY,EACZoB,EAAQG,gBAAgBvB,EAAU,GAAI,IAQ9CoB,EAAQI,cACZ,CAEAC,UAAAA,GAEI7C,KAAKc,YAAYgC,QAGjB9C,KAAKC,MAAQ,GACbD,KAAKE,YAAa,EAGlBF,KAAKM,kBAEL6B,QAAQC,IAAI,4BAChB,CAEAW,eAAAA,CAAgBxB,GAEZvB,KAAKc,YAAYgC,QAGjB9C,KAAKC,MAAQ,GACbD,KAAKE,YAAa,EAClBF,KAAKM,kBAEDiB,GACAvB,KAAKsB,YAAYC,GAGrBY,QAAQC,IAAI,gCAChB,E,gDCpEF,QAvDF,MACIxC,WAAAA,CAAYE,GACV,IAAKA,GAA4B,kBAAXA,EACpB,MAAM,IAAIkD,MAAM,4CAElBhD,KAAKF,OAASA,EACdE,KAAKiD,OAAS,4CAChB,CAQA,iBAAMC,CAAY3B,GAChB,MAAM4B,EAAU,CACdC,MAAO,gBACPC,SAAU,CACR,CAAEC,KAAM,SAAUC,QAJWC,UAAA9B,OAAA,QAAA+B,IAAAD,UAAA,GAAAA,UAAA,GAAG,oCAKhC,CAAEF,KAAM,OAAQC,QAAShC,IAE3BmC,WAAY,IACZC,YAAa,IAGf,IAAK,IAADC,EAAAC,EAAAC,EAAAC,EACF,MAAMC,QAAiBC,MAAMjE,KAAKiD,OAAQ,CACxCiB,OAAQ,OACRC,QAAS,CACP,cAAgB,UAADC,OAAYpE,KAAKF,QAChC,eAAgB,oBAElBuE,KAAMC,KAAKC,UAAUpB,KAGvB,IAAKa,EAASQ,GACZ,MAAM,IAAIxB,MAAM,mCAADoB,OAAoCJ,EAASS,SAG9D,MACMC,EAA0B,QAAfd,SADEI,EAASW,QACHC,eAAO,IAAAhB,GAAK,QAALC,EAAZD,EAAe,UAAE,IAAAC,GAAS,QAATC,EAAjBD,EAAmBgB,eAAO,IAAAf,GAAS,QAATC,EAA1BD,EAA4BP,eAAO,IAAAQ,OAAvB,EAAZA,EAAqCe,OAEzD,IAAKJ,EACH,MAAM,IAAI1B,MAAM,+CAGlB,OAAO0B,CACT,CAAE,MAAOrC,GAEP,MADAF,QAAQE,MAAM,kCAAmCA,GAC3CA,CACR,CACF,E,gGCjDJ,IACI0C,EAGAC,EAEAC,EAKAC,EAXAC,GAAuB,EAEvBC,EAAiB,GAMjBC,GAAoB,EACpBC,GAAkB,EAItB,SAASC,EAAQC,GAEbA,EAAQC,SAAQC,IAAoD,IAAnD,GAAEC,EAAE,UAAEC,EAAS,SAAEC,EAAQ,YAAEC,EAAc,IAAIJ,EAC1D7F,iBAAiBkG,eAAeJ,EAAgB,GAAZC,EAAgBC,EAAU,EAAGC,EAAY,GAErF,CAEA,SAASE,IACL,IAAIR,EAAU,CACV,CACE,GAAM,IACN,UAAa,GACb,SAAY,IACZ,YAAe,IAEjB,CACE,GAAM,KACN,UAAa,GACb,SAAY,IACZ,YAAe,KAIrBD,EAAgBC,EACpB,CAeA,SAASS,IACLpG,iBAAiB4C,kBACrB,CAyFA,SAASyD,EAAMC,GACX,OAAO,IAAIrE,SAAQC,GAAWO,WAAWP,EAASoE,IACtD,CAEAC,eAAeC,EAAW9E,GACtB0D,EAAc3E,kBACd,IAAIgG,QAA0BrB,EAAcrD,iBAAiBL,GAE7DY,QAAQC,IAAI,4BAA6BkE,GACzCnE,QAAQC,IAAI,gCAAiCb,GAC7CY,QAAQC,IAAI,mCAAoCkE,EAAkBC,eAElEpE,QAAQC,IAAI,iBAEZ,IAGI,aAFM8D,EAAMI,EAAkBC,cAAgB,KAC9CpE,QAAQC,IAAI,gBACLb,CACX,CAAE,MAAOc,GACiB,kBAAlBA,EAAMwC,QACN1C,QAAQC,IAAI,qBAEZD,QAAQE,MAAM,sBAAuBA,EAE7C,CACJ,CAEA+D,eAAeI,EAAcC,EAAYzB,GACrC,IAAIN,EAAc,GAElB,IACIA,QAAoBM,EAAW9B,YAAYuD,GAI3CtE,QAAQC,IAAI,gBAAiBsC,EACjC,CACA,MAAOxD,GACHiB,QAAQE,MAAM,iCAAkCnB,EACpD,CACA,IAMI,OALGwD,UACO2B,EAAW3B,GACjBQ,GAAmB,GAGhBR,CACX,CACA,MAAOxD,GACHiB,QAAQE,MAAM,iCAAkCnB,EACpD,CACJ,CAEAkF,eAAeM,EAAM7G,EAAkB8G,GACnCrB,GAAkB,EAIdL,EADA0B,EAASC,eAAkD,iBAA1BD,EAASC,cAC1B,IAAIC,EAAAA,EAAchH,EAAkB8G,EAASC,cAAe,UAG5D,IAAIC,EAAAA,EAAchH,EAAkB8G,EAASC,cAAcE,QAAS,UAKpF9B,EADA2B,EAASI,gBAAoD,iBAA3BJ,EAASI,eAC9B,IAAIC,EAAAA,EAAoBL,EAASI,gBAGjC,IAAIC,EAAAA,EAAoBL,EAASI,eAAeD,SAGjE,IAAIG,EAAY,CACZ,EAAK,6BACL,EAAK,iCACL,EAAK,uCACL,EAAK,oDACL,EAAK,iCACL,EAAK,kCAGT9E,QAAQC,IAAI,cAADgC,OAAe6C,IAE1B,IAAIC,EAAgBC,OAAOC,KAAKH,GAAWvF,OAE3CsE,UAEMK,EAAW,yCAADjC,OAA0C8C,EAAa,6DAEvEjB,IAEA,IAAIoB,EAAe,CAAC,EAEpB,IAAI,IAAIC,EAAI,EAAGA,GAAKJ,EAAeI,IAAI,CACnCnF,QAAQC,IAAI,qBAADgC,OAAsBkD,IAEjC,IAAIC,EAAsB,CAAC,EAEvBC,QAAuBnB,EAAWY,EAAUK,IAChDC,EAAoC,eAAIC,EACxCrF,QAAQC,IAAI,kBAAmBoF,GAE/B,IAAIC,QA5LD,IAAI3F,SAASC,IAChB,GAAI,4BAA6B2F,QAM7B,GALAvF,QAAQC,IAAI,gCACZiD,GAAoB,EAEpBlD,QAAQC,IAAI,mBAAoBkD,IAE3BA,EAAiB,CAClBH,GAAuB,EACvBG,GAAkB,EAGlB,MAAMqC,EAAoBD,OAAOC,mBAAqBD,OAAOE,wBAQ7D,IAAIC,EAPJ9C,EAAc,IAAI4C,EAElB5C,EAAY+C,YAAa,EACzB/C,EAAYgD,KAAO,QACnBhD,EAAYiD,gBAAiB,EAC7BjD,EAAYkD,gBAAkB,EAI9BlD,EAAYmD,SAAW9B,eAAe+B,GAClC,IAAI1B,EAAa,GACb2B,GAAU,EAEd,IAAK,IAAId,EAAIa,EAAME,YAAaf,EAAIa,EAAMG,QAAQ5G,OAAQ4F,IACtDb,GAAc0B,EAAMG,QAAQhB,GAAG,GAAGb,WAC9B0B,EAAMG,QAAQhB,GAAGc,UACjBA,GAAU,GAOlB,GAHAjG,QAAQC,IAAI,qBAAsBiD,GAG9B+C,GAAW3B,IAAerB,IAAmBC,EAAmB,CAChED,EAAiBqB,EACjBpB,GAAoB,EAGpB,IAAIkD,EAAoB9B,EAAW+B,cAAc1D,OACjD3C,QAAQC,IAAI,qBAAsBmG,GAGlCE,aAAaZ,GACb9C,EAAY2D,OAGZrD,GAAoB,EACpBtD,EAAQ0E,EACZ,MAEIgC,aAAaZ,GACbA,EAAevF,YAAW,KACtBH,QAAQC,IAAI,6CACZ2C,EAAY2D,OACZ3G,EAAQ0E,EAAW,GACpB,IAEX,EAEA1B,EAAY4D,MAAQ,KAChBxG,QAAQC,IAAI,6BACZL,EAAQqD,EAAe,EAG3BL,EAAY6D,QAAWT,IACnBhG,QAAQE,MAAM,4BAA6B8F,EAAM9F,OACjDoG,aAAaZ,GACb9F,EAAQqD,EAAe,CAE/B,OAEAjD,QAAQE,MAAM,oDACdN,IAGAoD,IACAJ,EAAY2B,QACZvE,QAAQC,IAAI,+BAChB,IA4GAmF,EAAmC,cAAIE,EACvCtF,QAAQC,IAAI,iBAAkBqF,GAE9B,IAAIoB,EAAiB,wJAAAzE,OAA2JoD,EAAc,oBAAApD,OAAmBqD,GAE7MqB,QAAuBtC,EAAcqC,EAAmB7D,GAC5DuC,EAAoC,eAAIuB,EACxC3G,QAAQC,IAAI,kBAAmB0G,GAE/BzB,EAAaC,GAAKC,CACtB,OAEMlB,EAAW,iDACXA,EAAW,gGACXA,EAAW,0CAEjBL,IArNAT,EATc,CACV,CACI,GAAM,MACN,UAAa,GACb,SAAY,IACZ,YAAe,YA2NjBW,EAAM,KACZD,IAEA,IAAI8C,EAAczE,KAAKC,UAAU8C,EAAc,KAAM,GAErDlF,QAAQC,IAAI,yBAADgC,OAA0B2E,GACzC,CAEA,SAASL,IACLvG,QAAQC,IAAI,iBAEZkD,GAAkB,EAClBnD,QAAQC,IAAI,mBAAoBkD,GAE5BH,IACAJ,EAAY2D,OACZvG,QAAQC,IAAI,gCAGhBD,QAAQC,IAAI,oBAChB,C","sources":["VISOS/action/verbalizers/SpeechManager.js","VISOS/cognition/TextToGptReconciler.js","modules/evaSimpleDialog.js"],"sourcesContent":["import { AudioConfig, SpeakerAudioDestination, SpeechConfig, SpeechSynthesizer } from \"microsoft-cognitiveservices-speech-sdk\";\n\nclass SpeechManager {\n    constructor(animationManager, apiKey, region) {\n        this.animationManager = animationManager;\n        this.apiKey = apiKey,\n        this.region = region,\n        this.queue = [];\n        this.isSpeaking = false;\n\n        this.player;\n        this.audioConfig;\n        this.audioInterrupt = false;\n\n        // Initialize synthesizer\n        this.initSynthesizer();\n    }\n\n    initSynthesizer() {\n        const apiKey = this.apiKey;\n        const region = this.region;\n        const speechConfig = SpeechConfig.fromSubscription(apiKey, region);\n        speechConfig.speechSynthesisVoiceName = \"en-US-JennyNeural\";\n\n        const player = new SpeakerAudioDestination();;\n        const audioConfig = AudioConfig.fromSpeakerOutput(player);\n\n        this.player = player\n        this.audioConfig = audioConfig\n\n        this.synthesizer = new SpeechSynthesizer(speechConfig, this.audioConfig);\n\n        this.synthesizer.visemeReceived = (s, e) => {\n            this.scheduleVisemeApplication(e.visemeId, e.audioOffset);\n        };\n    }\n\n    enqueueText(text) {\n        this.queue.push(text);\n        if (!this.isSpeaking) {\n            this.processQueue();\n        }\n    }\n\n    processQueue() {\n        if (this.queue.length === 0) {\n            this.isSpeaking = false;\n            return;\n        }\n\n        this.isSpeaking = true;\n        const text = this.queue.shift();\n        this.synthesizeSpeech(text).then(() => this.processQueue());\n    }\n\n    synthesizeSpeech(text) {\n        return new Promise((resolve, reject) => {\n            this.synthesizer.speakTextAsync(text,\n                result => {\n                    console.log(\"Speech synthesis completed.\");\n                    resolve(result);\n                },\n                error => {\n                    console.log(\"Error during speech synthesis, restarting\");\n                    reject(error);\n                });\n        });\n    }\n\n    scheduleVisemeApplication(visemeId, audioOffset) {\n        const offsetInMilliseconds = audioOffset / 10000;\n        setTimeout(() => {\n            this.applyVisemeToCharacter(visemeId);\n        }, offsetInMilliseconds);\n    }\n\n    applyVisemeToCharacter(visemeId) {\n        const facsLib = this.animationManager.facsLib;\n        // console.log('audioInterrupt:', this.audioInterrupt)\n        if (!this.audioInterrupt){\n            if (visemeId === 0) {\n                facsLib.setNeutralViseme(0.0);\n            } else {\n                visemeId -= 1;\n                facsLib.setTargetViseme(visemeId, 70, 0);\n            }\n        }\n        else{\n            this.animationManager.setFaceToNeutral()\n            facsLib.setNeutralViseme()\n        }\n\n        facsLib.updateEngine();\n    }\n\n    stopSpeech() {\n        // Dispose of the current synthesizer to stop speech\n        this.synthesizer.close();\n\n        // Clear the queue and reset the speaking state\n        this.queue = [];\n        this.isSpeaking = false;\n\n        // Reinitialize the synthesizer for future use\n        this.initSynthesizer();\n\n        console.log(\"Speech synthesis stopped.\");\n    }\n\n    interruptSpeech(text) {\n        // Dispose of the current synthesizer to stop speech\n        this.synthesizer.close();\n\n        // Clear the queue, reset the speaking state, and reinitialize the synthesizer\n        this.queue = [];\n        this.isSpeaking = false;\n        this.initSynthesizer();\n\n        if (text) {\n            this.enqueueText(text); // Enqueue and play the new text\n        }\n\n        console.log(\"Speech synthesis interrupted.\");\n    }\n}\n\nexport default SpeechManager;\n","class TextToGptReconciler {\n    constructor(apiKey) {\n      if (!apiKey || typeof apiKey !== 'string') {\n        throw new Error('A valid OpenAI API key must be provided.');\n      }\n      this.apiKey = apiKey;\n      this.apiUrl = 'https://api.openai.com/v1/chat/completions';\n    }\n  \n    /**\n     * Processes the provided text by sending it to the OpenAI API.\n     * @param {string} text - The input text to process.\n     * @param {string} instruction - Instruction or system prompt for the AI.\n     * @returns {Promise<string>} - The GPT response.\n     */\n    async processText(text, instruction = 'Answer in a professional manner:') {\n      const payload = {\n        model: 'gpt-3.5-turbo',  // You can switch this model based on your requirement\n        messages: [\n          { role: 'system', content: instruction },\n          { role: 'user', content: text }\n        ],\n        max_tokens: 150,  // Adjust as necessary\n        temperature: 0.7\n      };\n  \n      try {\n        const response = await fetch(this.apiUrl, {\n          method: 'POST',\n          headers: {\n            'Authorization': `Bearer ${this.apiKey}`,\n            'Content-Type': 'application/json'\n          },\n          body: JSON.stringify(payload)\n        });\n  \n        if (!response.ok) {\n          throw new Error(`API request failed with status: ${response.status}`);\n        }\n  \n        const data = await response.json();\n        const gptResponse = data.choices?.[0]?.message?.content?.trim();\n  \n        if (!gptResponse) {\n          throw new Error('Failed to get a valid response from the API');\n        }\n  \n        return gptResponse;\n      } catch (error) {\n        console.error('Error processing text with GPT:', error);\n        throw error;\n      }\n    }\n  }\n  \n  export default TextToGptReconciler;","import SpeechManager from \"../VISOS/action/verbalizers/SpeechManager\";\nimport TextToGptReconciler from \"../VISOS/cognition/TextToGptReconciler\";\n\nlet recognitionAvailable = false;\nlet recognition;\nlet lastTranscript = '';\n\nlet gptManager;\n\nlet speechManager;\n\nlet processingRequest = false;\nlet recognitionStop = false;\n\nlet numTransactions;\n\nfunction setFace(au_data){\n    // Extrapolated from AnimationManager.js\n    au_data.forEach(({ id, intensity, duration, explanation = \"\" }) => {\n        animationManager.scheduleChange(id, intensity * 90, duration, 0, explanation);\n    });\n}\n\nfunction slightSmileFace(){\n    let au_data = [\n        {\n          \"id\": \"6\",\n          \"intensity\": 0.4,\n          \"duration\": 750,\n          \"explanation\": \"\"\n        },\n        {\n          \"id\": \"12\",\n          \"intensity\": 0.5,\n          \"duration\": 750,\n          \"explanation\": \"\"\n        }\n      ]\n\n    setFace(au_data=au_data)\n}\n\nfunction wink(){\n    let au_data = [\n        {\n            \"id\": \"46L\",\n            \"intensity\": 0.9,\n            \"duration\": 300,\n            \"explanation\": \"\"\n        }\n    ]\n\n    setFace(au_data=au_data)\n}\n\nfunction neutralFace(){\n    animationManager.setFaceToNeutral()\n}\n\nfunction captureUserResponse() {\n    return new Promise((resolve) => {\n        if ('webkitSpeechRecognition' in window) {\n            console.log('SpeechRecognition available!');\n            processingRequest = false;\n\n            console.log('recognitionStop:', recognitionStop)\n\n            if (!recognitionStop) {\n                recognitionAvailable = true;\n                recognitionStop = false;\n\n                // Initialize SpeechRecognition object\n                const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;\n                recognition = new SpeechRecognition();\n\n                recognition.continuous = false;\n                recognition.lang = 'en-US';\n                recognition.interimResults = true;\n                recognition.maxAlternatives = 1;\n\n                let silenceTimer;\n\n                recognition.onresult = async function(event) {\n                    let transcript = '';\n                    let isFinal = false;\n\n                    for (let i = event.resultIndex; i < event.results.length; i++) {\n                        transcript += event.results[i][0].transcript;\n                        if (event.results[i].isFinal) {\n                            isFinal = true;\n                        }\n                    }\n\n                    console.log('processingRequest:', processingRequest);\n\n                    // Process only final results to avoid duplicates\n                    if (isFinal && transcript !== lastTranscript && !processingRequest) {\n                        lastTranscript = transcript;\n                        processingRequest = true;\n\n                        // Process transcript\n                        let processTranscript = transcript.toLowerCase().trim();\n                        console.log('Recognized Speech:', processTranscript);\n\n                        // Stop the timer and recognition when a final result is detected\n                        clearTimeout(silenceTimer);\n                        recognition.stop();\n\n                        // Call a function to handle the processed response\n                        processingRequest = false;\n                        resolve(transcript); // Resolve the promise to indicate processing is complete\n                    } else {\n                        // Reset silence timer if interim results are detected\n                        clearTimeout(silenceTimer);\n                        silenceTimer = setTimeout(() => {\n                            console.log('No speech detected, stopping recognition.');\n                            recognition.stop(); // Stop recognition after a period of silence\n                            resolve(transcript); // Resolve the promise to continue\n                        }, 2000); // Adjust the timeout duration as needed\n                    }\n                };\n\n                recognition.onend = () => {\n                    console.log('Speech recognition ended.');\n                    resolve(lastTranscript); // Resolve the promise when recognition ends\n                };\n\n                recognition.onerror = (event) => {\n                    console.error('Speech recognition error:', event.error);\n                    clearTimeout(silenceTimer);\n                    resolve(lastTranscript); // Resolve even on error\n                };\n            }\n        } else {\n            console.error('SpeechRecognition not available in this browser!');\n            resolve(); // Resolve if not available\n        }\n\n        if (recognitionAvailable) {\n            recognition.start();\n            console.log(\"Speech recognition started!\");\n        }\n    });\n}\n\n\nfunction delay(ms) {\n    return new Promise(resolve => setTimeout(resolve, ms));\n}\n\nasync function agentSpeak(text) {\n    speechManager.initSynthesizer();\n    let synthesize_result = await speechManager.synthesizeSpeech(text);\n\n    console.log('synthesize_result object:', synthesize_result);\n    console.log('synthesize result transcript:', text)\n    console.log('synthesize result audioDuration:', synthesize_result.audioDuration);\n\n    console.log('before delay!');\n\n    try {\n        await delay(synthesize_result.audioDuration / 10000);\n        console.log('after delay!');\n        return text\n    } catch (error) {\n        if (error.message === 'Delay aborted') {\n            console.log('Delay was aborted');\n        } else {\n            console.error('Error during delay:', error);\n        }\n    }\n}\n\nasync function processSpeech(transcript, gptManager){\n    let gptResponse = ''\n\n    try{\n        gptResponse = await gptManager.processText(transcript)\n        \n        // gptResponse = transcript\n\n        console.log('gptResponse: ', gptResponse)\n    }\n    catch (e){\n        console.error('Error processing GPT response:', e)\n    }\n    try{\n        if(gptResponse){\n            await agentSpeak(gptResponse)\n            numTransactions += 1\n        }\n\n        return gptResponse\n    }\n    catch (e) {\n        console.error('Error processing TTS response:', e)\n    }\n}\n\nasync function start(animationManager, settings){\n    recognitionStop = false;\n\n    // Initialize SpeechManager object\n    if (settings.azure_api_key && typeof settings.azure_api_key == 'string'){\n        speechManager = new SpeechManager(animationManager, settings.azure_api_key, 'eastus')\n    }\n    else{\n        speechManager = new SpeechManager(animationManager, settings.azure_api_key.default, 'eastus')\n    }\n\n    // Initialize TextToGptReconciler object\n    if (settings.openai_api_key && typeof settings.openai_api_key == 'string'){\n        gptManager = new TextToGptReconciler(settings.openai_api_key)\n    }\n    else{\n        gptManager = new TextToGptReconciler(settings.openai_api_key.default)\n    }\n\n    let questions = {\n        \"1\": \"Do you like attending FIU?\",\n        \"2\": \"Where are you originally from?\",\n        \"3\": \"Do you like living in South Florida?\",\n        \"4\": \"Where would you like to go for you next vacation?\",\n        \"5\": \"Do you enjoy speaking with me?\",\n        \"6\": \"Would you speak with me again?\"\n    }\n\n    console.log(`questions: ${questions}`)\n\n    let num_questions = Object.keys(questions).length\n\n    slightSmileFace()\n\n    await agentSpeak(`Hello, my name is eva. I will ask you ${num_questions} questions. Feel free to answer as you feel appropriate!`)\n\n    neutralFace()\n\n    let transactions = {}\n\n    for(let i = 1; i <= num_questions; i++){\n        console.log(`current question: ${i}`)\n\n        let current_transaction = {}\n\n        let agent_question = await agentSpeak(questions[i])\n        current_transaction['agent_question'] = agent_question\n        console.log('agent_question:', agent_question)\n\n        let user_response = await captureUserResponse()\n        current_transaction['user_response'] = user_response\n        console.log('user_response:', user_response)\n\n        let agent_instruction = `You are a helpful agent providing simple one sentence responses to user replies to your questions. Do not ask follow-up questions.\\nAgent Questions: ${agent_question}\\nUser Response:${user_response}`\n\n        let agent_response = await processSpeech(agent_instruction, gptManager)\n        current_transaction['agent_response'] = agent_response\n        console.log('agent_response:', agent_response)\n\n        transactions[i] = current_transaction\n    }\n\n    await agentSpeak(`It appears I've asked all my questions.`)\n    await agentSpeak(\"If you'd like to review this conversation, a transcript is available in the inspector.\")\n    await agentSpeak('Thanks for answering all my questions!')\n\n    slightSmileFace()\n    wink()\n    await delay(1000)\n    neutralFace()\n\n    let json_string = JSON.stringify(transactions, null, 2)\n\n    console.log(`Transaction History:\\n${json_string}`)\n}\n\nfunction stop(){\n    console.log('Calling stop!')\n    \n    recognitionStop = true\n    console.log('recognitionStop:', recognitionStop)\n    \n    if (recognitionAvailable){\n        recognition.stop()    \n        console.log(\"speech recognition stopped!\")\n    }\n\n    console.log('Reached end stop!')\n}\n\nexport { start, stop }"],"names":["constructor","animationManager","apiKey","region","this","queue","isSpeaking","player","audioConfig","audioInterrupt","initSynthesizer","speechConfig","SpeechConfig","fromSubscription","speechSynthesisVoiceName","SpeakerAudioDestination","AudioConfig","fromSpeakerOutput","synthesizer","SpeechSynthesizer","visemeReceived","s","e","scheduleVisemeApplication","visemeId","audioOffset","enqueueText","text","push","processQueue","length","shift","synthesizeSpeech","then","Promise","resolve","reject","speakTextAsync","result","console","log","error","setTimeout","applyVisemeToCharacter","facsLib","setFaceToNeutral","setNeutralViseme","setTargetViseme","updateEngine","stopSpeech","close","interruptSpeech","Error","apiUrl","processText","payload","model","messages","role","content","arguments","undefined","max_tokens","temperature","_data$choices","_data$choices$","_data$choices$$messag","_data$choices$$messag2","response","fetch","method","headers","concat","body","JSON","stringify","ok","status","gptResponse","json","choices","message","trim","recognition","gptManager","speechManager","numTransactions","recognitionAvailable","lastTranscript","processingRequest","recognitionStop","setFace","au_data","forEach","_ref","id","intensity","duration","explanation","scheduleChange","slightSmileFace","neutralFace","delay","ms","async","agentSpeak","synthesize_result","audioDuration","processSpeech","transcript","start","settings","azure_api_key","SpeechManager","default","openai_api_key","TextToGptReconciler","questions","num_questions","Object","keys","transactions","i","current_transaction","agent_question","user_response","window","SpeechRecognition","webkitSpeechRecognition","silenceTimer","continuous","lang","interimResults","maxAlternatives","onresult","event","isFinal","resultIndex","results","processTranscript","toLowerCase","clearTimeout","stop","onend","onerror","agent_instruction","agent_response","json_string"],"sourceRoot":""}